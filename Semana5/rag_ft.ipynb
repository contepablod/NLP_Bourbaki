{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf9e6d1",
   "metadata": {},
   "source": [
    "![Colegio Bourbaki](./Images/Bourbaki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70ffd6",
   "metadata": {},
   "source": [
    "## Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9a929",
   "metadata": {},
   "source": [
    "En este notebook haremos lo siguiente:\n",
    "\n",
    "1. **Explicaremos** la diferencia entre:   \n",
    "- Generación aumentada por recuperación (**RAG**)   \n",
    "- **Ajuste fino** de un modelo de lenguaje\n",
    "- Uso de **ambos juntos** \n",
    "\n",
    "2. **Implementaremos un pequeño proceso RAG**:   \n",
    "- Usaremos un transformador de oraciones para incrustar documentos  \n",
    "- Almacenaremos las incrustaciones en un índice vectorial  \n",
    "- Recuperaremos pasajes relevantes  \n",
    "- Usaremos un pequeño modelo de chat de pesos abiertos para responder preguntas de ese contexto \n",
    "\n",
    "3. **Ajustar un pequeño modelo de pesos abiertos** en un pequeño conjunto de datos de preguntas y respuestas   \n",
    "- Utilizar LoRA / QLoRA para ajustarlo a una GPU de ~4 GB   \n",
    "- Comparar las respuestas **antes y después** del ajuste. Suponemos que se trata de una GPU como la **NVIDIA GeForce GTX 1650 Ti 4 GB**, por lo que haremos lo siguiente: - Utilizar un modelo pequeño: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`. \n",
    "- Cargarlo en **4 bits** siempre que sea posible. \n",
    "- Mantener tamaños de lote pequeños. Este es un cuaderno *didáctico*: no espere una calidad de vanguardia, pero debería ayudarle a comprender **cuándo utilizar RAG y cuándo ajustar**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44efd8",
   "metadata": {},
   "source": [
    "## RAG frente al ajuste fino (conceptual)\n",
    "\n",
    "### ¿Qué es RAG (generación aumentada por recuperación)?\n",
    "\n",
    "Los LLM tienen un **conocimiento limitado**: solo saben lo que vieron durante el entrenamiento previo.  \n",
    "RAG añade un **almacén de conocimiento externo** (por ejemplo, una base de datos vectorial):\n",
    "\n",
    "1. Se **incrustan** los documentos (artículos, documentos, tickets) en vectores.\n",
    "2. En el momento de la consulta, se:\n",
    "   - Incrusta la pregunta del usuario.\n",
    "   - Recupera los **documentos más similares**.\n",
    "   - Pasa la *pregunta + el contexto recuperado* al LLM.\n",
    "3. El modelo responde *utilizando ese contexto*, sin cambiar sus pesos.\n",
    "\n",
    "**Ventajas:**\n",
    "- Ideal para **datos nuevos y que cambian con frecuencia** (como las noticias diarias).\n",
    "- No requiere un entrenamiento pesado, solo incrustación + recuperación.\n",
    "- Seguro: no sobrescribe el modelo.\n",
    "\n",
    "**Desventajas:**\n",
    "- La calidad de la respuesta depende de la **calidad de la recuperación** y del tamaño de la solicitud.\n",
    "- Limitado por la **ventana de contexto**: solo se puede pasar una cantidad limitada de texto.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Qué es el ajuste fino?\n",
    "\n",
    "El ajuste fino significa **continuar entrenando** un LLM preentrenado en una **tarea o dominio específico**:\n",
    "\n",
    "- Ejemplo: miles de pares de preguntas y respuestas sobre la nube, Kubernetes, fintech, etc.\n",
    "- El modelo **actualiza sus pesos** para interiorizar este dominio.\n",
    "\n",
    "**Ventajas:**\n",
    "- El modelo mejora de forma nativa en ese dominio o estilo.\n",
    "- No es necesario proporcionar siempre un contexto largo: «sabe» más en sus pesos.\n",
    "\n",
    "**Desventajas:**\n",
    "- **Es costoso** (tiempo de GPU, canalización de entrenamiento).\n",
    "- Necesita **datos buenos y seleccionados**.\n",
    "- El modelo sigue teniendo un límite de conocimiento fijo (no «verá» nuevos artículos a menos que se vuelva a entrenar).\n",
    "\n",
    "---\n",
    "\n",
    "Puede:\n",
    "\n",
    "- Utilizar GPT-4 / modelos más grandes (o cualquier «modelo experto») para **generar pares de preguntas y respuestas** a partir de documentos.\n",
    "- **Ajustar finamente un modelo de pesos abiertos más pequeño** en estos pares de preguntas y respuestas.\n",
    "- Mantener RAG también para inyectar **documentos muy recientes**.\n",
    "\n",
    "Resultado:\n",
    "- El modelo pequeño mejora en **jerga y estilo** gracias al ajuste fino.\n",
    "- RAG lo mantiene **actualizado** con nuevos documentos.\n",
    "\n",
    "En el resto de este cuaderno implementaremos:\n",
    "\n",
    "1. Un pequeño **canal RAG**.\n",
    "2. Un pequeño **ajuste fino LoRA**.\n",
    "3. Una rápida **comparación**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6eeee931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "# !pip install -q \\\n",
    "#   torch \\\n",
    "#   transformers \\\n",
    "#   accelerate \\\n",
    "#   bitsandbytes \\\n",
    "#   peft \\\n",
    "#   sentence-transformers \\\n",
    "#   datasets \\\n",
    "#   scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e4164",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d3488cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311877a",
   "metadata": {},
   "source": [
    "### Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d06e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "torch.backends.cuda.matmul.fp32_precision = (\n",
    "    \"ieee\"  # torch.backends.cuda.matmul.allow_tf32 = True\n",
    ")\n",
    "torch.backends.cudnn.conv.fp32_precision = (\n",
    "    \"tf32\"  # torch.backends.cudnn.allow_tf32 = True\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11005e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.12.11 (main, Sep  5 2025, 19:35:43) [GCC 13.3.0]\n",
      "__pyTorch VERSION: 2.9.0+cu128\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: 91002\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "print(\"__Python VERSION:\", sys.version)\n",
    "print(\"__pyTorch VERSION:\", torch.__version__)\n",
    "print(\n",
    "    \"__CUDA VERSION\",\n",
    ")\n",
    "print(\"__CUDNN VERSION:\", torch.backends.cudnn.version())\n",
    "print(\"__Number CUDA Devices:\", torch.cuda.device_count())\n",
    "print(\"__Devices\")\n",
    "print(\"Active CUDA Device: GPU\", torch.cuda.current_device())\n",
    "print(\"Available devices \", torch.cuda.device_count())\n",
    "print(\"Current cuda device \", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f8af7ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 17 21:30:13 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti     Off |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   58C    P0             14W /   50W |    1264MiB /   4096MiB |     31%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     11329      G   /usr/bin/gnome-shell                           81MiB |\n",
      "|    0   N/A  N/A     15284      G   /proc/self/exe                                228MiB |\n",
      "|    0   N/A  N/A   2664154      C   /usr/local/bin/python3.12.11                  948MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8b81482",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51598ba4",
   "metadata": {},
   "source": [
    "Vamos con un ejemplo pequeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "29266f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_docs = [\n",
    "    # 1\n",
    "    \"\"\"OpenAI released a new model that improves reasoning on complex code and math problems. \n",
    "    The model is optimized for tool use and retrieval-augmented generation pipelines.\"\"\",\n",
    "    # 2\n",
    "    \"\"\"Google announced updates to its Vertex AI platform, making it easier to deploy and monitor \n",
    "    large language models at enterprise scale.\"\"\",\n",
    "    # 3\n",
    "    \"\"\"Meta open-sourced a set of Llama-based models with billions of parameters, \n",
    "    enabling researchers and companies to fine-tune them for their own use cases.\"\"\",\n",
    "    # 4\n",
    "    \"\"\"Microsoft integrated generative AI into its Office suite, adding features such as \n",
    "    AI-powered summarization, drafting assistance, and automatic meeting notes generation.\"\"\",\n",
    "    # 5\n",
    "    \"\"\"Amazon Web Services introduced cheaper GPU instances optimized for inference workloads \n",
    "    like chatbots, code assistants, real-time search, and document question-answering.\"\"\",\n",
    "    # 6\n",
    "    \"\"\"NVIDIA released new open-source libraries for accelerating transformer inference, \n",
    "    offering significant speedups on consumer GPUs like the RTX 4090.\"\"\",\n",
    "    # 7\n",
    "    \"\"\"Anthropic published a research paper describing improvements in constitutional AI, \n",
    "    focusing on scalable oversight and safer model behavior.\"\"\",\n",
    "    # 8\n",
    "    \"\"\"Apple reportedly began testing on-device LLMs for future iPhone models, enabling \n",
    "    private AI features such as offline summarization and personal context reasoning.\"\"\",\n",
    "    # 9\n",
    "    \"\"\"Hugging Face launched a new inference API tier with higher throughput and native \n",
    "    support for vLLM, making it cheaper to serve models like Mistral-7B and Llama-3-8B.\"\"\",\n",
    "    # 10\n",
    "    \"\"\"Mistral AI released Mixtral-8x22B, a sparse mixture-of-experts model offering state-of-the-art \n",
    "    performance while remaining efficient enough for commercial deployment.\"\"\",\n",
    "    # 11\n",
    "    \"\"\"IBM announced a partnership with NASA to fine-tune foundation models on geospatial data \n",
    "    to improve climate analysis, wildfire prediction, and satellite imagery classification.\"\"\",\n",
    "    # 12\n",
    "    \"\"\"Databricks released DBRX, a 132B-weight mixture-of-experts model trained on curated \n",
    "    scientific and enterprise datasets, outperforming models of similar size.\"\"\",\n",
    "    # 13\n",
    "    \"\"\"Stability AI introduced Stable Diffusion 3, featuring improved text-image alignment \n",
    "    and reduced hallucination in multilingual prompting scenarios.\"\"\",\n",
    "    # 14\n",
    "    \"\"\"Snowflake added native vector search capabilities, allowing enterprises to store embeddings \n",
    "    and run RAG pipelines directly on their data warehouse.\"\"\",\n",
    "    # 15\n",
    "    \"\"\"Cohere launched a secure enterprise-grade embedding model designed for document retrieval, \n",
    "    semantic search, and multi-lingual knowledge-base applications.\"\"\",\n",
    "    # 16\n",
    "    \"\"\"Red Hat announced AI-enhanced DevOps tooling, including automated deployment validation \n",
    "    powered by small specialized LLMs.\"\"\",\n",
    "    # 17\n",
    "    \"\"\"Salesforce updated Einstein GPT with better CRM-specific reasoning, including lead scoring, \n",
    "    automatic email drafting, and pipeline forecasting.\"\"\",\n",
    "    # 18\n",
    "    \"\"\"Dropbox introduced AI-powered universal search across files, documents, PDFs, and images, \n",
    "    enabling users to query semantic content instantly.\"\"\",\n",
    "    # 19\n",
    "    \"\"\"Slack rolled out AI summarization for channels and threads, automatically generating \n",
    "    daily digests and extracting key decisions from long discussions.\"\"\",\n",
    "    # 20\n",
    "    \"\"\"Zoom added real-time conversation translation and AI-based meeting action items, \n",
    "    powered by a fine-tuned multilingual transformer model.\"\"\",\n",
    "]\n",
    "\n",
    "corpus_titles = [\n",
    "    \"OpenAI releases new reasoning model\",\n",
    "    \"Google updates Vertex AI\",\n",
    "    \"Meta open-sources Llama models\",\n",
    "    \"Microsoft adds AI to Office\",\n",
    "    \"AWS introduces cheaper GPU instances\",\n",
    "    \"NVIDIA releases transformer acceleration libs\",\n",
    "    \"Anthropic improves constitutional AI\",\n",
    "    \"Apple tests on-device LLMs\",\n",
    "    \"Hugging Face launches new inference tier\",\n",
    "    \"Mistral releases Mixtral-8x22B\",\n",
    "    \"IBM partners with NASA on geospatial AI\",\n",
    "    \"Databricks releases DBRX\",\n",
    "    \"Stability AI releases SD3\",\n",
    "    \"Snowflake adds vector search\",\n",
    "    \"Cohere launches enterprise embedding model\",\n",
    "    \"Red Hat adds AI DevOps tools\",\n",
    "    \"Salesforce updates Einstein GPT\",\n",
    "    \"Dropbox adds AI universal search\",\n",
    "    \"Slack adds AI summaries\",\n",
    "    \"Zoom adds real-time AI translation\",\n",
    "]\n",
    "\n",
    "len(corpus_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8d2ffe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small and fast embedding model (open weights)\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = SentenceTransformer(embedding_model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e592432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9d63a9640943b29932fc6352b80bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute embeddings\n",
    "doc_embeddings = embedder.encode(\n",
    "    corpus_docs, convert_to_numpy=True, show_progress_bar=True, device=device, normalize_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82482535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.05694531, -0.01272646, -0.06879752, ...,  0.05730383,\n",
       "          0.04768759,  0.00835872],\n",
       "        [-0.07661536, -0.08271152,  0.03887794, ..., -0.00933229,\n",
       "          0.05409345, -0.03391702],\n",
       "        [-0.03868605, -0.02878194, -0.02075998, ..., -0.04762491,\n",
       "         -0.01284006,  0.03692014],\n",
       "        ...,\n",
       "        [-0.02095782, -0.03330291, -0.04754037, ...,  0.04166466,\n",
       "          0.05232637,  0.02517612],\n",
       "        [-0.00392685, -0.02862822, -0.01042572, ...,  0.05525399,\n",
       "         -0.05279417, -0.02444052],\n",
       "        [-0.08633485, -0.04698378,  0.00952209, ...,  0.04447945,\n",
       "         -0.1053777 , -0.02525596]], dtype=float32),\n",
       " (20, 384))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_embeddings, doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7bf41aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "nn_index = NearestNeighbors(n_neighbors=3, metric=\"cosine\")\n",
    "nn_index.fit(doc_embeddings)\n",
    "# Faiss\n",
    "faiss_emb = np.array(doc_embeddings).astype(\"float32\")\n",
    "faiss_index = faiss.IndexFlatIP(faiss_emb.shape[1])  # cosine similarity via inner product\n",
    "faiss.normalize_L2(faiss_emb)\n",
    "faiss_index.add(faiss_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ba70968",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "785a9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bdef92d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'▁Bür': 15455,\n",
       "  'Pr': 4040,\n",
       "  '▁share': 6232,\n",
       "  '▁Major': 11019,\n",
       "  '▁Host': 16956,\n",
       "  '▁Lag': 16952,\n",
       "  'гу': 3325,\n",
       "  '▁сель': 11823,\n",
       "  'full': 8159,\n",
       "  'irche': 26846,\n",
       "  '▁Dro': 22938,\n",
       "  'Change': 7277,\n",
       "  'Processor': 18689,\n",
       "  'Access': 6638,\n",
       "  'тре': 11414,\n",
       "  '▁Late': 23089,\n",
       "  '▁scarc': 19494,\n",
       "  'ω': 30206,\n",
       "  'GA': 12739,\n",
       "  '▁Vil': 16450,\n",
       "  '▁Ball': 13402,\n",
       "  'oko': 15218,\n",
       "  '▁costa': 26303,\n",
       "  '▁vere': 24269,\n",
       "  '▁anywhere': 12214,\n",
       "  '▁typically': 12234,\n",
       "  'èt': 23855,\n",
       "  '▁également': 8648,\n",
       "  'itories': 20106,\n",
       "  '▁rang': 19120,\n",
       "  '▁brig': 16724,\n",
       "  '▁Gén': 26236,\n",
       "  '▁rim': 12726,\n",
       "  '▁Fish': 12030,\n",
       "  'Git': 28712,\n",
       "  'NET': 6006,\n",
       "  'key': 1989,\n",
       "  '▁upgrad': 20337,\n",
       "  '▁площа': 20281,\n",
       "  'ierten': 12025,\n",
       "  '▁Y': 612,\n",
       "  'estic': 15931,\n",
       "  'êtes': 22730,\n",
       "  '▁ERR': 22307,\n",
       "  'ře': 12859,\n",
       "  '▁relatives': 14576,\n",
       "  '▁cl': 1067,\n",
       "  '▁Oriental': 29702,\n",
       "  'Also': 17351,\n",
       "  '▁uncle': 22169,\n",
       "  '**': 1068,\n",
       "  '▁artifact': 24238,\n",
       "  '▁▁▁▁▁▁▁▁▁▁': 965,\n",
       "  '▁performance': 4180,\n",
       "  '▁craw': 29349,\n",
       "  '▁kleine': 20113,\n",
       "  'еди': 14996,\n",
       "  \"▁'/\": 8207,\n",
       "  '▁Blo': 11447,\n",
       "  '▁amaz': 21863,\n",
       "  'life': 19264,\n",
       "  '▁ihm': 9771,\n",
       "  '▁che': 923,\n",
       "  '▁Pom': 14351,\n",
       "  '▁poem': 26576,\n",
       "  'лях': 22373,\n",
       "  'free': 9021,\n",
       "  '▁bal': 6411,\n",
       "  '▁altogether': 19148,\n",
       "  '<0x28>': 43,\n",
       "  '▁timp': 29297,\n",
       "  'Timer': 14745,\n",
       "  '════': 24078,\n",
       "  '▁goals': 14433,\n",
       "  'ḩ': 31444,\n",
       "  '▁tijd': 14007,\n",
       "  '▁тому': 20178,\n",
       "  '▁Чем': 23214,\n",
       "  'igi': 10091,\n",
       "  'fte': 11927,\n",
       "  '▁exhib': 10371,\n",
       "  '▁output': 1962,\n",
       "  'гов': 9319,\n",
       "  '▁macro': 11758,\n",
       "  'erek': 20400,\n",
       "  'соб': 17393,\n",
       "  'str': 710,\n",
       "  'mysqli': 23727,\n",
       "  '▁Direct': 8797,\n",
       "  '▁suddenly': 11584,\n",
       "  '▁Hend': 17973,\n",
       "  'ссе': 16176,\n",
       "  '▁countries': 10916,\n",
       "  'ować': 28894,\n",
       "  '▁vorg': 28669,\n",
       "  'Sp': 5592,\n",
       "  'phere': 9085,\n",
       "  '서': 31093,\n",
       "  '▁rug': 29833,\n",
       "  'IABot': 24291,\n",
       "  'ctic': 20009,\n",
       "  'atern': 25744,\n",
       "  '▁infin': 8275,\n",
       "  '▁fert': 19965,\n",
       "  '▁insp': 8681,\n",
       "  '▁Че': 16525,\n",
       "  'mc': 14047,\n",
       "  'ancia': 14006,\n",
       "  '▁hos': 25506,\n",
       "  '▁deliber': 23663,\n",
       "  'unas': 17496,\n",
       "  '▁values': 1819,\n",
       "  'nu': 3433,\n",
       "  '）': 30409,\n",
       "  '▁fat': 9950,\n",
       "  'sol': 2929,\n",
       "  '▁Однако': 24017,\n",
       "  '▁clever': 23455,\n",
       "  'ims': 9893,\n",
       "  'atively': 6703,\n",
       "  '▁Spo': 24674,\n",
       "  '░': 30833,\n",
       "  'DEF': 24405,\n",
       "  '▁variant': 17305,\n",
       "  '▁Squad': 18827,\n",
       "  '▁жен': 14244,\n",
       "  '▁kill': 12088,\n",
       "  'dam': 16846,\n",
       "  'tensor': 20158,\n",
       "  '▁golden': 22843,\n",
       "  ',”': 3995,\n",
       "  '▁Medicine': 27529,\n",
       "  '▁Garc': 19734,\n",
       "  '▁św': 24785,\n",
       "  '▁caus': 3269,\n",
       "  '▁requirements': 11780,\n",
       "  'ক': 30995,\n",
       "  'ASS': 22933,\n",
       "  '▁gior': 17030,\n",
       "  '▁unit': 5190,\n",
       "  'atre': 6064,\n",
       "  '▁utwor': 28085,\n",
       "  '▁eine': 2128,\n",
       "  'rok': 16475,\n",
       "  '▁está': 7919,\n",
       "  'cito': 14524,\n",
       "  '▁klub': 14715,\n",
       "  'ASC': 28599,\n",
       "  '▁fed': 21242,\n",
       "  'jack': 21452,\n",
       "  '線': 31357,\n",
       "  'ril': 4115,\n",
       "  'ким': 9787,\n",
       "  '▁geomet': 28855,\n",
       "  '▁helping': 19912,\n",
       "  'ذ': 30851,\n",
       "  '\".': 1642,\n",
       "  '▁Mitch': 21190,\n",
       "  'ै': 31678,\n",
       "  'ἰ': 30985,\n",
       "  'rag': 1431,\n",
       "  'operatorname': 9158,\n",
       "  '▁règ': 27264,\n",
       "  'appe': 29065,\n",
       "  '▁Оте': 24229,\n",
       "  '▁Drop': 20724,\n",
       "  'вобо': 24950,\n",
       "  'aient': 10978,\n",
       "  '▁way': 982,\n",
       "  'leb': 19982,\n",
       "  '▁walked': 17096,\n",
       "  'BASE': 25416,\n",
       "  '▁probable': 16269,\n",
       "  '▁fi': 5713,\n",
       "  '▁spite': 21100,\n",
       "  '▁dez': 18466,\n",
       "  'ifact': 7060,\n",
       "  '/{': 19248,\n",
       "  'US': 3308,\n",
       "  'образ': 23944,\n",
       "  'Austral': 22537,\n",
       "  '▁osob': 27949,\n",
       "  'kow': 7000,\n",
       "  'url': 2271,\n",
       "  '▁environments': 23136,\n",
       "  'processing': 19170,\n",
       "  '▁requiring': 26795,\n",
       "  'érica': 10820,\n",
       "  '▁surr': 8388,\n",
       "  '▁mai': 5530,\n",
       "  '▁rac': 11021,\n",
       "  '▁orden': 16075,\n",
       "  '▁talál': 27764,\n",
       "  '▁zich': 9834,\n",
       "  'orer': 9386,\n",
       "  'cloud': 9274,\n",
       "  'rá': 6135,\n",
       "  'ibus': 19699,\n",
       "  '▁Одна': 18412,\n",
       "  'including': 18271,\n",
       "  '▁sig': 4365,\n",
       "  'keley': 27279,\n",
       "  '▁Gre': 4122,\n",
       "  'nach': 29683,\n",
       "  '▁remember': 6456,\n",
       "  '▁communicate': 23120,\n",
       "  'ancer': 25856,\n",
       "  '▁Кор': 20522,\n",
       "  \"\\\\'\": 20333,\n",
       "  '▁cpu': 26403,\n",
       "  'ikus': 17342,\n",
       "  '▁постро': 22486,\n",
       "  'ристи': 15932,\n",
       "  '▁группы': 22616,\n",
       "  '▁runner': 28877,\n",
       "  'got': 7085,\n",
       "  'ủ': 31556,\n",
       "  '▁című': 26773,\n",
       "  'always': 21936,\n",
       "  'unc': 4661,\n",
       "  'Den': 29315,\n",
       "  'ន': 31934,\n",
       "  '▁Allow': 29408,\n",
       "  'ada': 1114,\n",
       "  'แ': 31073,\n",
       "  'pect': 1103,\n",
       "  '▁Kör': 23722,\n",
       "  'зяй': 20188,\n",
       "  '▁hiding': 25508,\n",
       "  '▁Rudolf': 19027,\n",
       "  '▁Ale': 7801,\n",
       "  '▁další': 29067,\n",
       "  '▁Hand': 5166,\n",
       "  'frac': 1154,\n",
       "  '▁trat': 9248,\n",
       "  '▁Made': 18266,\n",
       "  'XML': 9165,\n",
       "  '▁mand': 9619,\n",
       "  'gesellschaft': 26175,\n",
       "  '▁royale': 25441,\n",
       "  'riteria': 21977,\n",
       "  'File': 2283,\n",
       "  'AUT': 20656,\n",
       "  'ado': 912,\n",
       "  '▁Daily': 23331,\n",
       "  'ca': 1113,\n",
       "  'ouwd': 26888,\n",
       "  '▁Ek': 16462,\n",
       "  'oured': 14076,\n",
       "  '▁Minn': 17988,\n",
       "  '▁leave': 5967,\n",
       "  'care': 18020,\n",
       "  'engu': 19636,\n",
       "  '▁$(': 2427,\n",
       "  'нием': 8751,\n",
       "  'Ne': 8139,\n",
       "  '▁Senior': 24260,\n",
       "  '▁secure': 11592,\n",
       "  'ков': 2530,\n",
       "  'ação': 8298,\n",
       "  '▁sid': 25349,\n",
       "  'geführt': 26210,\n",
       "  '▁oktober': 19306,\n",
       "  'special': 18732,\n",
       "  '》': 30843,\n",
       "  '▁iterator': 20380,\n",
       "  \"('#\": 14237,\n",
       "  '▁Dorf': 17207,\n",
       "  'жил': 23639,\n",
       "  '▁Tourn': 23720,\n",
       "  '▁recognition': 19679,\n",
       "  'CESS': 23524,\n",
       "  '▁Turner': 24215,\n",
       "  '▁Date': 4712,\n",
       "  '▁château': 20085,\n",
       "  '▁—': 813,\n",
       "  '▁hospital': 13457,\n",
       "  '▁FILE': 24080,\n",
       "  '▁Holland': 20262,\n",
       "  '▁lub': 14757,\n",
       "  'Could': 23323,\n",
       "  '▁Let': 2803,\n",
       "  'публи': 10862,\n",
       "  '~~~~~~~~': 26594,\n",
       "  'го': 588,\n",
       "  '▁prix': 19590,\n",
       "  'UES': 12996,\n",
       "  '├': 30188,\n",
       "  'ctor': 2801,\n",
       "  '▁кі': 25095,\n",
       "  '▁olymp': 15248,\n",
       "  '▁Mis': 20929,\n",
       "  '▁AD': 11033,\n",
       "  '▁cím': 22499,\n",
       "  'new': 1482,\n",
       "  '▁basket': 25972,\n",
       "  'tegr': 11528,\n",
       "  'Mem': 11442,\n",
       "  '▁Om': 13352,\n",
       "  '▁destru': 26468,\n",
       "  '小': 30446,\n",
       "  '▁junt': 25296,\n",
       "  'шо': 10093,\n",
       "  '▁greatly': 11180,\n",
       "  'iska': 8613,\n",
       "  'geben': 23475,\n",
       "  '▁attachment': 26305,\n",
       "  'pick': 23945,\n",
       "  '▁Orange': 26048,\n",
       "  'volume': 24623,\n",
       "  'Be': 3629,\n",
       "  'illet': 7324,\n",
       "  '▁questo': 11352,\n",
       "  'connection': 9965,\n",
       "  '▁Europa': 9646,\n",
       "  '▁Turk': 16559,\n",
       "  'ANCE': 23219,\n",
       "  'Defaults': 24863,\n",
       "  '▁NULL': 4265,\n",
       "  'LAB': 24461,\n",
       "  '▁metros': 24086,\n",
       "  'phabet': 17416,\n",
       "  '▁declared': 8052,\n",
       "  ']/': 16261,\n",
       "  'Dispatch': 14777,\n",
       "  'cers': 22543,\n",
       "  'ม': 30501,\n",
       "  '▁later': 2678,\n",
       "  '▁Emp': 7361,\n",
       "  'half': 24498,\n",
       "  '▁research': 5925,\n",
       "  'ule': 1297,\n",
       "  '▁carriage': 23840,\n",
       "  'Current': 7583,\n",
       "  '▁hrab': 28617,\n",
       "  '▁toward': 11183,\n",
       "  '▁vec': 9649,\n",
       "  'adow': 6986,\n",
       "  'under': 5062,\n",
       "  'indent': 12860,\n",
       "  'athers': 19467,\n",
       "  '▁form': 883,\n",
       "  'lea': 20774,\n",
       "  '▁volunt': 27081,\n",
       "  'く': 30568,\n",
       "  '▁Landkreis': 19126,\n",
       "  'ового': 11913,\n",
       "  '▁gang': 20676,\n",
       "  '(&': 6243,\n",
       "  '▁й': 7269,\n",
       "  '▁wit': 12309,\n",
       "  'hä': 24920,\n",
       "  'OF': 9800,\n",
       "  'ктив': 8888,\n",
       "  'оте': 20390,\n",
       "  'rides': 24040,\n",
       "  '▁occupied': 16404,\n",
       "  '▁params': 8636,\n",
       "  'ef': 1389,\n",
       "  '▁отли': 22216,\n",
       "  'Є': 30149,\n",
       "  '去': 31475,\n",
       "  '▁Tas': 23793,\n",
       "  'Bbb': 15165,\n",
       "  'стоя': 6428,\n",
       "  'Async': 8123,\n",
       "  'subset': 6484,\n",
       "  '▁Florida': 13813,\n",
       "  '▁Lane': 23841,\n",
       "  '参': 31125,\n",
       "  'arring': 23693,\n",
       "  '▁Dick': 12488,\n",
       "  '▁vil': 11928,\n",
       "  '正': 30724,\n",
       "  '▁programming': 8720,\n",
       "  '▁hes': 19066,\n",
       "  'CCESS': 26925,\n",
       "  'ovan': 10471,\n",
       "  '▁defines': 17645,\n",
       "  '▁дире': 20174,\n",
       "  'aster': 1901,\n",
       "  'бря': 5542,\n",
       "  'address': 7328,\n",
       "  '▁professional': 10257,\n",
       "  '▁Ill': 8408,\n",
       "  '▁External': 3985,\n",
       "  'ע': 30324,\n",
       "  '<0x0E>': 17,\n",
       "  'groupId': 9688,\n",
       "  'Progress': 14470,\n",
       "  '▁cried': 10680,\n",
       "  '~~': 7377,\n",
       "  \"'\\\\\": 12764,\n",
       "  'iciones': 14674,\n",
       "  '▁лу': 16050,\n",
       "  'center': 5064,\n",
       "  '▁Stat': 6666,\n",
       "  'texte': 29149,\n",
       "  'perm': 17858,\n",
       "  '▁versch': 8038,\n",
       "  '▁Notices': 16709,\n",
       "  '▁década': 22685,\n",
       "  'ath': 493,\n",
       "  'validation': 18157,\n",
       "  'стан': 4374,\n",
       "  '▁trop': 9201,\n",
       "  '▁defe': 8686,\n",
       "  '▁speaker': 25657,\n",
       "  '▁tool': 5780,\n",
       "  'няя': 23620,\n",
       "  '▁optimize': 24656,\n",
       "  'ordnung': 25432,\n",
       "  '▁Low': 17511,\n",
       "  '▁reducing': 27668,\n",
       "  'iju': 26323,\n",
       "  '▁proc': 9580,\n",
       "  '▁Diplom': 28243,\n",
       "  '甲': 31843,\n",
       "  '▁gam': 24988,\n",
       "  '▁нај': 26339,\n",
       "  'шен': 10084,\n",
       "  'avan': 29080,\n",
       "  'orb': 11831,\n",
       "  'xty': 29312,\n",
       "  'leaf': 29500,\n",
       "  'persistence': 28249,\n",
       "  '▁Online': 13542,\n",
       "  'mun': 24579,\n",
       "  '▁Amb': 15145,\n",
       "  'atori': 11168,\n",
       "  '▁ju': 3623,\n",
       "  '▁Cam': 5500,\n",
       "  '▁cientí': 23496,\n",
       "  'há': 19990,\n",
       "  '▁coinc': 22819,\n",
       "  'legt': 16260,\n",
       "  'entes': 5326,\n",
       "  '▁vš': 20145,\n",
       "  'O': 29949,\n",
       "  '▁foundation': 22778,\n",
       "  '▁individually': 29689,\n",
       "  '▁nur': 5595,\n",
       "  'Θ': 30546,\n",
       "  '漢': 31652,\n",
       "  '▁или': 7082,\n",
       "  'π': 30170,\n",
       "  '▁мно': 16071,\n",
       "  '▁rev': 6664,\n",
       "  '▁wł': 24009,\n",
       "  '▁Ми': 4860,\n",
       "  'h': 29882,\n",
       "  '┼': 30560,\n",
       "  '▁aggregate': 20431,\n",
       "  'page': 3488,\n",
       "  '▁investig': 7405,\n",
       "  '▁nada': 25801,\n",
       "  'ք': 31497,\n",
       "  '▁Cry': 22121,\n",
       "  '▁chief': 9087,\n",
       "  'étés': 28141,\n",
       "  'compare': 18307,\n",
       "  '(\"#': 14822,\n",
       "  '▁=': 353,\n",
       "  'nings': 11753,\n",
       "  'ąż': 19151,\n",
       "  'ху': 12735,\n",
       "  '▁wol': 20040,\n",
       "  '▁Regarding': 29499,\n",
       "  '▁имени': 21959,\n",
       "  '▁Ac': 7255,\n",
       "  '▁male': 14263,\n",
       "  'orus': 16566,\n",
       "  '/,': 19637,\n",
       "  'camera': 26065,\n",
       "  '▁Rou': 15915,\n",
       "  'Entity': 6691,\n",
       "  'NP': 25500,\n",
       "  'onom': 4917,\n",
       "  '▁alternative': 8671,\n",
       "  '▁opposition': 19626,\n",
       "  '▁theatre': 24520,\n",
       "  '▁Pack': 18744,\n",
       "  '▁The': 450,\n",
       "  'deg': 12163,\n",
       "  'atience': 24701,\n",
       "  'random': 8172,\n",
       "  '▁conocido': 25419,\n",
       "  'cookie': 21509,\n",
       "  'SSL': 18641,\n",
       "  '▁ships': 13968,\n",
       "  '?:': 25825,\n",
       "  'wür': 23970,\n",
       "  'aria': 4568,\n",
       "  '▁famille': 11449,\n",
       "  'Export': 26382,\n",
       "  '▁saying': 5934,\n",
       "  '▁duplicates': 20955,\n",
       "  'aur': 6698,\n",
       "  '▁districts': 24172,\n",
       "  'их': 15139,\n",
       "  'Ad': 3253,\n",
       "  '▁allemand': 22517,\n",
       "  '▁archive': 18871,\n",
       "  '▁aan': 4711,\n",
       "  'ϊ': 31832,\n",
       "  'rible': 11710,\n",
       "  '▁Sex': 21703,\n",
       "  'History': 20570,\n",
       "  '据': 30763,\n",
       "  'тан': 7951,\n",
       "  'aring': 4362,\n",
       "  '▁Рес': 17337,\n",
       "  'igny': 22771,\n",
       "  'жі': 29617,\n",
       "  'by': 1609,\n",
       "  'condition': 16122,\n",
       "  '▁folg': 10627,\n",
       "  '\"/': 23901,\n",
       "  '▁universitaire': 19270,\n",
       "  'inand': 16149,\n",
       "  '▁hely': 14827,\n",
       "  '▁suggestions': 10529,\n",
       "  '▁capabilities': 27108,\n",
       "  'oca': 6400,\n",
       "  '▁Graph': 12367,\n",
       "  'primary': 16072,\n",
       "  '▁Hibernate': 27772,\n",
       "  'Ə': 31127,\n",
       "  '∙': 31492,\n",
       "  'mund': 15282,\n",
       "  'cki': 22736,\n",
       "  '▁Grad': 19295,\n",
       "  'Dev': 16618,\n",
       "  '▁whether': 3692,\n",
       "  '▁media': 5745,\n",
       "  'MP': 3580,\n",
       "  '▁mart': 14436,\n",
       "  '▁bother': 24738,\n",
       "  'ong': 549,\n",
       "  '▁cloth': 13950,\n",
       "  'NI': 12916,\n",
       "  '▁zal': 21421,\n",
       "  'thers': 3341,\n",
       "  'an': 273,\n",
       "  'ywna': 20072,\n",
       "  '▁propre': 26138,\n",
       "  \"='\": 2433,\n",
       "  'ollary': 21982,\n",
       "  'ential': 2556,\n",
       "  'lace': 1265,\n",
       "  '{:': 25641,\n",
       "  'arth': 28696,\n",
       "  '▁coefficients': 16127,\n",
       "  '▁Nau': 26700,\n",
       "  '▁sue': 12991,\n",
       "  'ários': 26047,\n",
       "  'nitt': 9898,\n",
       "  'zec': 17938,\n",
       "  '▁Gar': 7455,\n",
       "  'landa': 25241,\n",
       "  '▁Wh': 806,\n",
       "  '▁Después': 25080,\n",
       "  'Ill': 10002,\n",
       "  'AN': 2190,\n",
       "  '▁Gruppe': 21757,\n",
       "  '▁actual': 3935,\n",
       "  '▁onClick': 14478,\n",
       "  '▁Hem': 20863,\n",
       "  '変': 31786,\n",
       "  '<0x94>': 151,\n",
       "  '▁органі': 26497,\n",
       "  '▁carry': 8677,\n",
       "  'digit': 26204,\n",
       "  '▁ainda': 23871,\n",
       "  '▁Apache': 13380,\n",
       "  'rot': 5450,\n",
       "  'aret': 10474,\n",
       "  'ieron': 10243,\n",
       "  'jsfiddle': 7822,\n",
       "  '▁ol': 13386,\n",
       "  '▁starb': 28414,\n",
       "  '▁ios': 10615,\n",
       "  '▁circumstances': 14209,\n",
       "  '们': 31381,\n",
       "  'uch': 987,\n",
       "  '▁той': 23879,\n",
       "  '▁AR': 9033,\n",
       "  'вала': 14906,\n",
       "  'се': 1502,\n",
       "  'ifts': 17741,\n",
       "  '▁potentially': 19998,\n",
       "  'door': 17433,\n",
       "  'hens': 26791,\n",
       "  '▁growth': 14321,\n",
       "  '▁Ide': 13001,\n",
       "  'setState': 24523,\n",
       "  '▁ł': 27518,\n",
       "  '▁Repub': 12804,\n",
       "  'шки': 15324,\n",
       "  'attributes': 15697,\n",
       "  '▁played': 5318,\n",
       "  '▁inter': 1006,\n",
       "  'зов': 11337,\n",
       "  '▁GUI': 14839,\n",
       "  'SG': 26016,\n",
       "  '▁Plus': 15113,\n",
       "  'edy': 7584,\n",
       "  '▁Filip': 21391,\n",
       "  '▁officer': 12139,\n",
       "  '▁Ł': 14124,\n",
       "  '▁forever': 22296,\n",
       "  'parameter': 15501,\n",
       "  'vek': 19134,\n",
       "  '▁offici': 29158,\n",
       "  '▁diversos': 22961,\n",
       "  '▁früher': 27509,\n",
       "  '▁LIM': 26890,\n",
       "  'expl': 24516,\n",
       "  'вших': 23265,\n",
       "  '▁rain': 17251,\n",
       "  '▁contempor': 12010,\n",
       "  'sn': 16586,\n",
       "  '▁Walter': 10705,\n",
       "  '▁Río': 25286,\n",
       "  'ча': 1282,\n",
       "  'xml': 3134,\n",
       "  '▁=\"': 29465,\n",
       "  'ു': 30442,\n",
       "  '▁Dam': 9865,\n",
       "  'age': 482,\n",
       "  'ziale': 28351,\n",
       "  'ähr': 5234,\n",
       "  '▁И': 2081,\n",
       "  'iona': 16017,\n",
       "  'Peter': 23686,\n",
       "  '▁@\"': 12490,\n",
       "  'odon': 28480,\n",
       "  '▁pill': 22549,\n",
       "  '▁keyword': 13553,\n",
       "  'endencies': 7158,\n",
       "  ']))': 12622,\n",
       "  '▁\\\\$': 20282,\n",
       "  '▁Ti': 18439,\n",
       "  'hyper': 24947,\n",
       "  '显': 31542,\n",
       "  'idenote': 26066,\n",
       "  \"▁'@\": 18803,\n",
       "  '▁PATH': 23611,\n",
       "  '▁hear': 8293,\n",
       "  'dire': 20146,\n",
       "  'burn': 18712,\n",
       "  'ゼ': 31718,\n",
       "  '▁sqlite': 21120,\n",
       "  'frica': 18961,\n",
       "  '台': 31037,\n",
       "  'ходи': 4647,\n",
       "  'lice': 5897,\n",
       "  '▁precise': 18378,\n",
       "  '▁Adding': 18804,\n",
       "  'which': 4716,\n",
       "  '▁Window': 18379,\n",
       "  '▁una': 1185,\n",
       "  'cler': 12121,\n",
       "  '▁obsc': 19726,\n",
       "  '▁erf': 23467,\n",
       "  '▁като': 12041,\n",
       "  'ements': 4110,\n",
       "  'isher': 22154,\n",
       "  '▁ду': 9023,\n",
       "  '▁Пра': 17913,\n",
       "  'ORS': 24125,\n",
       "  '▁rights': 10462,\n",
       "  'usch': 17974,\n",
       "  'iei': 27496,\n",
       "  '被': 31407,\n",
       "  '▁city': 4272,\n",
       "  '▁flags': 13449,\n",
       "  '▁sacred': 26546,\n",
       "  '▁individuals': 15724,\n",
       "  '▁outside': 5377,\n",
       "  '聖': 31581,\n",
       "  '▁rat': 7548,\n",
       "  '▁icons': 27673,\n",
       "  \"▁')\": 25710,\n",
       "  '▁///': 4363,\n",
       "  '▁окон': 17895,\n",
       "  '▁сай': 8347,\n",
       "  '▁Gr': 1632,\n",
       "  'registr': 29238,\n",
       "  'mq': 28466,\n",
       "  'compose': 19438,\n",
       "  '▁everybody': 26077,\n",
       "  'unkt': 11087,\n",
       "  '▁Platform': 28096,\n",
       "  '▁Rap': 16866,\n",
       "  'Μ': 30362,\n",
       "  '▁deren': 15286,\n",
       "  '▁fingers': 23915,\n",
       "  'igli': 11224,\n",
       "  'дела': 12761,\n",
       "  '▁Freder': 12294,\n",
       "  '▁paragraph': 14880,\n",
       "  'outh': 2438,\n",
       "  '▁communities': 23507,\n",
       "  '▁harm': 10311,\n",
       "  '▁path': 2224,\n",
       "  '▁führte': 21070,\n",
       "  '▁better': 2253,\n",
       "  '▁guarante': 10509,\n",
       "  'bbi': 14663,\n",
       "  '▁Factory': 27561,\n",
       "  'нец': 27720,\n",
       "  '▁проце': 14852,\n",
       "  '▁mentre': 11552,\n",
       "  'andal': 24258,\n",
       "  '▁evening': 11005,\n",
       "  '▁sehr': 13572,\n",
       "  '▁LaTeX': 29186,\n",
       "  '▁Amts': 29189,\n",
       "  '▁gemeente': 18124,\n",
       "  '▁гово': 29554,\n",
       "  '▁posible': 29125,\n",
       "  'oit': 20252,\n",
       "  'É': 30062,\n",
       "  '▁öff': 22113,\n",
       "  'slide': 19265,\n",
       "  '▁van': 1109,\n",
       "  '▁respectively': 8307,\n",
       "  'ǒ': 30971,\n",
       "  'cej': 24213,\n",
       "  '▁banks': 24388,\n",
       "  '▁Summer': 13329,\n",
       "  'ën': 28063,\n",
       "  'Photo': 25971,\n",
       "  'ení': 10639,\n",
       "  '▁religious': 12962,\n",
       "  'Constraints': 27427,\n",
       "  'ende': 3324,\n",
       "  '▁atoms': 28422,\n",
       "  '▁woj': 9237,\n",
       "  'andid': 5380,\n",
       "  '▁Dean': 23263,\n",
       "  '=$': 6080,\n",
       "  'Sync': 21077,\n",
       "  '▁Isaac': 28156,\n",
       "  '▁expensive': 19390,\n",
       "  'urls': 26045,\n",
       "  '▁plenty': 20947,\n",
       "  '▁freely': 28472,\n",
       "  '▁este': 4404,\n",
       "  '¥': 30563,\n",
       "  '▁conform': 14670,\n",
       "  'must': 21969,\n",
       "  'ele': 6146,\n",
       "  '▁ble': 10767,\n",
       "  '▁pit': 22754,\n",
       "  '▁Anyone': 26407,\n",
       "  '▁és': 2465,\n",
       "  'TeX': 16644,\n",
       "  'ava': 879,\n",
       "  '▁did': 1258,\n",
       "  '▁Els': 13144,\n",
       "  'National': 27325,\n",
       "  '▁Мон': 23796,\n",
       "  '▁Block': 15658,\n",
       "  '▁const': 1040,\n",
       "  '▁Mare': 21678,\n",
       "  '▁ambos': 27727,\n",
       "  '▁Kop': 27476,\n",
       "  'dotnet': 21328,\n",
       "  'pts': 16485,\n",
       "  'ename': 3871,\n",
       "  '▁tried': 1898,\n",
       "  '▁Catholic': 11865,\n",
       "  'OW': 9806,\n",
       "  'auss': 11214,\n",
       "  '▁prüfe': 21169,\n",
       "  '▁bringing': 20794,\n",
       "  'lices': 29399,\n",
       "  'arlo': 22431,\n",
       "  '▁entonces': 19665,\n",
       "  'comments': 21032,\n",
       "  'mina': 13257,\n",
       "  '▁находи': 14050,\n",
       "  '▁dead': 7123,\n",
       "  'Er': 2110,\n",
       "  '▁Christopher': 18888,\n",
       "  'тур': 6469,\n",
       "  ')}{': 10172,\n",
       "  'zeti': 21047,\n",
       "  '▁crypt': 24941,\n",
       "  'зидент': 27027,\n",
       "  'ame': 420,\n",
       "  'ssl': 16265,\n",
       "  'ждения': 18479,\n",
       "  '▁appearance': 10097,\n",
       "  'ucci': 27004,\n",
       "  '▁naturally': 18180,\n",
       "  '▁explan': 7309,\n",
       "  'вы': 4938,\n",
       "  'gress': 3663,\n",
       "  'essor': 6329,\n",
       "  'raum': 19527,\n",
       "  '▁систем': 20573,\n",
       "  '▁Durant': 27714,\n",
       "  '▁uninstall': 27608,\n",
       "  'scal': 19529,\n",
       "  '▁wider': 25734,\n",
       "  '▁Exception': 8960,\n",
       "  'рист': 28071,\n",
       "  'oslov': 24022,\n",
       "  '▁flight': 16286,\n",
       "  'Des': 4002,\n",
       "  '▁algebraic': 21531,\n",
       "  ')^{\\\\': 21422,\n",
       "  'ets': 1691,\n",
       "  '▁various': 5164,\n",
       "  '▁Ir': 6600,\n",
       "  '▁<>': 15271,\n",
       "  '▁одна': 15295,\n",
       "  'lit': 19411,\n",
       "  'atge': 26775,\n",
       "  'student': 18945,\n",
       "  '▁sv': 3731,\n",
       "  '▁zwar': 26979,\n",
       "  '▁Sax': 18574,\n",
       "  '▁violence': 21448,\n",
       "  'leted': 22742,\n",
       "  '▁edited': 8788,\n",
       "  '▁implemented': 8762,\n",
       "  '▁soap': 29559,\n",
       "  '定': 30495,\n",
       "  'чный': 19705,\n",
       "  '▁Customer': 21886,\n",
       "  'cout': 13147,\n",
       "  '▁geslacht': 25786,\n",
       "  '▁pt': 19592,\n",
       "  '▁spirit': 8548,\n",
       "  'Datos': 6086,\n",
       "  '▁anterior': 14123,\n",
       "  'Application': 4873,\n",
       "  '▁todo': 10481,\n",
       "  'antry': 15328,\n",
       "  '▁lady': 11379,\n",
       "  '▁europé': 17611,\n",
       "  'estig': 5286,\n",
       "  'note': 6812,\n",
       "  'cha': 5815,\n",
       "  '▁сту': 13224,\n",
       "  'though': 3592,\n",
       "  'maste': 19535,\n",
       "  'จ': 30991,\n",
       "  'entlich': 11280,\n",
       "  'onk': 23345,\n",
       "  '▁presso': 20885,\n",
       "  '▁Durante': 13912,\n",
       "  '▁desired': 7429,\n",
       "  '▁zast': 21893,\n",
       "  'hill': 29131,\n",
       "  'dienst': 26293,\n",
       "  'タ': 30369,\n",
       "  '▁NAS': 16938,\n",
       "  'oggle': 9804,\n",
       "  '▁combination': 10296,\n",
       "  '▁extreme': 18677,\n",
       "  'enst': 25059,\n",
       "  '▁gan': 9581,\n",
       "  'OP': 4590,\n",
       "  '▁built': 4240,\n",
       "  'хі': 22177,\n",
       "  '▁github': 18546,\n",
       "  '▁dominant': 28526,\n",
       "  '▁twice': 8951,\n",
       "  '▁valu': 17134,\n",
       "  '▁czł': 21390,\n",
       "  '▁soll': 10297,\n",
       "  '▁lit': 11872,\n",
       "  '▁Matrix': 22513,\n",
       "  'extend': 21843,\n",
       "  '▁apart': 12435,\n",
       "  'akers': 21079,\n",
       "  '(\"': 703,\n",
       "  'expect': 17854,\n",
       "  'Liter': 24938,\n",
       "  'platz': 16739,\n",
       "  'rating': 29741,\n",
       "  'iterator': 17609,\n",
       "  '▁wieku': 21777,\n",
       "  'altern': 26123,\n",
       "  '▁laws': 14243,\n",
       "  'Q': 29984,\n",
       "  'err': 3127,\n",
       "  '▁bless': 17065,\n",
       "  '▁сооб': 25032,\n",
       "  '▁oper': 1751,\n",
       "  '真': 30848,\n",
       "  '▁yourself': 7535,\n",
       "  '▁jelent': 23962,\n",
       "  '▁administ': 28777,\n",
       "  'メ': 30604,\n",
       "  'CAT': 23972,\n",
       "  'cation': 9252,\n",
       "  'vid': 8590,\n",
       "  '▁Proposition': 22206,\n",
       "  'èque': 5949,\n",
       "  'ksam': 29107,\n",
       "  'GEN': 24647,\n",
       "  '▁th': 266,\n",
       "  '▁modified': 9120,\n",
       "  'cowo': 18221,\n",
       "  'hui': 15669,\n",
       "  '▁Flash': 21967,\n",
       "  'ási': 23884,\n",
       "  '▁ад': 11437,\n",
       "  '▁den': 972,\n",
       "  '到': 30780,\n",
       "  '\"+': 17969,\n",
       "  '▁Gree': 18438,\n",
       "  'circle': 16622,\n",
       "  '▁combin': 5769,\n",
       "  '▁lect': 13081,\n",
       "  'шње': 14572,\n",
       "  '▁nab': 26924,\n",
       "  'ције': 23269,\n",
       "  '▁związ': 27859,\n",
       "  'ق': 30265,\n",
       "  '▁zawod': 25425,\n",
       "  'ціональ': 16839,\n",
       "  '▁três': 26299,\n",
       "  '▁ever': 3926,\n",
       "  '▁entr': 9953,\n",
       "  '▁gepubliceerd': 10711,\n",
       "  'merge': 14634,\n",
       "  '▁mig': 29542,\n",
       "  '▁Mode': 21864,\n",
       "  '▁GB': 19289,\n",
       "  '▁a': 263,\n",
       "  'autre': 18288,\n",
       "  'esh': 12094,\n",
       "  'რ': 30456,\n",
       "  'itudes': 20816,\n",
       "  'ality': 2877,\n",
       "  '▁Nä': 28615,\n",
       "  'älle': 23637,\n",
       "  'SA': 8132,\n",
       "  '▁элект': 22876,\n",
       "  '▁Han': 7169,\n",
       "  '▁Remove': 15154,\n",
       "  '▁Marse': 29626,\n",
       "  '▁aud': 12990,\n",
       "  'borg': 14203,\n",
       "  'ToString': 8246,\n",
       "  'oku': 9154,\n",
       "  'uster': 5402,\n",
       "  'izations': 17063,\n",
       "  'iale': 4379,\n",
       "  '▁Abr': 27782,\n",
       "  'empt': 3456,\n",
       "  '▁меда': 18922,\n",
       "  '........': 11296,\n",
       "  '▁aj': 13612,\n",
       "  '特': 31141,\n",
       "  '▁produce': 7738,\n",
       "  '▁apparent': 20295,\n",
       "  'Namespace': 23335,\n",
       "  '▁damit': 14733,\n",
       "  '▁spel': 18389,\n",
       "  '▁served': 6766,\n",
       "  'liament': 9745,\n",
       "  'じ': 31115,\n",
       "  '½': 30226,\n",
       "  '▁Legisl': 18991,\n",
       "  '▁clause': 11845,\n",
       "  'дол': 15906,\n",
       "  '▁nieder': 23634,\n",
       "  'elij': 3875,\n",
       "  '▁Iss': 16982,\n",
       "  'isser': 29415,\n",
       "  'prev': 16304,\n",
       "  '▁belang': 25585,\n",
       "  '▁container': 5639,\n",
       "  'yar': 8553,\n",
       "  'oms': 4835,\n",
       "  '▁Glad': 19319,\n",
       "  'cx': 18904,\n",
       "  '▁Barbara': 18506,\n",
       "  ']$.': 22689,\n",
       "  'стов': 10148,\n",
       "  '▁filling': 27523,\n",
       "  'Hz': 12661,\n",
       "  '▁belief': 17750,\n",
       "  '▁ubic': 18256,\n",
       "  'ervation': 20525,\n",
       "  '▁par': 610,\n",
       "  'ary': 653,\n",
       "  'light': 4366,\n",
       "  'ை': 31428,\n",
       "  'нар': 19619,\n",
       "  '▁Brig': 13051,\n",
       "  'clk': 20495,\n",
       "  ...},\n",
       " 32000)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "690e4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "705f046c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aed6f683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "LlamaForCausalLM                                   --\n",
       "├─LlamaModel: 1-1                                  --\n",
       "│    └─Embedding: 2-1                              65,536,000\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-2                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-3                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-4                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-5                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-6                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-7                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-8                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-9                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-10                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-11                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-12                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-13                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-14                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-15                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-16                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-17                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-18                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-19                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-20                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-21                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-22                22,024,192\n",
       "│    └─LlamaRMSNorm: 2-3                           2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                   --\n",
       "├─Linear: 1-2                                      65,536,000\n",
       "===========================================================================\n",
       "Total params: 615,606,272\n",
       "Trainable params: 131,164,160\n",
       "Non-trainable params: 484,442,112\n",
       "==========================================================================="
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e055ffaa",
   "metadata": {},
   "source": [
    "Vamos a crear una función que nos genera la salida bruta (input+output) y la salida neta (output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b3d339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, prompt, max_length, max_new_tokens):\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(device)\n",
    "\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, generation_config=gen_config)\n",
    "\n",
    "    # Full decoded output (prompt + generated)\n",
    "    full_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Only the continuation (generated tokens after the prompt)\n",
    "    generated_ids = output[0][inputs[\"input_ids\"].shape[1] :]\n",
    "    generated_decoded = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return full_decoded, generated_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f610d735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k, backend):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar documents using selected backend:\n",
    "        - 'sklearn' : NearestNeighbors (your current version)\n",
    "        - 'faiss'   : FAISS IndexFlatIP\n",
    "        - 'st'      : sentence_transformers.util.semantic_search\n",
    "    \"\"\"\n",
    "    q_emb = embedder.encode([question], convert_to_numpy=True, normalize_embeddings=True)\n",
    "\n",
    "    # 1) Scikit-Learn NearestNeighbors\n",
    "    if backend == \"sklearn\":\n",
    "        distances, indices = nn_index.kneighbors(q_emb, n_neighbors=k)\n",
    "        return [corpus_docs[i] for i in indices[0]]\n",
    "    \n",
    "    # 2) FAISS (cosine via inner product)\n",
    "    elif backend == \"faiss\":\n",
    "        q = q_emb.astype(\"float32\")\n",
    "        faiss.normalize_L2(q)\n",
    "        distances, indices = faiss_index.search(q, k)\n",
    "        return [corpus_docs[i] for i in indices[0]]\n",
    "\n",
    "    # 3) SentenceTransformers semantic search\n",
    "    elif backend == \"st\":\n",
    "        hits = util.semantic_search(q_emb, doc_embeddings, top_k=k)[0]\n",
    "        return [corpus_docs[hit[\"corpus_id\"]] for hit in hits]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown retrieval backend: {backend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "238b1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, tokenizer, contexts):\n",
    "    context_text = contexts[0]\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant specialized in technology news.\n",
    "    Use ONLY the context below to answer the user question.\n",
    "    If the answer is not in the context, say I don't know.\n",
    "    Answer one short sentence. Do NOT repeat context or question\n",
    "    \\n  Question: {question}\n",
    "    \\n  Context: {context_text}\n",
    "    \\n Answer:\n",
    "    \"\"\"\n",
    "    count = len(tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "    return prompt, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "35cc63c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(model, tokenizer, question, k, max_length, max_new_tokens, backend):\n",
    "    \"\"\"\n",
    "    Full RAG flow:\n",
    "    - Retrieve similar docs\n",
    "    - Build ChatML prompt with context\n",
    "    - Generate answer with the LLM\n",
    "    \"\"\"\n",
    "    contexts = retrieve_context(question, k, backend)\n",
    "    prompt, token_count = build_prompt(question, tokenizer, contexts)\n",
    "    full_output, gen_output = generate_answer(model, tokenizer, prompt, max_length, max_new_tokens)\n",
    "    return prompt, full_output, gen_output, contexts, token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11820608",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which company open-sourced Llama-based models?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "52a98110",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, raw_answer, answer, ctx, token_count = rag_answer(\n",
    "    base_model, tokenizer, question, 3, 256, 128, \"st\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a203a6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \n",
      "    You are a helpful assistant specialized in technology news.\n",
      "    Use ONLY the context below to answer the user question.\n",
      "    If the answer is not in the context, say I don't know.\n",
      "    Answer one short sentence. Do NOT repeat context or question\n",
      "    \n",
      "  Question: Which company open-sourced Llama-based models?\n",
      "    \n",
      "  Context: Meta open-sourced a set of Llama-based models with billions of parameters, \n",
      "    enabling researchers and companies to fine-tune them for their own use cases.\n",
      "    \n",
      " Answer:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrompt:\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f2d7bcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt token count: 135\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt token count:\", token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1f01ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Contexts:\n",
      "\n",
      "--- Context 1 ---\n",
      "Meta open-sourced a set of Llama-based models with billions of parameters, \n",
      "    enabling researchers and companies to fine-tune them for their own use cases. \n",
      "\n",
      "--- Context 2 ---\n",
      "Hugging Face launched a new inference API tier with higher throughput and native \n",
      "    support for vLLM, making it cheaper to serve models like Mistral-7B and Llama-3-8B. \n",
      "\n",
      "--- Context 3 ---\n",
      "Apple reportedly began testing on-device LLMs for future iPhone models, enabling \n",
      "    private AI features such as offline summarization and personal context reasoning. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieved Contexts:\\n\")\n",
    "for i, c in enumerate(ctx, 1):\n",
    "    print(f\"--- Context {i} ---\")\n",
    "    print(c.strip(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e767601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw RAG Answer: \n",
      "    You are a helpful assistant specialized in technology news.\n",
      "    Use ONLY the context below to answer the user question.\n",
      "    If the answer is not in the context, say I don't know.\n",
      "    Answer one short sentence. Do NOT repeat context or question\n",
      "    \n",
      "  Question: Which company open-sourced Llama-based models?\n",
      "    \n",
      "  Context: Meta open-sourced a set of Llama-based models with billions of parameters, \n",
      "    enabling researchers and companies to fine-tune them for their own use cases.\n",
      "    \n",
      " Answer:\n",
      "     Meta is an American multinational technology company that develops artificial intelligence and machine learning technologies.\n",
      "     Llama is a type of deep learning model that is popular in the field of natural language processing.\n",
      "     Open-sourcing Llama-based models is a significant step towards making AI more accessible to researchers and companies.\n",
      "     The models can be used for a wide range of applications, including natural language processing, speech recognition, and image classification.\n",
      "     The open-sourcing of these models is a significant step towards making AI more accessible to researchers and companies.\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw RAG Answer:\", raw_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50d04d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Answer:\n",
      " Meta is an American multinational technology company that develops artificial intelligence and machine learning technologies.\n",
      "     Llama is a type of deep learning model that is popular in the field of natural language processing.\n",
      "     Open-sourcing Llama-based models is a significant step towards making AI more accessible to researchers and companies.\n",
      "     The models can be used for a wide range of applications, including natural language processing, speech recognition, and image classification.\n",
      "     The open-sourcing of these models is a significant step towards making AI more accessible to researchers and companies.\n"
     ]
    }
   ],
   "source": [
    "print('RAG Answer:\\n', answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21e67a",
   "metadata": {},
   "source": [
    "Veamos algo interesante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "201e2a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which company open-sourced Llama-based models?'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d56abbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, question, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "36a846cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f527fa",
   "metadata": {},
   "source": [
    "Usemos de nuevo el prompt anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c486d2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    You are a helpful assistant specialized in technology news.\\n    Use ONLY the context below to answer the user question.\\n    If the answer is not in the context, say I don't know.\\n    Answer one short sentence. Do NOT repeat context or question\\n    \\n  Question: Which company open-sourced Llama-based models?\\n    \\n  Context: Meta open-sourced a set of Llama-based models with billions of parameters, \\n    enabling researchers and companies to fine-tune them for their own use cases.\\n    \\n Answer:\\n    \""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2f227fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " Meta open-sourced a set of Llama-based models with billions of parameters, enabling researchers and companies to fine-tune them for their own use cases.\n",
      "     The models are based on the Llama architecture, which is a popular machine learning framework.\n",
      "     The models are open-sourced to enable researchers and companies to use them for their own use cases.\n",
      "     The context is a specific use case, so the answer is not general knowledge.\n"
     ]
    }
   ],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, prompt, 256, 128)\n",
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5d22b",
   "metadata": {},
   "source": [
    "Creamos un nuevo prompt que tenga ordenes, pregunta y espacio para respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dbdf1ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "ONLY answer the question below. Do NOT repeat the question below in the answer.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4881867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " The answer is Google.\n"
     ]
    }
   ],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, prompt, 256, 128)\n",
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b5197",
   "metadata": {},
   "source": [
    "Veamos esto que también es interesante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20e4b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"{question}:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0bdbb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which company open-sourced Llama-based models?:'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9157fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " The Llama project, which aims to create a standard for building machine learning models, has opened up its source code to the public. The project, which was founded by Google's DeepMind AI research group, has been working on the project for the past year. The Llama project is a collection of open-source models that are designed to be easy to use and to be able to be trained on a variety of data sets. The project is focused on building models that can be used for a variety of tasks, including natural language processing, image recognition, and recommendation systems. The project is open-s\n"
     ]
    }
   ],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, prompt, 256, 128)\n",
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf294f9",
   "metadata": {},
   "source": [
    "Veamos ahora como hacer fine tuning con un toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bc0f91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = [\n",
    "    {\n",
    "        \"instruction\": \"Which company released a new reasoning model optimized for tool use and RAG?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"OpenAI released a new model optimized for tool use and retrieval-augmented generation.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What did Google update to make it easier to deploy large language models?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Google updated its Vertex AI platform to make it easier to deploy and monitor large language models.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Which company open-sourced Llama-based models and why is this important?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Meta open-sourced Llama-based models, enabling researchers and companies to fine-tune them for their own use cases.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What AI features did Microsoft add to Office?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Microsoft added generative AI features to Office, including AI-powered summarization, drafting, and meeting notes.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What did AWS introduce for inference workloads?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"AWS introduced cheaper GPU instances optimized for inference workloads like chatbots, code assistants, and document question-answering.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What did NVIDIA release to accelerate transformer inference?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"NVIDIA released open-source libraries that accelerate transformer inference, improving performance on consumer GPUs.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What research focus did Anthropic publish new work on?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Anthropic published research on improving constitutional AI with scalable oversight and safer model behavior.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What AI capability is Apple reportedly testing for iPhones?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Apple is testing on-device LLMs that enable private offline capabilities like summarization and personal context reasoning.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What did Hugging Face launch to improve model serving?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Hugging Face launched a new inference API tier with high throughput and native vLLM support.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"Which company released the Mixtral-8x22B model and what type of model is it?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Mistral AI released Mixtral-8x22B, a sparse mixture-of-experts model that provides state-of-the-art performance efficiently.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What partnership did IBM announce involving geospatial data?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"IBM partnered with NASA to fine-tune foundation models on geospatial data to improve climate and satellite analysis.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What model did Databricks release and what is notable about it?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Databricks released DBRX, a 132B-parameter MoE model trained on curated scientific and enterprise datasets.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What did Stability AI introduce with improved text-image alignment?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Stability AI introduced Stable Diffusion 3, offering better text-image alignment and reduced hallucinations.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What new capability did Snowflake add for enterprise AI pipelines?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Snowflake added native vector search, enabling RAG workflows directly within its data warehouse.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What product did Cohere launch for retrieval and semantic search?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Cohere launched an enterprise-grade embedding model designed for semantic search and multilingual retrieval.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What AI enhancement did Red Hat introduce for DevOps workflows?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Red Hat introduced AI-enhanced DevOps tools, including automated deployment validation powered by small LLMs.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What updates did Salesforce make to Einstein GPT?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Salesforce updated Einstein GPT with improved CRM reasoning, including lead scoring and automated email drafting.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What AI feature did Dropbox add to help users navigate their files?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Dropbox added AI-powered universal search that allows semantic querying across documents, PDFs, and images.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How is Slack using AI to help teams stay informed?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Slack added AI summarization for channels and threads, generating digests and extracting key decisions.\",\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"What real-time AI capabilities did Zoom add to its platform?\",\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"Zoom added real-time translation and AI-generated meeting action items powered by a multilingual transformer model.\",\n",
    "    },\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b7b20ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_list(qa_pairs)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aca8cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    # Alpaca-style formatting\n",
    "    if example[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction and an input. Write a helpful answer.\n",
    "\n",
    "### Instruction:\n",
    "{example[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{example[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{example[\"output\"]}\n",
    "\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction. Write a helpful answer.\n",
    "\n",
    "### Instruction:\n",
    "{example[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{example[\"output\"]}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    text = format_example(example)\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # For causal LM, labels = input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db21f402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6665c8b8242849828c06e9f11c956b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "# Remove the original text columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
    "\n",
    "# Set format\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e04c01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    1, 13866,   338,   385, 15278, 29889, 14350,   263,  8444,  1234,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  8809,\n",
       "           436,  5001,  5492,   263,   716, 24481,  1904, 27545,   363,  5780,\n",
       "           671,   322,   390, 10051, 29973,    13,    13,  2277, 29937, 13291,\n",
       "         29901,    13,  6585, 23869,  5492,   263,   716,  1904, 27545,   363,\n",
       "          5780,   671,   322,  5663, 16837, 29899,  2987,   358,   287, 12623,\n",
       "         29889,    13,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([    1, 13866,   338,   385, 15278, 29889, 14350,   263,  8444,  1234,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  8809,\n",
       "           436,  5001,  5492,   263,   716, 24481,  1904, 27545,   363,  5780,\n",
       "           671,   322,   390, 10051, 29973,    13,    13,  2277, 29937, 13291,\n",
       "         29901,    13,  6585, 23869,  5492,   263,   716,  1904, 27545,   363,\n",
       "          5780,   671,   322,  5663, 16837, 29899,  2987,   358,   287, 12623,\n",
       "         29889,    13,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84f24d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3267f3",
   "metadata": {},
   "source": [
    "Definimos un nuevo modelo a realizar FT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e025bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, quantization_config=bnb_config, device_map=\"auto\", use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3cb673ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"k_proj\",\n",
    "        \"o_proj\",\n",
    "    ],  # may need to adjust per model\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d26d1b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for k-bit training (LoRA on top of 4-bit base)\n",
    "ft_model.to(device).train()\n",
    "ft_model = prepare_model_for_kbit_training(ft_model)\n",
    "ft_model = get_peft_model(ft_model, lora_config)\n",
    "ft_model.gradient_checkpointing_enable(\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "ft_model.enable_input_require_grads()\n",
    "ft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce2a0727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "PeftModelForCausalLM                                              --\n",
       "├─LoraModel: 1-1                                                  --\n",
       "│    └─LlamaForCausalLM: 2-1                                      --\n",
       "│    │    └─LlamaModel: 3-1                                       552,323,072\n",
       "│    │    └─Linear: 3-2                                           (65,536,000)\n",
       "==========================================================================================\n",
       "Total params: 617,859,072\n",
       "Trainable params: 2,252,800\n",
       "Non-trainable params: 615,606,272\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f35a6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(ft_model.parameters(), lr=1e-3, amsgrad=True, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ae04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 1 | Loss: 14.5722\n",
      "Epoch 1 | Step 2 | Loss: 8.6726\n",
      "Epoch 1 | Step 3 | Loss: 3.9434\n",
      "Epoch 1 | Step 4 | Loss: 1.1957\n",
      "Epoch 1 | Step 5 | Loss: 0.9001\n",
      "Epoch 1 | Step 6 | Loss: 0.9521\n",
      "Epoch 1 | Step 7 | Loss: 0.8895\n",
      "Epoch 1 | Step 8 | Loss: 0.9358\n",
      "Epoch 1 | Step 9 | Loss: 0.9274\n",
      "Epoch 1 | Step 10 | Loss: 0.8169\n",
      "== Epoch 1 finished | Avg loss: 3.3806 ==\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# batch is a dict of tensors with shape [batch_size, seq_len]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m----> 8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/peft/peft_model.py:1850\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1848\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1849\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1850\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1853\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1855\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1858\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1859\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1861\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1863\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:222\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    441\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/generic.py:1064\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 395\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    407\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    408\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    409\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/modeling_layers.py:93\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m         message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mrstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_compile.py:53\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive, wrapping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:1044\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1046\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/utils/checkpoint.py:503\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, early_stop, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# Runs pre-forward logic\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28mnext\u001b[39m(gen)\n\u001b[0;32m--> 503\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# Runs post-forward logic\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:294\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:264\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    254\u001b[0m     query_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    261\u001b[0m )\n\u001b[1;32m    263\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 264\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:504\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     output \u001b[38;5;241m=\u001b[39m lora_B(lora_A(dropout(x))) \u001b[38;5;241m*\u001b[39m scaling\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m requires_conversion:\n\u001b[0;32m--> 504\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpected_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m     result \u001b[38;5;241m=\u001b[39m result \u001b[38;5;241m+\u001b[39m output\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # batch is a dict of tensors with shape [batch_size, seq_len]\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = ft_model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Step {step+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"== Epoch {epoch+1} finished | Avg loss: {avg_loss:.4f} ==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c96534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapter to: ./tinyllama-tech-lora\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./tinyllama-tech-lora\"\n",
    "ft_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Saved LoRA adapter to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9d1f839d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-21): 22 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bdc37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Question ===\n",
      "Which company open-sourced Llama-based models?\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "56ba5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_answer, answer = zero_shot_answer(ft_model, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c6b619d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Baseline (no RAG, no FT) ---\n",
      "Yes, Llama is an open-source project that allows developers to use the Llama library for building models.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Baseline (no RAG, no FT) ---\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b9a66f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAG (no FT) ---\n",
      "Meta is an American multinational technology company that specializes in artificial intelligence.\n",
      "     Llama is a type of machine learning algorithm that is popular in the field of natural language processing.\n",
      "     Open-sourcing means that the code is available for others to use, modify, and improve.\n",
      "     Meta's Llama-based models are a set of pre-trained language models that can be fine-tuned for specific tasks.\n",
      "     The models are available for researchers and companies to use for their own purposes.\n",
      "     Meta's open-sourcing of Llama-\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- RAG (no FT) ---\")\n",
    "prompt, full_output, gen_output, contexts, token_count = rag_answer(\n",
    "    base_model, tokenizer, question, 3, 256, 128, 'faiss'\n",
    ")\n",
    "print(gen_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "79fbd5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-tuned + RAG ---\n",
      "Meta is an American multinational technology company that develops and sells social media platforms, including Facebook, Instagram, and WhatsApp.\n",
      "     Llama is a machine learning framework developed by Google that is used for training large neural networks.\n",
      "     Open-sourcing Llama-based models is a significant step towards making AI more accessible to researchers and companies.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Fine-tuned + RAG ---\")\n",
    "_, raw_answer, answer, _, _ = rag_answer(\n",
    "    ft_model, tokenizer, question, 3, 256, 128, \"faiss\"\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef0e63",
   "metadata": {},
   "source": [
    "Ahora, vamos a obtener noticias de la API de hacker-news"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3d1db8c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b3146893",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def extract_hn_articles_bs(max_items: int = 10, min_score: int = 20):\n",
    "    \"\"\"\n",
    "    Extract tech-ish article text from Hacker News using BeautifulSoup.\n",
    "    Returns a list of article dicts:\n",
    "    {\n",
    "        \"id\": <hn_id>,\n",
    "        \"title\": <title>,\n",
    "        \"url\": <url>,\n",
    "        \"score\": <hn_score>,\n",
    "        \"text\": <full_page_text>\n",
    "    }\n",
    "    \"\"\"\n",
    "    ids = requests.get(\"https://hacker-news.firebaseio.com/v0/newstories.json\").json()\n",
    "    \n",
    "    articles = []\n",
    "\n",
    "    for story_id in ids:\n",
    "        item = requests.get(\n",
    "            f\"https://hacker-news.firebaseio.com/v0/item/{story_id}.json\"\n",
    "        ).json()\n",
    "\n",
    "        if not item:\n",
    "            continue\n",
    "\n",
    "        score = item.get(\"score\", 0)\n",
    "        url = item.get(\"url\")\n",
    "        title = item.get(\"title\", \"\")\n",
    "\n",
    "        # Need a URL + minimum score\n",
    "        if not url or score < min_score:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=15)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        if resp.status_code != 200 or \"text/html\" not in resp.headers.get(\n",
    "            \"Content-Type\", \"\"\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # crude but workable: join all paragraph text\n",
    "        paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n",
    "        text = \"\\n\".join(p for p in paragraphs if p)\n",
    "\n",
    "        # skip very short pages\n",
    "        if len(text) < 500:\n",
    "            continue\n",
    "\n",
    "        articles.append(\n",
    "            {\n",
    "                \"id\": story_id,\n",
    "                \"title\": title,\n",
    "                \"url\": url,\n",
    "                \"score\": score,\n",
    "                \"text\": text,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if len(articles) >= max_items:\n",
    "            break\n",
    "\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc8ca558",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "articles = extract_hn_articles_bs(max_items=5,min_score=20)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e9dd32f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def save_articles_json(articles, filepath):\n",
    "    \"\"\"\n",
    "    Save list of article dicts to a JSON file (array of objects).\n",
    "    \"\"\"\n",
    "    path = Path(filepath)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(articles)} articles to {filepath}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46a28f81",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "save_articles_json(articles, \"Data/hn_articles.json\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74bdbae9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def load_articles_json(filepath: str):\n",
    "    \"\"\"\n",
    "    Load the list of article dicts from a JSON file.\n",
    "    \"\"\"\n",
    "    path = Path(filepath)\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        articles = json.load(f)\n",
    "    print(f\"Loaded {len(articles)} articles from {filepath}\")\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "raw",
   "id": "586f8215",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "articles = load_articles_json(\"Data/hn_articles.json\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "154f099d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6738839",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "for i in range(len(articles)):\n",
    "    print(f'Article {i}: {len(articles[i]['text'])}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82390bc6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73884203",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def window_text(text, tokenizer, max_tokens=1000, overlap=100):\n",
    "    \"\"\"\n",
    "    Split an article into windows of ~1000 tokens with some overlap.\n",
    "    \"\"\"\n",
    "    enc = tokenizer.encode(text, add_special_tokens=False)\n",
    "    print(len(enc))\n",
    "    windows = []\n",
    "\n",
    "    start = 0\n",
    "    while start < len(enc):\n",
    "        end = start + max_tokens\n",
    "        chunk = enc[start:end]\n",
    "        windows.append(tokenizer.decode(chunk))\n",
    "        start = end - overlap  # move with overlap\n",
    "\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf9d46d6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "len(window_text(articles[0]['text'], tokenizer))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3105680c",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def summarize_chunk(model, tokenizer, chunk):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following text in a factual, concise way. Max 5 sentences.\n",
    "\n",
    "Text:\n",
    "\\\"\\\"\\\"{chunk}\\\"\\\"\\\"\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "    summary = generate_answer(model, tokenizer, prompt, max_new_tokens=256)\n",
    "    return summary.strip()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e61ddcc8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def summarize_article(model, tokenizer, article_text):\n",
    "    \"\"\"\n",
    "    Summarize an arbitrarily long article using 1000-token windows.\n",
    "    Returns a final short global summary.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Split into windows\n",
    "    windows = window_text(article_text, tokenizer, max_tokens=1000, overlap=100)\n",
    "\n",
    "    print(f\"   - Article length: {len(article_text)} chars\")\n",
    "    print(f\"   - Windows created: {len(windows)}\")\n",
    "\n",
    "    # 2) Summaries for each window\n",
    "    window_summaries = []\n",
    "    for i, win in enumerate(windows):\n",
    "        print(f\"      Summarizing window {i+1}/{len(windows)}...\")\n",
    "        s = summarize_chunk(model, tokenizer, win)\n",
    "        window_summaries.append(s)\n",
    "\n",
    "    # 3) Final summary combining all partial summaries\n",
    "    combined = \"\\n\".join(window_summaries)\n",
    "\n",
    "    final_prompt = f\"\"\"\n",
    "Combine the following partial summaries into one unified, concise summary.\n",
    "Keep it under 8 sentences. Avoid repetition.\n",
    "\n",
    "Partial summaries:\n",
    "\\\"\\\"\\\"{combined}\\\"\\\"\\\"\n",
    "\n",
    "Final summary:\n",
    "\"\"\"\n",
    "\n",
    "    final_summary = generate_answer(model, tokenizer, final_prompt, max_new_tokens=256)\n",
    "    return final_summary.strip()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c07a6fd",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "summary = summarize_article(base_model, tokenizer, articles[0][\"text\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "847233fc",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ba1755",
   "metadata": {},
   "source": [
    "Vamos a realizar chunking. Hay muchas estrategias."
   ]
  },
  {
   "cell_type": "raw",
   "id": "64d25cfb",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def chunk_text(tokenizer, text, max_tokens=256, overlap=96):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    chunks = []\n",
    "\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens)) \n",
    "        chunk = tokens[start:end]\n",
    "        chunks.append(tokenizer.decode(chunk))\n",
    "\n",
    "        start += max_tokens - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "raw",
   "id": "596b9bd1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "chunks = chunk_text(tokenizer, articles[1][\"text\"], max_tokens=800, overlap=200)\n",
    "print(\"Number of chunks:\", len(chunks))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60048408",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c623b5a8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def generate_qa_for_article(model, tokenizer, article, num_questions):\n",
    "    \"\"\"\n",
    "    Use teacher model to generate num_questions Q/A pairs from a single article.\n",
    "    Expected output format (plain text):\n",
    "\n",
    "    Q: ...\n",
    "    A: ...\n",
    "    Q: ...\n",
    "    A: ...\n",
    "\n",
    "    Returns a list of dicts with keys:\n",
    "    - article_id\n",
    "    - article_title\n",
    "    - instruction\n",
    "    - input\n",
    "    - output\n",
    "    \"\"\"\n",
    "    article_text = article[\"text\"]\n",
    "    article_id = article[\"id\"]\n",
    "    article_title = article[\"title\"]\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are an AI that generates question-answer pairs from news articles. \"\n",
    "        \"You MUST follow the formatting instructions exactly.\"\n",
    "    )\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "Article:\n",
    "\\\"\\\"\\\"{article_text}\\\"\\\"\\\"\n",
    "\n",
    "Task:\n",
    "Generate {num_questions} question-answer pairs that a user might ask about this article.\n",
    "\n",
    "CRITICAL FORMAT RULES:\n",
    "- Use ONLY information from the article.\n",
    "- Do NOT mention these instructions in your output.\n",
    "- Do NOT add explanations or commentary.\n",
    "- Output ONLY lines in this exact pattern:\n",
    "\n",
    "Q: <question 1>\n",
    "A: <answer 1>\n",
    "Q: <question 2>\n",
    "A: <answer 2>\n",
    "Q: <question 3>\n",
    "A: <answer 3>\n",
    "...\n",
    "\n",
    "Do NOT output JSON.\n",
    "Do NOT number the questions as \"1.\" or \"(1)\".\n",
    "Do NOT write anything before the first 'Q:' or after the last 'A:'.\n",
    "\"\"\"\n",
    "\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    raw_output = generate_answer(model, tokenizer, full_prompt, max_new_tokens=512)\n",
    "\n",
    "    # --- PARSING ---\n",
    "    qa_pairs = []\n",
    "    current_q = None\n",
    "\n",
    "    for raw_line in raw_output.splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Accept 'Q:' or 'Q1:' or 'Q1.' etc.\n",
    "        q_match = re.match(r\"^Q[:\\d\\.\\)\\- ]+(.*)$\", line)\n",
    "        a_match = re.match(r\"^A[:\\d\\.\\)\\- ]+(.*)$\", line)\n",
    "\n",
    "        if q_match:\n",
    "            question = q_match.group(1).strip()\n",
    "            if question:\n",
    "                current_q = question\n",
    "\n",
    "        elif a_match and current_q is not None:\n",
    "            answer = a_match.group(1).strip()\n",
    "            if answer:\n",
    "                qa_pairs.append(\n",
    "                    {\n",
    "                        \"article_id\": article_id,\n",
    "                        \"article_title\": article_title,\n",
    "                        \"instruction\": current_q,\n",
    "                        \"input\": \"\",\n",
    "                        \"output\": answer,\n",
    "                    }\n",
    "                )\n",
    "                current_q = None  # reset for next Q/A\n",
    "\n",
    "    # Clip to requested amount\n",
    "    if len(qa_pairs) > num_questions:\n",
    "        qa_pairs = qa_pairs[:num_questions]\n",
    "\n",
    "    if len(qa_pairs) == 0:\n",
    "        print(\n",
    "            f\"[WARN] Parsed 0 Q/A pairs for article {article_id} - showing output snippet:\"\n",
    "        )\n",
    "        print(raw_output[:500], \"...\\n\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Parsed {len(qa_pairs)} Q/A pairs for article {article_id}\")\n",
    "\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd61cca6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "all_qa_pairs = []\n",
    "\n",
    "for idx, article in enumerate(articles):\n",
    "    print(f\"\\n=== Article {idx+1}/{len(articles)}: {article['title']} ===\")\n",
    "    chunks = chunk_text(\n",
    "        tokenizer, \n",
    "        article[\"text\"], max_tokens=800, overlap=200\n",
    "    )\n",
    "\n",
    "    for j, chunk in enumerate(chunks):\n",
    "        print(f\"   - Generating Q/A for chunk {j+1}/{len(chunks)}\")\n",
    "        qa_pairs = generate_qa_for_article(base_model, tokenizer,\n",
    "            {\"id\": article[\"id\"], \"title\": article[\"title\"], \"text\": chunk},\n",
    "            num_questions=2,\n",
    "        )\n",
    "        all_qa_pairs.extend(qa_pairs)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "753a6584",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def generate_with_teacher(model, prompt, max_new_tokens=512, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate plain text continuation — no chat formatting.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=2048\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    # decode only the continuation\n",
    "    generated = tokenizer.decode(\n",
    "        output_ids[0][inputs[\"input_ids\"].shape[-1] :], skip_special_tokens=True\n",
    "    )\n",
    "    return generated.strip()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e1f5732",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "test_article = articles[0]  # or load from json\n",
    "print(\"TITLE:\", test_article[\"title\"])\n",
    "print(\"TEXT SNIPPET:\\n\", test_article[\"text\"][:500])\n",
    "\n",
    "from textwrap import shorten\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an AI that generates question-answer pairs from news articles. \"\n",
    "    \"Given an article, you will create concise, factual questions and answers \"\n",
    "    \"based ONLY on the content of the article.\"\n",
    ")\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "Article:\n",
    "\\\"\\\"\\\"{test_article[\"text\"]}\\\"\\\"\\\"\n",
    "\n",
    "Task:\n",
    "Generate 5 question-answer pairs that a user might ask about this article.\n",
    "\n",
    "Return ONLY in this plain text format:\n",
    "\n",
    "Q: <question 1>\n",
    "A: <answer 1>\n",
    "Q: <question 2>\n",
    "A: <answer 2>\n",
    "Q: <question 3>\n",
    "A: <answer 3>\n",
    "Q: <question 4>\n",
    "A: <answer 4>\n",
    "Q: <question 5>\n",
    "A: <answer 5>\n",
    "\"\"\"\n",
    "\n",
    "full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "raw_output = generate_with_teacher(base_model, full_prompt, max_new_tokens=512)\n",
    "print(\"\\nRAW MODEL OUTPUT (first 800 chars):\\n\")\n",
    "print(shorten(raw_output, width=800, placeholder=\"...\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aed598e7",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def save_qa_pairs_json(qa_pairs, filepath):\n",
    "    \"\"\"\n",
    "    Save list of Q/A dicts to a JSON file (array of objects).\n",
    "    \"\"\"\n",
    "    path = Path(filepath)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(qa_pairs)} Q/A pairs to {filepath}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57e220e6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "save_qa_pairs_json(all_qa_pairs, \"data/hn_qa_pairs.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
