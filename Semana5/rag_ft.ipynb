{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bf9e6d1",
   "metadata": {},
   "source": [
    "![Colegio Bourbaki](./Images/Bourbaki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70ffd6",
   "metadata": {},
   "source": [
    "## Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9a929",
   "metadata": {},
   "source": [
    "En este notebook haremos lo siguiente:\n",
    "\n",
    "1. **Explicaremos** la diferencia entre:   \n",
    "- Generación aumentada por recuperación (**RAG**)   \n",
    "- **Ajuste fino** de un modelo de lenguaje\n",
    "- Uso de **ambos juntos** \n",
    "\n",
    "2. **Implementaremos un pequeño proceso RAG**:   \n",
    "- Usaremos un transformador de oraciones para incrustar documentos  \n",
    "- Almacenaremos las incrustaciones en un índice vectorial  \n",
    "- Recuperaremos pasajes relevantes  \n",
    "- Usaremos un pequeño modelo de chat de pesos abiertos para responder preguntas de ese contexto \n",
    "\n",
    "3. **Ajustar un pequeño modelo de pesos abiertos** en un pequeño conjunto de datos de preguntas y respuestas   \n",
    "- Utilizar LoRA / QLoRA para ajustarlo a una GPU de ~4 GB   \n",
    "- Comparar las respuestas **antes y después** del ajuste. Se trata de una GPU como la **NVIDIA GeForce GTX 1650 Ti 4 GB**, por lo que haremos lo siguiente: - Utilizar un modelo pequeño: `TinyLlama/TinyLlama-1.1B-Chat-v1.0`. \n",
    "- Cargarlo en **4 bits** siempre que sea posible. \n",
    "- Mantener tamaños de lote pequeños.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f44efd8",
   "metadata": {},
   "source": [
    "## RAG frente al ajuste fino (conceptual)\n",
    "\n",
    "### ¿Qué es RAG (generación aumentada por recuperación)?\n",
    "\n",
    "Los LLM tienen un **conocimiento limitado**: solo saben lo que vieron durante el entrenamiento previo.  \n",
    "RAG añade un **almacén de conocimiento externo** (por ejemplo, una base de datos vectorial):\n",
    "\n",
    "1. Se **incrustan** los documentos (artículos, documentos, tickets) en vectores.\n",
    "2. En el momento de la consulta, se:\n",
    "   - Incrusta la pregunta del usuario.\n",
    "   - Recupera los **documentos más similares**.\n",
    "   - Pasa la *pregunta + el contexto recuperado* al LLM.\n",
    "3. El modelo responde *utilizando ese contexto*, sin cambiar sus pesos.\n",
    "\n",
    "**Ventajas:**\n",
    "- Ideal para **datos nuevos y que cambian con frecuencia** (como las noticias diarias).\n",
    "- No requiere un entrenamiento pesado, solo incrustación + recuperación.\n",
    "- Seguro: no sobrescribe el modelo.\n",
    "\n",
    "**Desventajas:**\n",
    "- La calidad de la respuesta depende de la **calidad de la recuperación** y del tamaño de la solicitud.\n",
    "- Limitado por la **ventana de contexto**: solo se puede pasar una cantidad limitada de texto.\n",
    "\n",
    "---\n",
    "\n",
    "### ¿Qué es el ajuste fino?\n",
    "\n",
    "El ajuste fino significa **continuar entrenando** un LLM preentrenado en una **tarea o dominio específico**:\n",
    "\n",
    "- Ejemplo: miles de pares de preguntas y respuestas sobre la nube, Kubernetes, fintech, etc.\n",
    "- El modelo **actualiza sus pesos** para interiorizar este dominio.\n",
    "\n",
    "**Ventajas:**\n",
    "- El modelo mejora de forma nativa en ese dominio o estilo.\n",
    "- No es necesario proporcionar siempre un contexto largo: «sabe» más en sus pesos.\n",
    "\n",
    "**Desventajas:**\n",
    "- **Es costoso** (tiempo de GPU, canalización de entrenamiento).\n",
    "- Necesita **datos buenos y seleccionados**.\n",
    "- El modelo sigue teniendo un límite de conocimiento fijo (no «verá» nuevos artículos a menos que se vuelva a entrenar).\n",
    "\n",
    "---\n",
    "\n",
    "Puede:\n",
    "\n",
    "- Utilizar GPT-4 / modelos más grandes (o cualquier «modelo experto») para **generar pares de preguntas y respuestas** a partir de documentos.\n",
    "- **Ajustar finamente un modelo de pesos abiertos más pequeño** en estos pares de preguntas y respuestas.\n",
    "- Mantener RAG también para inyectar **documentos muy recientes**.\n",
    "\n",
    "Resultado:\n",
    "- El modelo pequeño mejora en **jerga y estilo** gracias al ajuste fino.\n",
    "- RAG lo mantiene **actualizado** con nuevos documentos.\n",
    "\n",
    "En el resto de este cuaderno implementaremos:\n",
    "\n",
    "1. Un pequeño **canal RAG**.\n",
    "2. Un pequeño **ajuste fino LoRA**.\n",
    "3. Una rápida **comparación**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eeee931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "# !pip install -q \\\n",
    "#   torch \\\n",
    "#   transformers \\\n",
    "#   accelerate \\\n",
    "#   bitsandbytes \\\n",
    "#   peft \\\n",
    "#   sentence-transformers \\\n",
    "#   datasets \\\n",
    "#   scikit-learn \\\n",
    "#   faiss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e4164",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3488cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from pathlib import Path\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from pprint import pprint\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GenerationConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311877a",
   "metadata": {},
   "source": [
    "### Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d06e040",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "torch.backends.cuda.matmul.fp32_precision = (\n",
    "    \"ieee\"  # torch.backends.cuda.matmul.allow_tf32 = True\n",
    ")\n",
    "torch.backends.cudnn.conv.fp32_precision = (\n",
    "    \"tf32\"  # torch.backends.cudnn.allow_tf32 = True\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11005e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.12.11 (main, Sep  5 2025, 19:35:43) [GCC 13.3.0]\n",
      "__pyTorch VERSION: 2.9.0+cu128\n",
      "__CUDA VERSION\n",
      "__CUDNN VERSION: 91002\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "print(\"__Python VERSION:\", sys.version)\n",
    "print(\"__pyTorch VERSION:\", torch.__version__)\n",
    "print(\n",
    "    \"__CUDA VERSION\",\n",
    ")\n",
    "print(\"__CUDNN VERSION:\", torch.backends.cudnn.version())\n",
    "print(\"__Number CUDA Devices:\", torch.cuda.device_count())\n",
    "print(\"__Devices\")\n",
    "print(\"Active CUDA Device: GPU\", torch.cuda.current_device())\n",
    "print(\"Available devices \", torch.cuda.device_count())\n",
    "print(\"Current cuda device \", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8af7ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 19 18:18:15 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti     Off |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   56C    P8              2W /   50W |     177MiB /   4096MiB |     38%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      8641      G   /usr/bin/gnome-shell                            1MiB |\n",
      "|    0   N/A  N/A   1306267      G   /proc/self/exe                                170MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b81482",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51598ba4",
   "metadata": {},
   "source": [
    "Vamos con un ejemplo pequeño"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29266f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_docs = [\n",
    "    # 1\n",
    "    \"\"\"OpenAI released a new model that improves reasoning on complex code and math problems. \n",
    "    The model is optimized for tool use and retrieval-augmented generation pipelines.\"\"\",\n",
    "    # 2\n",
    "    \"\"\"Google announced updates to its Vertex AI platform, making it easier to deploy and monitor \n",
    "    large language models at enterprise scale.\"\"\",\n",
    "    # 3\n",
    "    \"\"\"Meta open-sourced a set of Llama-based models with billions of parameters, \n",
    "    enabling researchers and companies to fine-tune them for their own use cases.\"\"\",\n",
    "    # 4\n",
    "    \"\"\"Microsoft integrated generative AI into its Office suite, adding features such as \n",
    "    AI-powered summarization, drafting assistance, and automatic meeting notes generation.\"\"\",\n",
    "    # 5\n",
    "    \"\"\"Amazon Web Services introduced cheaper GPU instances optimized for inference workloads \n",
    "    like chatbots, code assistants, real-time search, and document question-answering.\"\"\",\n",
    "    # 6\n",
    "    \"\"\"NVIDIA released new open-source libraries for accelerating transformer inference, \n",
    "    offering significant speedups on consumer GPUs like the RTX 4090.\"\"\",\n",
    "    # 7\n",
    "    \"\"\"Anthropic published a research paper describing improvements in constitutional AI, \n",
    "    focusing on scalable oversight and safer model behavior.\"\"\",\n",
    "    # 8\n",
    "    \"\"\"Apple reportedly began testing on-device LLMs for future iPhone models, enabling \n",
    "    private AI features such as offline summarization and personal context reasoning.\"\"\",\n",
    "    # 9\n",
    "    \"\"\"Hugging Face launched a new inference API tier with higher throughput and native \n",
    "    support for vLLM, making it cheaper to serve models like Mistral-7B and Llama-3-8B.\"\"\",\n",
    "    # 10\n",
    "    \"\"\"Mistral AI released Mixtral-8x22B, a sparse mixture-of-experts model offering state-of-the-art \n",
    "    performance while remaining efficient enough for commercial deployment.\"\"\",\n",
    "    # 11\n",
    "    \"\"\"IBM announced a partnership with NASA to fine-tune foundation models on geospatial data \n",
    "    to improve climate analysis, wildfire prediction, and satellite imagery classification.\"\"\",\n",
    "    # 12\n",
    "    \"\"\"Databricks released DBRX, a 132B-weight mixture-of-experts model trained on curated \n",
    "    scientific and enterprise datasets, outperforming models of similar size.\"\"\",\n",
    "    # 13\n",
    "    \"\"\"Stability AI introduced Stable Diffusion 3, featuring improved text-image alignment \n",
    "    and reduced hallucination in multilingual prompting scenarios.\"\"\",\n",
    "    # 14\n",
    "    \"\"\"Snowflake added native vector search capabilities, allowing enterprises to store embeddings \n",
    "    and run RAG pipelines directly on their data warehouse.\"\"\",\n",
    "    # 15\n",
    "    \"\"\"Cohere launched a secure enterprise-grade embedding model designed for document retrieval, \n",
    "    semantic search, and multi-lingual knowledge-base applications.\"\"\",\n",
    "    # 16\n",
    "    \"\"\"Red Hat announced AI-enhanced DevOps tooling, including automated deployment validation \n",
    "    powered by small specialized LLMs.\"\"\",\n",
    "    # 17\n",
    "    \"\"\"Salesforce updated Einstein GPT with better CRM-specific reasoning, including lead scoring, \n",
    "    automatic email drafting, and pipeline forecasting.\"\"\",\n",
    "    # 18\n",
    "    \"\"\"Dropbox introduced AI-powered universal search across files, documents, PDFs, and images, \n",
    "    enabling users to query semantic content instantly.\"\"\",\n",
    "    # 19\n",
    "    \"\"\"Slack rolled out AI summarization for channels and threads, automatically generating \n",
    "    daily digests and extracting key decisions from long discussions.\"\"\",\n",
    "    # 20\n",
    "    \"\"\"Zoom added real-time conversation translation and AI-based meeting action items, \n",
    "    powered by a fine-tuned multilingual transformer model.\"\"\",\n",
    "]\n",
    "\n",
    "corpus_titles = [\n",
    "    \"OpenAI releases new reasoning model\",\n",
    "    \"Google updates Vertex AI\",\n",
    "    \"Meta open-sources Llama models\",\n",
    "    \"Microsoft adds AI to Office\",\n",
    "    \"AWS introduces cheaper GPU instances\",\n",
    "    \"NVIDIA releases transformer acceleration libs\",\n",
    "    \"Anthropic improves constitutional AI\",\n",
    "    \"Apple tests on-device LLMs\",\n",
    "    \"Hugging Face launches new inference tier\",\n",
    "    \"Mistral releases Mixtral-8x22B\",\n",
    "    \"IBM partners with NASA on geospatial AI\",\n",
    "    \"Databricks releases DBRX\",\n",
    "    \"Stability AI releases SD3\",\n",
    "    \"Snowflake adds vector search\",\n",
    "    \"Cohere launches enterprise embedding model\",\n",
    "    \"Red Hat adds AI DevOps tools\",\n",
    "    \"Salesforce updates Einstein GPT\",\n",
    "    \"Dropbox adds AI universal search\",\n",
    "    \"Slack adds AI summaries\",\n",
    "    \"Zoom adds real-time AI translation\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58330e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dc2cc8",
   "metadata": {},
   "source": [
    "Realizamos el embedding de los documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d2ffe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small and fast embedding model (open weights)\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97a6f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(model_name_or_path=embedding_model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e592432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16773d655693413db2bafaa2276a7110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute embeddings\n",
    "doc_embeddings = embedder.encode(\n",
    "    sentences=corpus_docs, \n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    device=device,\n",
    "    normalize_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82482535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.05694531, -0.01272646, -0.06879752, ...,  0.05730383,\n",
       "          0.04768759,  0.00835872],\n",
       "        [-0.07661536, -0.08271152,  0.03887794, ..., -0.00933229,\n",
       "          0.05409345, -0.03391702],\n",
       "        [-0.03868605, -0.02878194, -0.02075998, ..., -0.04762491,\n",
       "         -0.01284006,  0.03692014],\n",
       "        ...,\n",
       "        [-0.02095782, -0.03330291, -0.04754037, ...,  0.04166466,\n",
       "          0.05232637,  0.02517612],\n",
       "        [-0.00392685, -0.02862822, -0.01042572, ...,  0.05525399,\n",
       "         -0.05279417, -0.02444052],\n",
       "        [-0.08633485, -0.04698378,  0.00952209, ...,  0.04447945,\n",
       "         -0.1053777 , -0.02525596]], dtype=float32),\n",
       " (20, 384))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_embeddings, doc_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b09fb",
   "metadata": {},
   "source": [
    "Vamos a crear un índice FAISS para búsqueda eficiente de vecinos más cercanos y un indice por NearestNeighbors en sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bf41aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "nn_index = NearestNeighbors(n_neighbors=3, metric=\"cosine\")\n",
    "nn_index.fit(doc_embeddings)\n",
    "\n",
    "# Faiss\n",
    "faiss_emb = np.array(doc_embeddings).astype(\"float32\")\n",
    "faiss_index = faiss.IndexFlatIP(faiss_emb.shape[1])  # cosine similarity via inner product\n",
    "faiss.normalize_L2(faiss_emb)\n",
    "faiss_index.add(faiss_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e284facc",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8081088a",
   "metadata": {},
   "source": [
    "![Quant1](./Images/RAG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8cad9",
   "metadata": {},
   "source": [
    "Fuente: P. Iusztin & M. Labonne - LLM Engineer's Handbook - Chapter 4 - RAG Feature Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff963db0",
   "metadata": {},
   "source": [
    "Vamos a crear una función que nos genera la salida bruta (input+output) y la salida neta (output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "782b9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, prompt, max_length, max_new_tokens):\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    ).to(device)\n",
    "\n",
    "    gen_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,  # activa el muestreo aleatorio (sampling) en lugar de argmax\n",
    "        # necesario para que temperature / top_p tengan efecto\n",
    "        temperature=0.3,  # escala la \"suavidad\" del softmax\n",
    "        top_p=0.9,  # nucleus sampling: el modelo elige solo entre las palabras que\n",
    "        # acumulan el 90% de la probabilidad total (variable-size)\n",
    "        # top_k=50,        # OPCIONAL: limitar a las k palabras más probables\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(**inputs, generation_config=gen_config)\n",
    "\n",
    "    # Full decoded output (prompt + generated)\n",
    "    full_decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Only the continuation (generated tokens after the prompt)\n",
    "    generated_ids = output[0][inputs[\"input_ids\"].shape[1] :]\n",
    "    generated_decoded = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "    return full_decoded, generated_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94423eac",
   "metadata": {},
   "source": [
    "En la generación de texto, estos parámetros controlan cuánta **aleatoriedad**, **creatividad** o **determinismo** tendrá el modelo.\n",
    "\n",
    "**1. do_sample**\n",
    "Indica si el modelo debe usar muestreo aleatorio en lugar de escoger siempre la palabra más probable.\n",
    "\n",
    "`do_sample=False` → Decodificación determinista\n",
    "- El modelo siempre elige el token con mayor probabilidad (argmax).  \n",
    "- Equivale a *greedy decoding* o *beam search*.  \n",
    "- La salida es siempre igual para un mismo input.\n",
    "\n",
    "`do_sample=True` → Decodificación con muestreo\n",
    "- El modelo **no** toma siempre la palabra más probable.  \n",
    "- Muestra aleatoriamente según la distribución de probabilidades (softmax).  \n",
    "- Permite creatividad y variación.\n",
    "\n",
    "\n",
    "**2. Temperatura**\n",
    "La temperatura controla qué tan “plana” o “concentrada” es la distribución de probabilidades.\n",
    "\n",
    "Efectos prácticos:\n",
    "- **Temperatura baja (0.0 – 0.5):**  \n",
    "  Texto más determinista, formal, predecible.\n",
    "- **Temperatura media (0.7 – 1.0):**  \n",
    "  Buen balance entre coherencia y creatividad.\n",
    "- **Temperatura alta (≥1.2):**  \n",
    "  Texto muy creativo e impredecible.\n",
    "\n",
    "Interpretación intuitiva:\n",
    "“Más temperatura = más libertad para elegir palabras.”\n",
    "\n",
    "Matemáticamente:\n",
    "La temperatura \\(T\\) se aplica escalando los logits del modelo:\n",
    "\n",
    "$P(w_i) = \\frac{e^{z_i / T}}{\\sum_j e^{z_j / T}}$\n",
    "\n",
    "donde:\n",
    "- $z_i$ = logit del token \\(i\\)  \n",
    "- $T$ = temperatura\n",
    "\n",
    "\n",
    "**3. Top-p (Nucleus Sampling)**\n",
    "Top-p controla aleatoriedad seleccionando solo los tokens cuya **probabilidad acumulada** alcanza un umbral \\(p\\).\n",
    "\n",
    "Ejemplos:\n",
    "- **p = 0.5** → muy conservador  \n",
    "- **p = 0.9** → equilibrado (el más usado)  \n",
    "- **p = 0.95–0.99** → más creativo  \n",
    "\n",
    "Algoritmo:\n",
    "1. Ordenar los tokens por probabilidad:  \n",
    "   $P(w_1) \\ge P(w_2) \\ge \\dots \\ge P(w_n)$\n",
    "2. Construir el conjunto mínimo \\(S\\) tal que:  \n",
    "   $\\sum_{w_i \\in S} P(w_i) \\ge p$\n",
    "3. Hacer muestreo **solo dentro de \\(S\\)**:\n",
    "   $w \\sim \\text{Multinomial}\\big(P(w_i \\mid w_i \\in S)\\big)$\n",
    "\n",
    "Propiedad clave:\n",
    "El tamaño del conjunto **varía dinámicamente** según la distribución → más flexible que top-k.\n",
    "\n",
    "\n",
    "**4. Top-k**\n",
    "Top-k limita la elección a las **k palabras más probables**, descartando el resto.\n",
    "\n",
    "Ejemplos:\n",
    "- **k pequeño (10):** más control y coherencia.  \n",
    "- **k grande (50–100):** más diversidad.  \n",
    "- **k infinito / desactivado:** usa todos los tokens.\n",
    "\n",
    "Matemáticamente:\n",
    "Top-k actúa **antes del softmax**:\n",
    "\n",
    "1. Seleccionar los $k$ logits más altos.  \n",
    "2. Descartar los otros.  \n",
    "3. Aplicar softmax solo sobre esos $k$:\n",
    "\n",
    "$\n",
    "P(w_i)=\n",
    "\\begin{cases}\n",
    "\\frac{e^{z_i}}{\\sum_{j \\in \\text{top-k}} e^{z_j}} & i \\in \\text{top-k} \\\\\n",
    "0 & i \\notin \\text{top-k}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "**Resumen comparativo**\n",
    "\n",
    "| Parámetro | Qué controla | Tipo de límite |\n",
    "|-----------|--------------|----------------|\n",
    "| **do_sample** | Si hay muestreo o no | booleano |\n",
    "| **temperatura** | Suavidad de la distribución | escala continua |\n",
    "| **top-k** | Número fijo de candidatos | tamaño fijo |\n",
    "| **top-p** | Probabilidad acumulada | tamaño variable |\n",
    "\n",
    "**Top-p es más flexible e inteligente**, porque se adapta a la forma de la distribución.  \n",
    "**Top-k es más simple y estable**, pero rígido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049ddc90",
   "metadata": {},
   "source": [
    "Ahora, una funcion retrieval de contexto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "122ee662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, k, backend):\n",
    "    \"\"\"\n",
    "    Retrieve top-k most similar documents using a selected backend:\n",
    "\n",
    "        - 'sklearn' : brute-force KNN using Scikit-Learn\n",
    "        - 'faiss'   : FAISS IndexFlatIP (optimized inner-product ANN)\n",
    "        - 'st'      : SentenceTransformers' own cosine-similarity search\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed query → normalized vector (good for cosine similarity / inner product)\n",
    "    q_emb = embedder.encode(\n",
    "        [question], convert_to_numpy=True, normalize_embeddings=True\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 1) Scikit-Learn NearestNeighbors (exact search)\n",
    "    # ---------------------------------------------------------\n",
    "    if backend == \"sklearn\":\n",
    "        # Brute-force cosine similarity via sklearn's KNN search.\n",
    "        # Works well for small / medium corpora (<100k).\n",
    "        distances, indices = nn_index.kneighbors(q_emb, n_neighbors=k)\n",
    "        return [corpus_docs[i] for i in indices[0]]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 2) FAISS (fast ANN search using inner product)\n",
    "    # ---------------------------------------------------------\n",
    "    elif backend == \"faiss\":\n",
    "        # FAISS expects float32 arrays.\n",
    "        q = q_emb.astype(\"float32\")\n",
    "        # Normalize for cosine similarity (since IP ≈ cosine when vectors are normalized)\n",
    "        faiss.normalize_L2(q)\n",
    "        # Very fast search (exact or ANN depending on index type)\n",
    "        distances, indices = faiss_index.search(q, k)\n",
    "        return [corpus_docs[i] for i in indices[0]]\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # 3) SentenceTransformers semantic_search (exact cosine)\n",
    "    # ---------------------------------------------------------\n",
    "    elif backend == \"st\":\n",
    "        # Computes cosine similarity against all doc embeddings.\n",
    "        # This is brute-force but highly optimized in PyTorch/Numpy.\n",
    "        hits = util.semantic_search(q_emb, doc_embeddings, top_k=k)[0]\n",
    "        return [corpus_docs[hit[\"corpus_id\"]] for hit in hits]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown retrieval backend: {backend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d55c1d",
   "metadata": {},
   "source": [
    "Definimos 2 funciones: una que arma el prompt y otra que genera la respuesta del sistema RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ffdbeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(question, tokenizer, contexts):\n",
    "    context_text = contexts[0]\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant specialized in technology news.\n",
    "    Use ONLY the context below to answer the user question.\n",
    "    If the answer is not in the context, say I don't know.\n",
    "    Answer ONE short sentence. Do NOT repeat context or question\n",
    "    \\n  Question: {question}\n",
    "    \\n  Context: {context_text}\n",
    "    \\n Answer:\n",
    "    \"\"\"\n",
    "    count = len(tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0])\n",
    "    return prompt, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10fb66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_answer(model, tokenizer, question, k, max_length, max_new_tokens, backend):\n",
    "    \"\"\"\n",
    "    Full RAG flow:\n",
    "    - Retrieve similar docs\n",
    "    - Build ChatML prompt with context\n",
    "    - Generate answer with the LLM\n",
    "    \"\"\"\n",
    "    contexts = retrieve_context(question, k, backend)\n",
    "    prompt, token_count = build_prompt(question, tokenizer, contexts)\n",
    "    full_output, gen_output = generate_answer(model, tokenizer, prompt, max_length, max_new_tokens)\n",
    "    return prompt, full_output, gen_output, contexts, token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f27cb3",
   "metadata": {},
   "source": [
    "Elegimos un modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ba70968",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e46e0",
   "metadata": {},
   "source": [
    "Link del modelo: https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f46d2e",
   "metadata": {},
   "source": [
    "Link de interes: https://codingscape.com/blog/llms-with-largest-context-windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20830fa7",
   "metadata": {},
   "source": [
    "Y un tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "785a9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdef92d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'▁Ott': 13476,\n",
       "  '▁clust': 16993,\n",
       "  'chem': 14969,\n",
       "  '▁`.': 5050,\n",
       "  'φ': 30237,\n",
       "  'remove': 5992,\n",
       "  '══': 13648,\n",
       "  '▁Fragment': 19063,\n",
       "  'ведения': 25321,\n",
       "  '▁ie': 19282,\n",
       "  '▁vars': 24987,\n",
       "  '▁bi': 4768,\n",
       "  '▁приз': 25660,\n",
       "  'km': 8848,\n",
       "  'amentos': 26376,\n",
       "  '▁already': 2307,\n",
       "  'half': 24498,\n",
       "  '<0xBE>': 193,\n",
       "  'Float': 11031,\n",
       "  '▁spread': 9677,\n",
       "  '▁flow': 4972,\n",
       "  'pub': 5467,\n",
       "  'LES': 17101,\n",
       "  'istic': 4695,\n",
       "  '▁dressed': 27121,\n",
       "  '▁R': 390,\n",
       "  'System': 3924,\n",
       "  'anging': 9776,\n",
       "  'joint': 12090,\n",
       "  'евич': 13177,\n",
       "  '▁etwas': 23452,\n",
       "  '▁Raz': 24961,\n",
       "  '▁Gabriel': 18672,\n",
       "  '▁meets': 28103,\n",
       "  'Async': 8123,\n",
       "  'zet': 4975,\n",
       "  'plementation': 14607,\n",
       "  '←': 30245,\n",
       "  'ським': 29796,\n",
       "  'handler': 13789,\n",
       "  '▁entities': 16212,\n",
       "  'otta': 13536,\n",
       "  '▁charset': 17425,\n",
       "  'spre': 17703,\n",
       "  'Mon': 7185,\n",
       "  'isti': 14194,\n",
       "  '▁брига': 26672,\n",
       "  '▁Д': 1453,\n",
       "  'zeti': 21047,\n",
       "  '▁storia': 19097,\n",
       "  '▁ros': 14652,\n",
       "  'anna': 9713,\n",
       "  \"%'\": 29001,\n",
       "  '▁город': 12816,\n",
       "  '▁SC': 12314,\n",
       "  'variant': 19365,\n",
       "  '▁made': 1754,\n",
       "  'нцикло': 18222,\n",
       "  '▁−': 13935,\n",
       "  '▁lleg': 10953,\n",
       "  'la': 433,\n",
       "  'pal': 7830,\n",
       "  '▁cosm': 27973,\n",
       "  '▁Charlie': 20283,\n",
       "  'unicí': 27634,\n",
       "  '▁Dig': 10951,\n",
       "  '▁says': 4083,\n",
       "  '์': 30779,\n",
       "  'куп': 28971,\n",
       "  'cluding': 22368,\n",
       "  'œuvre': 19898,\n",
       "  '▁Token': 25159,\n",
       "  '▁Len': 16206,\n",
       "  'views': 7406,\n",
       "  '▁Bahn': 15131,\n",
       "  '▁browsers': 14376,\n",
       "  '后': 30822,\n",
       "  ')}': 2915,\n",
       "  '▁jap': 10215,\n",
       "  '麻': 31846,\n",
       "  'oh': 1148,\n",
       "  'inct': 5562,\n",
       "  'ogle': 1882,\n",
       "  'Dat': 16390,\n",
       "  'Gr': 3338,\n",
       "  'ety': 3305,\n",
       "  'спо': 3565,\n",
       "  'encia': 5760,\n",
       "  '▁techn': 5722,\n",
       "  'itage': 16639,\n",
       "  'ren': 1267,\n",
       "  '▁vär': 28412,\n",
       "  'Acc': 7504,\n",
       "  '):': 1125,\n",
       "  'Spec': 10299,\n",
       "  '▁colonial': 25539,\n",
       "  'walk': 20919,\n",
       "  '▁Object': 4669,\n",
       "  '▁Fue': 14692,\n",
       "  'кры': 10300,\n",
       "  'ames': 1280,\n",
       "  '▁vesc': 28293,\n",
       "  '▁segments': 24611,\n",
       "  'ぶ': 31782,\n",
       "  'Route': 12085,\n",
       "  'hos': 15656,\n",
       "  '▁convert': 3588,\n",
       "  '▁fabric': 18187,\n",
       "  'uchs': 19873,\n",
       "  'acyj': 21469,\n",
       "  '▁Richard': 6123,\n",
       "  'issent': 19047,\n",
       "  'í': 29983,\n",
       "  '&=': 20644,\n",
       "  '▁bin': 9016,\n",
       "  '▁cut': 5700,\n",
       "  '▁populations': 23093,\n",
       "  '▁receive': 7150,\n",
       "  '▁lá': 24303,\n",
       "  '▁време': 26958,\n",
       "  '▁strong': 4549,\n",
       "  '▁вне': 21944,\n",
       "  '▁arrival': 18517,\n",
       "  '▁pron': 11504,\n",
       "  '▁artifact': 24238,\n",
       "  '▁Valent': 26411,\n",
       "  '▁ces': 7015,\n",
       "  '▁sei': 13106,\n",
       "  'église': 20822,\n",
       "  '▁loro': 9905,\n",
       "  '▁kar': 10856,\n",
       "  '▁Despite': 19454,\n",
       "  '▁va': 2947,\n",
       "  '▁stated': 8703,\n",
       "  '▁notifications': 25913,\n",
       "  'ritz': 18238,\n",
       "  '▁documents': 10701,\n",
       "  '▁Shaw': 28548,\n",
       "  '▁West': 3122,\n",
       "  '▁объ': 11779,\n",
       "  'дно': 12869,\n",
       "  '▁wont': 20668,\n",
       "  'Desktop': 17600,\n",
       "  '▁rund': 17048,\n",
       "  'MD': 5773,\n",
       "  '作': 30732,\n",
       "  '▁liked': 23289,\n",
       "  'ники': 9645,\n",
       "  '▁erre': 13930,\n",
       "  '▁inherit': 13125,\n",
       "  'proj': 20865,\n",
       "  '▁began': 4689,\n",
       "  '▁SI': 22717,\n",
       "  '▁Histor': 4731,\n",
       "  'Image': 2940,\n",
       "  '▁Record': 14164,\n",
       "  'mongodb': 23264,\n",
       "  '/#': 8484,\n",
       "  '▁розта': 19735,\n",
       "  'Extra': 18126,\n",
       "  '▁living': 8471,\n",
       "  '▁у': 863,\n",
       "  'ił': 27233,\n",
       "  'ORY': 18929,\n",
       "  'ORD': 25593,\n",
       "  '\\u2002': 30244,\n",
       "  '▁consid': 16133,\n",
       "  'оте': 20390,\n",
       "  '▁có': 28810,\n",
       "  'VA': 20449,\n",
       "  '▁di': 652,\n",
       "  '“.': 9533,\n",
       "  '▁ratio': 11959,\n",
       "  '▁Medical': 20795,\n",
       "  '▁эконо': 24297,\n",
       "  '▁wetenschapp': 10143,\n",
       "  '터': 31856,\n",
       "  '１': 31936,\n",
       "  '▁Alan': 17102,\n",
       "  '▁compute': 10272,\n",
       "  '▁blob': 23755,\n",
       "  '对': 30783,\n",
       "  '▁virtual': 6901,\n",
       "  'cient': 15566,\n",
       "  'ugh': 6129,\n",
       "  'gl': 3820,\n",
       "  '▁estos': 21010,\n",
       "  'Track': 17936,\n",
       "  'Marker': 24619,\n",
       "  'clou': 23642,\n",
       "  'prim': 9469,\n",
       "  '▁hid': 20552,\n",
       "  '▁equilib': 22213,\n",
       "  'ency': 3819,\n",
       "  'ов': 516,\n",
       "  'anning': 9450,\n",
       "  'bow': 17729,\n",
       "  'eree': 23987,\n",
       "  'cred': 11944,\n",
       "  'ץ': 31671,\n",
       "  'rund': 29160,\n",
       "  'ires': 2658,\n",
       "  '▁independent': 7417,\n",
       "  '▁Init': 10886,\n",
       "  '▁minister': 11050,\n",
       "  '▁lem': 9336,\n",
       "  'ate': 403,\n",
       "  'ARY': 19926,\n",
       "  'ictwo': 28223,\n",
       "  '▁Max': 5918,\n",
       "  'aped': 10501,\n",
       "  '▁unfortunately': 15428,\n",
       "  '▁това': 18077,\n",
       "  '▁Werk': 15492,\n",
       "  '▁esta': 7444,\n",
       "  '▁пер': 4651,\n",
       "  'мина': 12189,\n",
       "  'Distance': 27469,\n",
       "  'ŏ': 30476,\n",
       "  '▁circumstances': 14209,\n",
       "  \"▁'';\": 18231,\n",
       "  '▁stones': 25702,\n",
       "  'athon': 25206,\n",
       "  '▁Right': 10428,\n",
       "  '▁ny': 7098,\n",
       "  'entication': 6002,\n",
       "  'graph': 4262,\n",
       "  'eny': 15274,\n",
       "  'ω': 30206,\n",
       "  'edes': 11696,\n",
       "  '▁test': 1243,\n",
       "  'inn': 2559,\n",
       "  'uber': 11234,\n",
       "  'ensoort': 26006,\n",
       "  'ened': 6419,\n",
       "  'caught': 24348,\n",
       "  '▁Johannes': 15265,\n",
       "  '<0xF5>': 248,\n",
       "  '▁Virtual': 19181,\n",
       "  'aked': 12535,\n",
       "  '▁giv': 1517,\n",
       "  'кси': 11153,\n",
       "  '连': 31903,\n",
       "  '▁prac': 16689,\n",
       "  'WS': 7811,\n",
       "  '▁IT': 13315,\n",
       "  'plit': 2830,\n",
       "  'Å': 30129,\n",
       "  'icha': 24227,\n",
       "  '▁aged': 26552,\n",
       "  'display': 4990,\n",
       "  '▁имени': 21959,\n",
       "  'setText': 12038,\n",
       "  '▁aussi': 5695,\n",
       "  '▁dois': 19760,\n",
       "  'most': 3242,\n",
       "  'ention': 2509,\n",
       "  'ingsområ': 14723,\n",
       "  '▁elle': 4875,\n",
       "  'tr': 509,\n",
       "  '▁Library': 9538,\n",
       "  '▁Sic': 18349,\n",
       "  '=.': 21098,\n",
       "  'ธ': 31547,\n",
       "  '世': 30793,\n",
       "  '戦': 30863,\n",
       "  '▁work': 664,\n",
       "  'ého': 6690,\n",
       "  '▁bom': 18523,\n",
       "  '▁results': 2582,\n",
       "  'лін': 28338,\n",
       "  'Options': 5856,\n",
       "  '▁In': 512,\n",
       "  '▁Под': 20266,\n",
       "  '▁Command': 10516,\n",
       "  '▁Спољашње': 16110,\n",
       "  '▁lives': 12080,\n",
       "  'lder': 25943,\n",
       "  'ît': 13871,\n",
       "  '▁&': 669,\n",
       "  '▁jú': 16210,\n",
       "  'вое': 18072,\n",
       "  'судар': 9840,\n",
       "  '+\"': 13578,\n",
       "  'heiten': 21795,\n",
       "  'One': 6716,\n",
       "  '▁Hunter': 25703,\n",
       "  '▁advis': 25228,\n",
       "  '▁théâtre': 25574,\n",
       "  '▁Catal': 11732,\n",
       "  'algorithm': 20567,\n",
       "  '▁killed': 9445,\n",
       "  'forward': 11333,\n",
       "  '▁McK': 24053,\n",
       "  '▁Dro': 22938,\n",
       "  'LayoutParams': 26476,\n",
       "  '▁nouvelles': 26957,\n",
       "  '▁andra': 21912,\n",
       "  '▁Handle': 29273,\n",
       "  '%;': 8874,\n",
       "  '▁engaged': 17785,\n",
       "  '▁destination': 12551,\n",
       "  '▁retour': 18948,\n",
       "  '▁quartier': 29553,\n",
       "  '▁Ne': 2448,\n",
       "  'prüng': 21563,\n",
       "  'ige': 2231,\n",
       "  '▁einz': 15747,\n",
       "  '▁vý': 9660,\n",
       "  '▁av': 1029,\n",
       "  'con': 535,\n",
       "  'вин': 21187,\n",
       "  '▁mainly': 14364,\n",
       "  '▁rolled': 29081,\n",
       "  '▁Count': 3917,\n",
       "  '▁LIKE': 22962,\n",
       "  '▁const': 1040,\n",
       "  '▁î': 2961,\n",
       "  '▁Eq': 12345,\n",
       "  '▁vary': 13100,\n",
       "  '▁França': 29253,\n",
       "  '▁love': 5360,\n",
       "  '▁significa': 28711,\n",
       "  '▁utf': 23616,\n",
       "  'Sub': 4035,\n",
       "  '▁quasi': 16452,\n",
       "  'aking': 5086,\n",
       "  'cem': 19335,\n",
       "  'Unit': 8325,\n",
       "  'Alpha': 28630,\n",
       "  '여': 31457,\n",
       "  '▁Next': 8084,\n",
       "  '▁Half': 28144,\n",
       "  '▁Anna': 11230,\n",
       "  '▁nev': 29865,\n",
       "  'definition': 16553,\n",
       "  'look': 6914,\n",
       "  '▁partly': 22669,\n",
       "  'PART': 26092,\n",
       "  'ови': 1928,\n",
       "  'Py': 19737,\n",
       "  '▁\"+': 15691,\n",
       "  'ром': 4416,\n",
       "  '▁Na': 4465,\n",
       "  '▁difficult': 5189,\n",
       "  '▁Ocean': 21091,\n",
       "  '▁Rose': 14008,\n",
       "  'resol': 9778,\n",
       "  '▁particulier': 28353,\n",
       "  '▁induction': 21445,\n",
       "  '▁phases': 29540,\n",
       "  '▁recre': 28709,\n",
       "  '▁Mer': 4702,\n",
       "  '非': 31838,\n",
       "  '▁Ні': 23865,\n",
       "  '▁lok': 25194,\n",
       "  'ἰ': 30985,\n",
       "  '▁одно': 17636,\n",
       "  '▁fle': 9115,\n",
       "  '▁exclaimed': 17463,\n",
       "  '‹': 30474,\n",
       "  '▁databases': 21218,\n",
       "  '▁Kob': 28014,\n",
       "  '▁performances': 21637,\n",
       "  'uellement': 27751,\n",
       "  'forall': 10956,\n",
       "  '▁estruct': 26530,\n",
       "  '▁Maj': 6973,\n",
       "  '▁Baseball': 23185,\n",
       "  'arna': 11441,\n",
       "  'paths': 24772,\n",
       "  '▁BB': 29449,\n",
       "  'arily': 6275,\n",
       "  'UIView': 26726,\n",
       "  '▁None': 6213,\n",
       "  '이': 30393,\n",
       "  'perate': 21194,\n",
       "  'lek': 28508,\n",
       "  'À': 30113,\n",
       "  '▁hely': 14827,\n",
       "  'onnen': 16678,\n",
       "  '▁~[': 18173,\n",
       "  'CE': 4741,\n",
       "  '▁protect': 12566,\n",
       "  'ické': 15510,\n",
       "  'ous': 681,\n",
       "  'ал': 11887,\n",
       "  '▁Eli': 26864,\n",
       "  '▁secolo': 15200,\n",
       "  'export': 15843,\n",
       "  'osti': 16098,\n",
       "  'ne': 484,\n",
       "  '▁invariant': 22619,\n",
       "  'umble': 15563,\n",
       "  '▁passing': 6819,\n",
       "  'Driver': 12376,\n",
       "  'ḫ': 30996,\n",
       "  'syntax': 29562,\n",
       "  '▁fmt': 19200,\n",
       "  '▁heures': 26893,\n",
       "  '▁WHERE': 5754,\n",
       "  'INCT': 28852,\n",
       "  'čen': 19534,\n",
       "  'cock': 24956,\n",
       "  '♠': 31067,\n",
       "  'mil': 23853,\n",
       "  '▁получил': 21250,\n",
       "  '▁Il': 1720,\n",
       "  'ври': 20219,\n",
       "  '▁строи': 22493,\n",
       "  'apk': 16681,\n",
       "  '▁sit': 7845,\n",
       "  'Metadata': 18417,\n",
       "  '▁companies': 14582,\n",
       "  '}</': 16040,\n",
       "  'пет': 29647,\n",
       "  'PP': 18009,\n",
       "  '₃': 30174,\n",
       "  'ora': 2207,\n",
       "  '▁consent': 20218,\n",
       "  '▁дивизи': 18311,\n",
       "  '▁Era': 21018,\n",
       "  '▁representative': 21097,\n",
       "  '▁IM': 22313,\n",
       "  'ר': 30236,\n",
       "  'nginx': 23257,\n",
       "  'rup': 17827,\n",
       "  'attro': 19114,\n",
       "  '╝': 31269,\n",
       "  '▁команди': 19137,\n",
       "  '▁avait': 7905,\n",
       "  '▁pendant': 13180,\n",
       "  'ше': 2237,\n",
       "  '▁vict': 6879,\n",
       "  'lobal': 3157,\n",
       "  '▁Hills': 24128,\n",
       "  'ershell': 27456,\n",
       "  'cho': 1859,\n",
       "  'grid': 7720,\n",
       "  'ż': 30042,\n",
       "  '▁succeeded': 14792,\n",
       "  '▁repres': 10981,\n",
       "  '▁Chron': 15336,\n",
       "  'Query': 3010,\n",
       "  '▁prim': 1903,\n",
       "  '▁erste': 11870,\n",
       "  'schule': 15361,\n",
       "  '▁abb': 22195,\n",
       "  '▁tools': 8492,\n",
       "  'lj': 14042,\n",
       "  'oris': 29367,\n",
       "  '▁hold': 4808,\n",
       "  'urbed': 28179,\n",
       "  'teilung': 18958,\n",
       "  '▁начале': 26266,\n",
       "  'maz': 16986,\n",
       "  '▁dop': 22357,\n",
       "  '▁поме': 27724,\n",
       "  '德': 31169,\n",
       "  '...\"': 17794,\n",
       "  '▁okrę': 25937,\n",
       "  'ormal': 2759,\n",
       "  '▁cargo': 17040,\n",
       "  '▁Template': 25663,\n",
       "  '▁RA': 18865,\n",
       "  '▁Google': 5087,\n",
       "  '▁repo': 13761,\n",
       "  '▁escri': 21415,\n",
       "  '▁run': 1065,\n",
       "  '▁succ': 8348,\n",
       "  '▁Бі': 20432,\n",
       "  '▁ім': 19240,\n",
       "  'asure': 3745,\n",
       "  '▁silly': 24866,\n",
       "  '▁Let': 2803,\n",
       "  '▁acknow': 18145,\n",
       "  '▁Spirit': 20799,\n",
       "  '▁inserted': 15478,\n",
       "  'cap': 5030,\n",
       "  'PC': 9026,\n",
       "  'stein': 5465,\n",
       "  'ья': 14252,\n",
       "  'rent': 7771,\n",
       "  'syn': 19274,\n",
       "  '▁Platform': 28096,\n",
       "  'ział': 17634,\n",
       "  'xx': 4419,\n",
       "  'lis': 23443,\n",
       "  '\"': 29908,\n",
       "  '▁integrate': 22782,\n",
       "  '▁tuvo': 20596,\n",
       "  '*\"': 20605,\n",
       "  '▁incl': 1343,\n",
       "  '▁Rang': 25800,\n",
       "  '▁importante': 13483,\n",
       "  'LOAD': 29428,\n",
       "  'れ': 30553,\n",
       "  '▁hot': 7375,\n",
       "  '▁Anal': 11597,\n",
       "  '▁október': 23882,\n",
       "  'rame': 3128,\n",
       "  '▁Depart': 23242,\n",
       "  '▁Watson': 28284,\n",
       "  '▁cou': 3581,\n",
       "  '<0x3D>': 64,\n",
       "  'imately': 15084,\n",
       "  '▁typescript': 23741,\n",
       "  '▁intention': 16392,\n",
       "  'віт': 11703,\n",
       "  'Espagne': 24624,\n",
       "  'cc': 617,\n",
       "  '▁stable': 13714,\n",
       "  '(@': 10394,\n",
       "  '▁candidate': 14020,\n",
       "  '▁Wien': 12899,\n",
       "  'ensed': 21144,\n",
       "  '▁hierarchy': 21277,\n",
       "  '▁proof': 5296,\n",
       "  'arda': 23144,\n",
       "  '...]': 17361,\n",
       "  '▁фамили': 18147,\n",
       "  '▁effort': 7225,\n",
       "  '▁Sul': 13358,\n",
       "  '▁stim': 20436,\n",
       "  '▁CA': 12766,\n",
       "  '▁Simple': 12545,\n",
       "  'iba': 16912,\n",
       "  '▁understand': 2274,\n",
       "  '▁G': 402,\n",
       "  '▁rép': 25179,\n",
       "  'фек': 18201,\n",
       "  '▁zawod': 25425,\n",
       "  '▁Types': 28025,\n",
       "  'ffic': 2416,\n",
       "  '▁maximum': 7472,\n",
       "  'ART': 8322,\n",
       "  '\\\\{': 10045,\n",
       "  '▁Maur': 11475,\n",
       "  '▁zweiten': 20496,\n",
       "  '▁corso': 25438,\n",
       "  '▁Carol': 8562,\n",
       "  '▁grows': 25088,\n",
       "  'че': 1093,\n",
       "  'ː': 30215,\n",
       "  '▁dod': 21130,\n",
       "  '▁NAT': 26038,\n",
       "  '▁wrote': 5456,\n",
       "  'Right': 7341,\n",
       "  'aga': 7781,\n",
       "  '/\"': 12975,\n",
       "  '▁разли': 16481,\n",
       "  '▁movements': 24147,\n",
       "  'кет': 27268,\n",
       "  'ain': 475,\n",
       "  'Definition': 14683,\n",
       "  '▁rest': 1791,\n",
       "  '▁Jackson': 11886,\n",
       "  '▁radius': 11855,\n",
       "  'ва': 846,\n",
       "  '▁pont': 13185,\n",
       "  '▁xx': 15473,\n",
       "  '▁Lie': 7326,\n",
       "  'SN': 19296,\n",
       "  '▁turned': 6077,\n",
       "  'тери': 8747,\n",
       "  '▁Iowa': 25327,\n",
       "  'Init': 6644,\n",
       "  '▁manus': 18831,\n",
       "  '▁Context': 15228,\n",
       "  'USER': 11889,\n",
       "  '▁Jon': 9937,\n",
       "  'onn': 3409,\n",
       "  'ὁ': 31219,\n",
       "  'CP': 6271,\n",
       "  'PG': 16903,\n",
       "  '}=\\\\': 8738,\n",
       "  'ost': 520,\n",
       "  '.:\\u200a': 26847,\n",
       "  '+=': 23661,\n",
       "  'ologne': 13835,\n",
       "  '▁Од': 29770,\n",
       "  '▁Estado': 16763,\n",
       "  'arian': 13956,\n",
       "  'jas': 14196,\n",
       "  '▁Уи': 22505,\n",
       "  'jön': 29858,\n",
       "  '▁перед': 17702,\n",
       "  '▁brief': 11473,\n",
       "  '▁выступа': 24657,\n",
       "  '▁También': 18247,\n",
       "  '▁attached': 10959,\n",
       "  'кая': 22972,\n",
       "  '▁SSH': 22343,\n",
       "  'álva': 22590,\n",
       "  '▁Ge': 1879,\n",
       "  '▁pół': 21232,\n",
       "  'putation': 14584,\n",
       "  '▁anci': 7242,\n",
       "  'Part': 7439,\n",
       "  '▁dispose': 27905,\n",
       "  'цу': 9062,\n",
       "  '▁Douglas': 16721,\n",
       "  '▁functionality': 9863,\n",
       "  'icted': 18186,\n",
       "  '▁storage': 8635,\n",
       "  '▁understanding': 8004,\n",
       "  'ктиче': 28616,\n",
       "  '▁del': 628,\n",
       "  '▁Укра': 15949,\n",
       "  '▁people': 2305,\n",
       "  'integr': 14146,\n",
       "  '▁headers': 9066,\n",
       "  '▁trigger': 7135,\n",
       "  'ansion': 9454,\n",
       "  'ome': 608,\n",
       "  'ít': 2468,\n",
       "  'ral': 1705,\n",
       "  '▁totally': 14909,\n",
       "  '▁Any': 3139,\n",
       "  '▁Росси': 7676,\n",
       "  'ured': 2955,\n",
       "  'Local': 7717,\n",
       "  '▁opera': 14495,\n",
       "  '▁alloc': 6643,\n",
       "  'ata': 532,\n",
       "  '▁Ess': 11044,\n",
       "  '▁Хо': 14120,\n",
       "  '▁Brun': 17116,\n",
       "  'ydro': 11279,\n",
       "  '▁Dia': 22866,\n",
       "  '▁text': 1426,\n",
       "  'edeut': 13032,\n",
       "  '▁minim': 6260,\n",
       "  '▁Chicago': 10059,\n",
       "  'factory': 14399,\n",
       "  '▁accomplish': 12709,\n",
       "  '▁compare': 7252,\n",
       "  'hus': 14116,\n",
       "  'odn': 22452,\n",
       "  'веде': 12055,\n",
       "  'anted': 9714,\n",
       "  '▁Hans': 6971,\n",
       "  '▁Sebastian': 26631,\n",
       "  '▁deix': 26801,\n",
       "  '▁possibly': 10075,\n",
       "  '▁%>': 6580,\n",
       "  'ісля': 13297,\n",
       "  '▁hum': 3165,\n",
       "  'Autres': 27932,\n",
       "  '▁ž': 6145,\n",
       "  'uck': 2707,\n",
       "  '<0x00>': 3,\n",
       "  'auto': 6921,\n",
       "  'érique': 20600,\n",
       "  '▁amely': 18260,\n",
       "  '▁„': 1768,\n",
       "  'Ü': 30104,\n",
       "  '楽': 31739,\n",
       "  '▁bý': 24964,\n",
       "  '▁medi': 14457,\n",
       "  '▁тех': 11425,\n",
       "  '▁lum': 19703,\n",
       "  'Az': 16748,\n",
       "  '▁Chrome': 10228,\n",
       "  'étr': 18949,\n",
       "  '▁Islands': 17839,\n",
       "  '▁Syl': 19628,\n",
       "  '▁lex': 19566,\n",
       "  '典': 31259,\n",
       "  'germeister': 25382,\n",
       "  'ugust': 2781,\n",
       "  'ția': 17160,\n",
       "  '▁extern': 3622,\n",
       "  '▁denomin': 14267,\n",
       "  'ヤ': 31258,\n",
       "  'NN': 10262,\n",
       "  'jections': 24247,\n",
       "  'ategy': 8963,\n",
       "  'ным': 4470,\n",
       "  'usz': 10958,\n",
       "  '\\x94': 30361,\n",
       "  'bad': 12313,\n",
       "  '▁sent': 2665,\n",
       "  '▁lingu': 21110,\n",
       "  '▁mid': 7145,\n",
       "  '▁Marie': 9932,\n",
       "  '▁quella': 16189,\n",
       "  '▁sql': 4576,\n",
       "  'ination': 3381,\n",
       "  '▁fr': 1424,\n",
       "  'validate': 15480,\n",
       "  'ське': 19000,\n",
       "  '▁Après': 13861,\n",
       "  'cot': 26235,\n",
       "  '▁pulling': 28420,\n",
       "  '▁implicitly': 27063,\n",
       "  '▁Getting': 24162,\n",
       "  '▁Melbourne': 22103,\n",
       "  '▁Looks': 19887,\n",
       "  'Length': 6513,\n",
       "  '▁również': 20316,\n",
       "  '▁serait': 25810,\n",
       "  '++': 1817,\n",
       "  '▁як': 9285,\n",
       "  'actual': 19304,\n",
       "  '▁serious': 10676,\n",
       "  'alling': 27855,\n",
       "  '▁dense': 20619,\n",
       "  '▁Art': 3012,\n",
       "  '++;': 9107,\n",
       "  '▁blo': 6668,\n",
       "  '▁Republican': 21178,\n",
       "  'Exception': 2451,\n",
       "  'leans': 25210,\n",
       "  '▁imply': 22366,\n",
       "  'intern': 14168,\n",
       "  '▁independ': 11061,\n",
       "  '▁коро': 10210,\n",
       "  '▁iss': 1721,\n",
       "  '▁failure': 10672,\n",
       "  '▁sprite': 29227,\n",
       "  'iors': 18930,\n",
       "  '▁pressing': 24795,\n",
       "  'чная': 23006,\n",
       "  'high': 9812,\n",
       "  'flex': 16041,\n",
       "  'owym': 15105,\n",
       "  '▁Lig': 21894,\n",
       "  '▁config': 2295,\n",
       "  'ouble': 2074,\n",
       "  '::': 1057,\n",
       "  '▁tick': 16892,\n",
       "  'guard': 17728,\n",
       "  '▁Bundle': 24470,\n",
       "  '▁CREATE': 14602,\n",
       "  'ologique': 20015,\n",
       "  'datei': 27563,\n",
       "  '▁closely': 16467,\n",
       "  'ensis': 6322,\n",
       "  '▁Vers': 10138,\n",
       "  'phi': 2876,\n",
       "  '▁overwrite': 26556,\n",
       "  '<0x4C>': 79,\n",
       "  '▁debido': 21197,\n",
       "  '▁sweet': 14225,\n",
       "  '▁notable': 18697,\n",
       "  '▁eredetiből': 27900,\n",
       "  '<0x9D>': 160,\n",
       "  'зна': 4372,\n",
       "  '▁Register': 12577,\n",
       "  '▁SK': 18581,\n",
       "  '▁Haus': 10727,\n",
       "  '▁région': 14326,\n",
       "  '▁Food': 25453,\n",
       "  'юр': 24872,\n",
       "  'erten': 19106,\n",
       "  '▁прав': 16901,\n",
       "  '单': 31166,\n",
       "  '▁Magic': 26494,\n",
       "  'Shared': 21741,\n",
       "  'Ste': 7789,\n",
       "  'ем': 3098,\n",
       "  'рова': 4962,\n",
       "  'tersuch': 26547,\n",
       "  'neut': 17821,\n",
       "  '▁friend': 5121,\n",
       "  '▁Ath': 9193,\n",
       "  'By': 2059,\n",
       "  '}^{\\\\': 6292,\n",
       "  '<0x52>': 85,\n",
       "  '▁girl': 7826,\n",
       "  'Num': 8009,\n",
       "  '!\"': 3850,\n",
       "  'timer': 20404,\n",
       "  'tu': 9161,\n",
       "  'term': 8489,\n",
       "  '▁algunos': 20071,\n",
       "  '面': 30806,\n",
       "  '5': 29945,\n",
       "  'lea': 20774,\n",
       "  '▁Fac': 14184,\n",
       "  '▁Mont': 4526,\n",
       "  '▁lors': 7628,\n",
       "  '▁Identifier': 20286,\n",
       "  '▁calcio': 29515,\n",
       "  'ethe': 23043,\n",
       "  '▁quand': 18097,\n",
       "  'Ġ': 31937,\n",
       "  '▁inten': 17818,\n",
       "  '▁Gas': 19141,\n",
       "  'itut': 12937,\n",
       "  'bet': 6878,\n",
       "  'Psi': 14118,\n",
       "  '▁Orts': 22007,\n",
       "  '▁label': 3858,\n",
       "  '▁reform': 11736,\n",
       "  'ORDER': 22364,\n",
       "  'യ': 30674,\n",
       "  'scala': 15820,\n",
       "  'лович': 27115,\n",
       "  '⊗': 31486,\n",
       "  'aye': 15802,\n",
       "  'Btn': 20808,\n",
       "  'centering': 9525,\n",
       "  '▁objects': 3618,\n",
       "  '우': 31327,\n",
       "  '▁FA': 13515,\n",
       "  'wald': 18370,\n",
       "  '▁remains': 9242,\n",
       "  '▁speak': 7726,\n",
       "  '▁róż': 28801,\n",
       "  '▁u': 318,\n",
       "  'al': 284,\n",
       "  '▁periods': 23704,\n",
       "  'examples': 19057,\n",
       "  '動': 31124,\n",
       "  '▁Auflage': 16028,\n",
       "  'ША': 10941,\n",
       "  '▁ext': 1294,\n",
       "  'zess': 28790,\n",
       "  'super': 9136,\n",
       "  '▁Grace': 23350,\n",
       "  '▁Month': 23471,\n",
       "  '▁geg': 21598,\n",
       "  'opo': 12861,\n",
       "  '添': 31538,\n",
       "  'ོ': 31104,\n",
       "  'marks': 22848,\n",
       "  '▁tasks': 9595,\n",
       "  'p': 29886,\n",
       "  '▁posterior': 13446,\n",
       "  'oplus': 17201,\n",
       "  'та': 676,\n",
       "  'calc': 28667,\n",
       "  '▁Body': 24928,\n",
       "  '▁buy': 15649,\n",
       "  '($': 1566,\n",
       "  '▁Jugend': 19472,\n",
       "  '교': 31972,\n",
       "  '▁Республи': 21087,\n",
       "  'пня': 14840,\n",
       "  'AG': 10051,\n",
       "  '▁tournament': 14743,\n",
       "  '嘉': 31961,\n",
       "  '學': 31274,\n",
       "  '▁tempt': 25782,\n",
       "  '▁uso': 17448,\n",
       "  '▁victim': 28985,\n",
       "  '{(': 8001,\n",
       "  '▁Wright': 22927,\n",
       "  '▁réalisé': 24984,\n",
       "  '▁использова': 17442,\n",
       "  'slant': 17139,\n",
       "  '判': 31791,\n",
       "  'Memory': 16015,\n",
       "  '▁directeur': 26114,\n",
       "  'ocument': 4463,\n",
       "  '▁dz': 9275,\n",
       "  'aty': 11156,\n",
       "  'ə': 30184,\n",
       "  'success': 8698,\n",
       "  'ну': 1864,\n",
       "  'ο': 30123,\n",
       "  'bie': 10993,\n",
       "  'aussian': 17019,\n",
       "  '▁Otto': 13832,\n",
       "  '▁explo': 16035,\n",
       "  '▁preserve': 19905,\n",
       "  '▁октября': 16482,\n",
       "  'thur': 9743,\n",
       "  '▁ups': 24081,\n",
       "  '▁height': 3171,\n",
       "  '▁LE': 11060,\n",
       "  '.....': 18598,\n",
       "  '▁navig': 12402,\n",
       "  '▁Schön': 28565,\n",
       "  'unto': 12578,\n",
       "  'Short': 21322,\n",
       "  'oken': 4476,\n",
       "  'antic': 7716,\n",
       "  '▁Ron': 11546,\n",
       "  'овий': 18944,\n",
       "  'igner': 21216,\n",
       "  '▁church': 6586,\n",
       "  'oss': 2209,\n",
       "  '▁Michael': 5765,\n",
       "  'argent': 23641,\n",
       "  '▁curves': 19684,\n",
       "  '▁zones': 20542,\n",
       "  '▁inside': 2768,\n",
       "  '▁ду': 9023,\n",
       "  'Datos': 6086,\n",
       "  'ices': 1575,\n",
       "  'onymous': 11428,\n",
       "  '▁то': 2721,\n",
       "  '∀': 30315,\n",
       "  'lop': 4757,\n",
       "  '▁frequency': 10868,\n",
       "  '▁commence': 24108,\n",
       "  'ново': 29525,\n",
       "  'rus': 15816,\n",
       "  'äger': 19586,\n",
       "  '▁Apol': 28017,\n",
       "  '▁optimal': 14413,\n",
       "  '▁dies': 2977,\n",
       "  '_*': 24563,\n",
       "  'txt': 3945,\n",
       "  'ic': 293,\n",
       "  'hora': 15255,\n",
       "  '▁tea': 23429,\n",
       "  '▁Га': 8544,\n",
       "  'viously': 16604,\n",
       "  'ver': 369,\n",
       "  'dit': 27423,\n",
       "  'ternoon': 16691,\n",
       "  '▁maggior': 22159,\n",
       "  'ège': 15532,\n",
       "  'ц': 29996,\n",
       "  '▁необ': 23842,\n",
       "  'ierungs': 21212,\n",
       "  'orm': 555,\n",
       "  '▁provision': 25161,\n",
       "  '▁Tom': 4335,\n",
       "  '▁Pse': 17646,\n",
       "  'каз': 16602,\n",
       "  '<0x94>': 151,\n",
       "  '▁azure': 15699,\n",
       "  '▁constructor': 5823,\n",
       "  'cile': 21873,\n",
       "  '▁chron': 17168,\n",
       "  'AndroidRuntime': 12746,\n",
       "  '▁perd': 11113,\n",
       "  'қ': 30609,\n",
       "  '<0x3B>': 62,\n",
       "  '▁Deux': 26079,\n",
       "  '▁mieszkańców': 28589,\n",
       "  '▁brings': 23522,\n",
       "  'lö': 27575,\n",
       "  '西': 30602,\n",
       "  'ḩ': 31444,\n",
       "  'kow': 7000,\n",
       "  '▁sons': 18025,\n",
       "  \"!'\": 20714,\n",
       "  '▁Final': 9550,\n",
       "  '.\");': 18327,\n",
       "  'ńskiego': 22199,\n",
       "  'vés': 28010,\n",
       "  '▁erano': 15584,\n",
       "  'classes': 13203,\n",
       "  '▁flav': 21054,\n",
       "  '▁generate': 5706,\n",
       "  '▁notify': 26051,\n",
       "  '▁l': 301,\n",
       "  '▁Geography': 27340,\n",
       "  'cript': 924,\n",
       "  '▁Ric': 13675,\n",
       "  'ermeister': 23282,\n",
       "  'Batch': 23145,\n",
       "  'Objects': 12724,\n",
       "  'öt': 9618,\n",
       "  '▁alternate': 25010,\n",
       "  '▁introduce': 14944,\n",
       "  'uo': 25608,\n",
       "  'ABC': 19658,\n",
       "  'sv': 4501,\n",
       "  'David': 19504,\n",
       "  'မ': 31233,\n",
       "  '▁Crow': 29445,\n",
       "  'clean': 14941,\n",
       "  '▁engl': 19784,\n",
       "  'varepsilon': 6018,\n",
       "  '\\u200c': 30430,\n",
       "  '▁puts': 15223,\n",
       "  '▁apar': 19173,\n",
       "  'oun': 1309,\n",
       "  'eggi': 28137,\n",
       "  'üb': 20518,\n",
       "  '▁kw': 9049,\n",
       "  '▁credit': 16200,\n",
       "  '▁gods': 27379,\n",
       "  'туа': 29412,\n",
       "  'ли': 644,\n",
       "  'мия': 17682,\n",
       "  'addClass': 20213,\n",
       "  'objects': 12650,\n",
       "  'Pixel': 29637,\n",
       "  'ulos': 19733,\n",
       "  'DOM': 22141,\n",
       "  '▁информа': 25565,\n",
       "  '▁Settings': 19215,\n",
       "  'country': 13509,\n",
       "  '_{{\\\\': 24033,\n",
       "  'wy': 12822,\n",
       "  '▁May': 2610,\n",
       "  'osa': 3628,\n",
       "  'łoż': 20156,\n",
       "  'oti': 15297,\n",
       "  'ется': 4364,\n",
       "  '▁Feder': 7351,\n",
       "  '▁contacts': 25957,\n",
       "  ...},\n",
       " 32000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab, tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa3656",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c46097",
   "metadata": {},
   "source": [
    "Para hacer mas eficiente el worfklow vamos a realizar 'quantization'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7bb5c5",
   "metadata": {},
   "source": [
    "![Quant1](./Images/Quant1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a1661f",
   "metadata": {},
   "source": [
    "![Quant1](./Images/Quant2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41ba940",
   "metadata": {},
   "source": [
    "Fuente: P. Iusztin & M. Labonne - LLM Engineer's Handbook - Chapter 8 - Inference Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "887c28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-bit quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f2f763",
   "metadata": {},
   "source": [
    "Definimos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "690e4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "705f046c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aed6f683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "LlamaForCausalLM                                   --\n",
       "├─LlamaModel: 1-1                                  --\n",
       "│    └─Embedding: 2-1                              65,536,000\n",
       "│    └─ModuleList: 2-2                             --\n",
       "│    │    └─LlamaDecoderLayer: 3-1                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-2                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-3                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-4                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-5                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-6                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-7                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-8                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-9                 22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-10                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-11                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-12                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-13                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-14                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-15                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-16                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-17                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-18                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-19                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-20                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-21                22,024,192\n",
       "│    │    └─LlamaDecoderLayer: 3-22                22,024,192\n",
       "│    └─LlamaRMSNorm: 2-3                           2,048\n",
       "│    └─LlamaRotaryEmbedding: 2-4                   --\n",
       "├─Linear: 1-2                                      65,536,000\n",
       "===========================================================================\n",
       "Total params: 615,606,272\n",
       "Trainable params: 131,164,160\n",
       "Non-trainable params: 484,442,112\n",
       "==========================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a365d77",
   "metadata": {},
   "source": [
    "Probemos el sistema RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11820608",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which company open-sourced Llama-based models and for what purpose?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52a98110",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, raw_answer, answer, ctx, token_count = rag_answer(\n",
    "    base_model, tokenizer, question, 3, 256, 128, \"faiss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a203a6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \n",
      "    You are a helpful assistant specialized in technology news.\n",
      "    Use ONLY the context below to answer the user question.\n",
      "    If the answer is not in the context, say I don't know.\n",
      "    Answer ONE short sentence. Do NOT repeat context or question\n",
      "    \n",
      "  Question: Which company open-sourced Llama-based models and for what purpose?\n",
      "    \n",
      "  Context: Meta open-sourced a set of Llama-based models with billions of parameters, \n",
      "    enabling researchers and companies to fine-tune them for their own use cases.\n",
      "    \n",
      " Answer:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrompt:\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2d7bcfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt token count: 140\n"
     ]
    }
   ],
   "source": [
    "print(\"Prompt token count:\", token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1f01ae20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Contexts:\n",
      "\n",
      "--- Context 1 ---\n",
      "Meta open-sourced a set of Llama-based models with billions of parameters, \n",
      "    enabling researchers and companies to fine-tune them for their own use cases. \n",
      "\n",
      "--- Context 2 ---\n",
      "Hugging Face launched a new inference API tier with higher throughput and native \n",
      "    support for vLLM, making it cheaper to serve models like Mistral-7B and Llama-3-8B. \n",
      "\n",
      "--- Context 3 ---\n",
      "Apple reportedly began testing on-device LLMs for future iPhone models, enabling \n",
      "    private AI features such as offline summarization and personal context reasoning. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieved Contexts:\\n\")\n",
    "for i, c in enumerate(ctx, 1):\n",
    "    print(f\"--- Context {i} ---\")\n",
    "    print(c.strip(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e767601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw RAG Answer: \n",
      "    You are a helpful assistant specialized in technology news.\n",
      "    Use ONLY the context below to answer the user question.\n",
      "    If the answer is not in the context, say I don't know.\n",
      "    Answer ONE short sentence. Do NOT repeat context or question\n",
      "    \n",
      "  Question: Which company open-sourced Llama-based models and for what purpose?\n",
      "    \n",
      "  Context: Meta open-sourced a set of Llama-based models with billions of parameters, \n",
      "    enabling researchers and companies to fine-tune them for their own use cases.\n",
      "    \n",
      " Answer:\n",
      "     Meta's Llama-based models are open-sourced for the purpose of enabling researchers and companies to fine-tune them for their own use cases.\n",
      "     The models are based on Llama, a popular open-source machine learning library.\n",
      "     Meta's Llama-based models are available for research and development purposes.\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw RAG Answer:\", raw_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50d04d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Answer:\n",
      " Meta's Llama-based models are open-sourced for the purpose of enabling researchers and companies to fine-tune them for their own use cases.\n",
      "     The models are based on Llama, a popular open-source machine learning library.\n",
      "     Meta's Llama-based models are available for research and development purposes.\n"
     ]
    }
   ],
   "source": [
    "print('RAG Answer:\\n', answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21e67a",
   "metadata": {},
   "source": [
    "Veamos algo interesante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "201e2a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which company open-sourced Llama-based models and for what purpose?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d56abbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, question, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36a846cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8ee27c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f527fa",
   "metadata": {},
   "source": [
    "Usemos de nuevo el prompt anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c486d2ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    You are a helpful assistant specialized in technology news.\\n    Use ONLY the context below to answer the user question.\\n    If the answer is not in the context, say I don't know.\\n    Answer ONE short sentence. Do NOT repeat context or question\\n    \\n  Question: Which company open-sourced Llama-based models and for what purpose?\\n    \\n  Context: Meta open-sourced a set of Llama-based models with billions of parameters, \\n    enabling researchers and companies to fine-tune them for their own use cases.\\n    \\n Answer:\\n    \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f227fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " Meta open-sourced Llama-based models for research and development.\n",
      "     The models are used for various applications, including natural language processing, \n",
      "     computer vision, and machine learning.\n",
      "     The open-sourcing of these models is a significant step towards making them more accessible to the wider community.\n",
      "     The models are available for researchers and companies to fine-tune for their own use cases.\n",
      "     Meta's commitment to open-sourcing its technology is a testament to its commitment to innovation and collaboration.\n"
     ]
    }
   ],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, prompt, 256, 128)\n",
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a5d22b",
   "metadata": {},
   "source": [
    "Creamos un nuevo prompt que tenga ordenes, pregunta y espacio para respuesta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbdf1ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "ONLY answer the question below. Do NOT repeat the question below in the answer.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4881867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " Llama-based models are open-sourced by Google. They are used for image recognition and object detection tasks. The company uses these models for various applications, including autonomous vehicles, medical imaging, and security.\n"
     ]
    }
   ],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, prompt, 256, 128)\n",
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b5197",
   "metadata": {},
   "source": [
    "Veamos esto que también es interesante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20e4b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"{question}:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0bdbb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which company open-sourced Llama-based models and for what purpose?:'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9157fbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " Llama is a machine learning library for Python, which is open-sourced by its author, <|assistant|> (https://github.com/johannesbuchner/llama). The library is designed to be easy to use and powerful, and it provides a wide range of tools for building and deploying machine learning models.\n",
      "\n",
      "The main purpose of Llama is to provide a powerful and flexible framework for building and deploying machine learning models. It is designed to be easy to use, with a focus on simplicity and ease of use. Llama provides a wide range of tools for\n"
     ]
    }
   ],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, prompt, 256, 128)\n",
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf294f9",
   "metadata": {},
   "source": [
    "Veamos ahora como hacer fine tuning con un toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "76bab45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/qa_data.json\", \"r\") as file:\n",
    "    qa_pairs = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd1ad44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Which company released a new reasoning model optimized for tool use and RAG?',\n",
       "  'input': '',\n",
       "  'output': 'OpenAI released a new model optimized for tool use and retrieval-augmented generation.'},\n",
       " {'instruction': 'What did Google update to make it easier to deploy large language models?',\n",
       "  'input': '',\n",
       "  'output': 'Google updated its Vertex AI platform to make it easier to deploy and monitor large language models.'},\n",
       " {'instruction': 'Which company open-sourced Llama-based models and why is this important?',\n",
       "  'input': '',\n",
       "  'output': 'Meta open-sourced Llama-based models, enabling researchers and companies to fine-tune them for their own use cases.'},\n",
       " {'instruction': 'What AI features did Microsoft add to Office?',\n",
       "  'input': '',\n",
       "  'output': 'Microsoft added generative AI features to Office, including AI-powered summarization, drafting, and meeting notes.'},\n",
       " {'instruction': 'What did AWS introduce for inference workloads?',\n",
       "  'input': '',\n",
       "  'output': 'AWS introduced cheaper GPU instances optimized for inference workloads like chatbots, code assistants, and document question-answering.'},\n",
       " {'instruction': 'What did NVIDIA release to accelerate transformer inference?',\n",
       "  'input': '',\n",
       "  'output': 'NVIDIA released open-source libraries that accelerate transformer inference, improving performance on consumer GPUs.'},\n",
       " {'instruction': 'What research focus did Anthropic publish new work on?',\n",
       "  'input': '',\n",
       "  'output': 'Anthropic published research on improving constitutional AI with scalable oversight and safer model behavior.'},\n",
       " {'instruction': 'What AI capability is Apple reportedly testing for iPhones?',\n",
       "  'input': '',\n",
       "  'output': 'Apple is testing on-device LLMs that enable private offline capabilities like summarization and personal context reasoning.'},\n",
       " {'instruction': 'What did Hugging Face launch to improve model serving?',\n",
       "  'input': '',\n",
       "  'output': 'Hugging Face launched a new inference API tier with high throughput and native vLLM support.'},\n",
       " {'instruction': 'Which company released the Mixtral-8x22B model and what type of model is it?',\n",
       "  'input': '',\n",
       "  'output': 'Mistral AI released Mixtral-8x22B, a sparse mixture-of-experts model that provides state-of-the-art performance efficiently.'},\n",
       " {'instruction': 'What partnership did IBM announce involving geospatial data?',\n",
       "  'input': '',\n",
       "  'output': 'IBM partnered with NASA to fine-tune foundation models on geospatial data to improve climate and satellite analysis.'},\n",
       " {'instruction': 'What model did Databricks release and what is notable about it?',\n",
       "  'input': '',\n",
       "  'output': 'Databricks released DBRX, a 132B-parameter MoE model trained on curated scientific and enterprise datasets.'},\n",
       " {'instruction': 'What did Stability AI introduce with improved text-image alignment?',\n",
       "  'input': '',\n",
       "  'output': 'Stability AI introduced Stable Diffusion 3, offering better text-image alignment and reduced hallucinations.'},\n",
       " {'instruction': 'What new capability did Snowflake add for enterprise AI pipelines?',\n",
       "  'input': '',\n",
       "  'output': 'Snowflake added native vector search, enabling RAG workflows directly within its data warehouse.'},\n",
       " {'instruction': 'What product did Cohere launch for retrieval and semantic search?',\n",
       "  'input': '',\n",
       "  'output': 'Cohere launched an enterprise-grade embedding model designed for semantic search and multilingual retrieval.'},\n",
       " {'instruction': 'What AI enhancement did Red Hat introduce for DevOps workflows?',\n",
       "  'input': '',\n",
       "  'output': 'Red Hat introduced AI-enhanced DevOps tools, including automated deployment validation powered by small LLMs.'},\n",
       " {'instruction': 'What updates did Salesforce make to Einstein GPT?',\n",
       "  'input': '',\n",
       "  'output': 'Salesforce updated Einstein GPT with improved CRM reasoning, including lead scoring and automated email drafting.'},\n",
       " {'instruction': 'What AI feature did Dropbox add to help users navigate their files?',\n",
       "  'input': '',\n",
       "  'output': 'Dropbox added AI-powered universal search that allows semantic querying across documents, PDFs, and images.'},\n",
       " {'instruction': 'How is Slack using AI to help teams stay informed?',\n",
       "  'input': '',\n",
       "  'output': 'Slack added AI summarization for channels and threads, generating digests and extracting key decisions.'},\n",
       " {'instruction': 'What real-time AI capabilities did Zoom add to its platform?',\n",
       "  'input': '',\n",
       "  'output': 'Zoom added real-time translation and AI-generated meeting action items powered by a multilingual transformer model.'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b7b20ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_list(qa_pairs)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f55f90f",
   "metadata": {},
   "source": [
    "### Supervised FT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5881e364",
   "metadata": {},
   "source": [
    "![ft1](./Images/FT1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f824a1cf",
   "metadata": {},
   "source": [
    "![ft2](./Images/FT2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc6d30e",
   "metadata": {},
   "source": [
    "![ft3](./Images/FT3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae264760",
   "metadata": {},
   "source": [
    "Fuente: P. Iusztin & M. Labonne - LLM Engineer's Handbook - Chapter 5 - Supervised Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aca8cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    # Alpaca-style formatting\n",
    "    if example[\"input\"]:\n",
    "        return f\"\"\"Below is an instruction and an input. Write a helpful answer.\n",
    "\n",
    "### Instruction:\n",
    "{example[\"instruction\"]}\n",
    "\n",
    "### Input:\n",
    "{example[\"input\"]}\n",
    "\n",
    "### Response:\n",
    "{example[\"output\"]}\n",
    "\"\"\"\n",
    "    else:\n",
    "        return f\"\"\"Below is an instruction. Write a helpful answer.\n",
    "\n",
    "### Instruction:\n",
    "{example[\"instruction\"]}\n",
    "\n",
    "### Response:\n",
    "{example[\"output\"]}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def tokenize_function(example):\n",
    "    text = format_example(example)\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # For causal LM, labels = input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "db21f402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5bdc9742034187a0f00c6e293c662d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=False)\n",
    "\n",
    "# Remove the original text columns\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"instruction\", \"input\", \"output\"])\n",
    "\n",
    "# Set format\n",
    "tokenized_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e04c01e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([    1, 13866,   338,   385, 15278, 29889, 14350,   263,  8444,  1234,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  8809,\n",
       "           436,  5001,  5492,   263,   716, 24481,  1904, 27545,   363,  5780,\n",
       "           671,   322,   390, 10051, 29973,    13,    13,  2277, 29937, 13291,\n",
       "         29901,    13,  6585, 23869,  5492,   263,   716,  1904, 27545,   363,\n",
       "          5780,   671,   322,  5663, 16837, 29899,  2987,   358,   287, 12623,\n",
       "         29889,    13,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([    1, 13866,   338,   385, 15278, 29889, 14350,   263,  8444,  1234,\n",
       "         29889,    13,    13,  2277, 29937,  2799,  4080, 29901,    13,  8809,\n",
       "           436,  5001,  5492,   263,   716, 24481,  1904, 27545,   363,  5780,\n",
       "           671,   322,   390, 10051, 29973,    13,    13,  2277, 29937, 13291,\n",
       "         29901,    13,  6585, 23869,  5492,   263,   716,  1904, 27545,   363,\n",
       "          5780,   671,   322,  5663, 16837, 29899,  2987,   358,   287, 12623,\n",
       "         29889,    13,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "             2,     2,     2,     2,     2,     2])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84f24d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3267f3",
   "metadata": {},
   "source": [
    "Definimos un nuevo modelo a realizar FT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e025bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name, quantization_config=bnb_config, device_map=\"auto\", use_cache=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3cb673ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"k_proj\",\n",
    "        \"o_proj\",\n",
    "    ],  # may need to adjust per model\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d26d1b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for k-bit training (LoRA on top of 4-bit base)\n",
    "ft_model.to(device).train()\n",
    "ft_model = prepare_model_for_kbit_training(ft_model)\n",
    "ft_model = get_peft_model(ft_model, lora_config)\n",
    "ft_model.gradient_checkpointing_enable(\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "ft_model.enable_input_require_grads()\n",
    "ft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce2a0727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "PeftModelForCausalLM                                              --\n",
       "├─LoraModel: 1-1                                                  --\n",
       "│    └─LlamaForCausalLM: 2-1                                      --\n",
       "│    │    └─LlamaModel: 3-1                                       552,323,072\n",
       "│    │    └─Linear: 3-2                                           (65,536,000)\n",
       "==========================================================================================\n",
       "Total params: 617,859,072\n",
       "Trainable params: 2,252,800\n",
       "Non-trainable params: 615,606,272\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(ft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f35a6e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(ft_model.parameters(), lr=1e-3, amsgrad=True, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a1ae04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 1 | Loss: 14.7345\n",
      "Epoch 1 | Step 2 | Loss: 8.1762\n",
      "Epoch 1 | Step 3 | Loss: 3.2726\n",
      "Epoch 1 | Step 4 | Loss: 0.9673\n",
      "Epoch 1 | Step 5 | Loss: 0.9050\n",
      "Epoch 1 | Step 6 | Loss: 0.9186\n",
      "Epoch 1 | Step 7 | Loss: 0.8490\n",
      "Epoch 1 | Step 8 | Loss: 0.8782\n",
      "Epoch 1 | Step 9 | Loss: 0.8042\n",
      "Epoch 1 | Step 10 | Loss: 0.6960\n",
      "== Epoch 1 finished | Avg loss: 3.2202 ==\n",
      "Epoch 2 | Step 1 | Loss: 0.7238\n",
      "Epoch 2 | Step 2 | Loss: 0.5138\n",
      "Epoch 2 | Step 3 | Loss: 0.6198\n",
      "Epoch 2 | Step 4 | Loss: 0.7040\n",
      "Epoch 2 | Step 5 | Loss: 0.5648\n",
      "Epoch 2 | Step 6 | Loss: 0.6583\n",
      "Epoch 2 | Step 7 | Loss: 0.4236\n",
      "Epoch 2 | Step 8 | Loss: 0.4197\n",
      "Epoch 2 | Step 9 | Loss: 0.4426\n",
      "Epoch 2 | Step 10 | Loss: 0.4661\n",
      "== Epoch 2 finished | Avg loss: 0.5537 ==\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        # batch is a dict of tensors with shape [batch_size, seq_len]\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = ft_model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1} | Step {step+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"== Epoch {epoch+1} finished | Avg loss: {avg_loss:.4f} ==\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1589ab",
   "metadata": {},
   "source": [
    "Vamos a cargar el modelo ya entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c96534b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapter to: ./tinyllama-tech-lora\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"./tinyllama-tech-lora\"\n",
    "ft_model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Saved LoRA adapter to:\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2e342407",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "13df8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load FT Model\n",
    "# ft_model = AutoModelForCausalLM.from_pretrained(output_dir)\n",
    "\n",
    "# # Load the tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9d1f839d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3eae8",
   "metadata": {},
   "source": [
    "Nuevamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c54ea131",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which company open-sourced Llama-based models and for what purpose?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9f42163e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "Caching is incompatible with gradient checkpointing in LlamaDecoderLayer. Setting `past_key_values=None`.\n"
     ]
    }
   ],
   "source": [
    "raw_answer, answer = generate_answer(ft_model, tokenizer, question, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ada362d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d21572fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which company open-sourced Llama-based models and for what purpose?:'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9231c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_answer, answer = generate_answer(ft_model, tokenizer, prompt, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4a6b41a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9a66f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAG (no FT) ---\n",
      "Meta open-sourced Llama-based models for research and development.\n",
      "     The models are designed for tasks such as image classification, object detection, and language modeling.\n",
      "     The open-sourcing of Llama models will enable researchers and companies to fine-tune them for their own use cases.\n",
      "     The models are designed for tasks such as image classification, object detection, and language modeling.\n",
      "     Meta open-sourced Llama-based models for research and development.\n",
      "     The models are designed for tasks such as image classification, object detection, and language model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- RAG (no FT) ---\")\n",
    "prompt, full_output, gen_output, contexts, token_count = rag_answer(\n",
    "    base_model, tokenizer, question, 3, 256, 128, 'faiss'\n",
    ")\n",
    "print(gen_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ab86353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Which company open-sourced Llama-based models and for what purpose?'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "79fbd5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-tuned + RAG ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Fine-tuned + RAG ---\")\n",
    "_, raw_answer, answer, _, _ = rag_answer(\n",
    "    ft_model, tokenizer, question, 3, 256, 128, \"faiss\"\n",
    ")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef0e63",
   "metadata": {},
   "source": [
    "Ahora, vamos a entrenar un modelo un poco más grande desde HF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5049b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ds = load_dataset(\"HuggingFaceH4/ultrafeedback_binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22982131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n",
      "    num_rows: 61135\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_sft = raw_ds[\"train_sft\"]\n",
    "test_sft = raw_ds[\"test_sft\"]\n",
    "pprint(train_sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8da39beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chosen': [{'content': 'how can i develop a habit of drawing daily',\n",
      "             'role': 'user'},\n",
      "            {'content': 'Developing a daily habit of drawing can be '\n",
      "                        'challenging but with consistent practice and a few '\n",
      "                        'tips, it can become an enjoyable and rewarding part '\n",
      "                        'of your daily routine. Here are some strategies to '\n",
      "                        'help you develop the habit of drawing daily:\\n'\n",
      "                        '\\n'\n",
      "                        '1. Set a specific time: Allocate a specific time of '\n",
      "                        'the day to draw. It could be in the morning, '\n",
      "                        'afternoon, or evening. Make drawing a part of your '\n",
      "                        'daily routine.\\n'\n",
      "                        '2. Set a specific duration: Determine the amount of '\n",
      "                        'time you want to spend on drawing each day. It can be '\n",
      "                        'as little as 10 minutes or as long as an hour. Be '\n",
      "                        'consistent with the duration to help build the '\n",
      "                        'habit.\\n'\n",
      "                        \"3. Start small and simple: Don't try to create a \"\n",
      "                        'masterpiece every day, start with simple and '\n",
      "                        'easy-to-do sketches. Focus on improving your skills '\n",
      "                        'gradually.\\n'\n",
      "                        '4. Use a variety of tools and mediums: Experiment '\n",
      "                        'with different tools like pencils, pens, markers, and '\n",
      "                        'different mediums like paper, canvas, or digital apps '\n",
      "                        'to keep your drawing practice interesting and '\n",
      "                        'engaging.\\n'\n",
      "                        '5. Take breaks and rest: Taking breaks and resting '\n",
      "                        'after some time of drawing can help you avoid burnout '\n",
      "                        'and stay motivated.\\n'\n",
      "                        '6. Challenge yourself: Set challenges like drawing '\n",
      "                        'objects from memory or a specific subject to improve '\n",
      "                        'your skills and keep your drawing practice '\n",
      "                        'interesting.\\n'\n",
      "                        '7. Track your progress: Keep a record of your daily '\n",
      "                        'drawing practice and track your progress. This can be '\n",
      "                        'a source of motivation and help you see how far '\n",
      "                        \"you've come.\\n\"\n",
      "                        '\\n'\n",
      "                        'Remember, developing a habit takes time and patience. '\n",
      "                        'Stay consistent with your drawing practice, be '\n",
      "                        'flexible and open to trying new things, and with '\n",
      "                        \"time, you'll develop a habit of daily drawing that \"\n",
      "                        'brings you joy and satisfaction.',\n",
      "             'role': 'assistant'}],\n",
      " 'messages': [{'content': 'how can i develop a habit of drawing daily',\n",
      "               'role': 'user'},\n",
      "              {'content': 'Developing a daily habit of drawing can be '\n",
      "                          'challenging but with consistent practice and a few '\n",
      "                          'tips, it can become an enjoyable and rewarding part '\n",
      "                          'of your daily routine. Here are some strategies to '\n",
      "                          'help you develop the habit of drawing daily:\\n'\n",
      "                          '\\n'\n",
      "                          '1. Set a specific time: Allocate a specific time of '\n",
      "                          'the day to draw. It could be in the morning, '\n",
      "                          'afternoon, or evening. Make drawing a part of your '\n",
      "                          'daily routine.\\n'\n",
      "                          '2. Set a specific duration: Determine the amount of '\n",
      "                          'time you want to spend on drawing each day. It can '\n",
      "                          'be as little as 10 minutes or as long as an hour. '\n",
      "                          'Be consistent with the duration to help build the '\n",
      "                          'habit.\\n'\n",
      "                          \"3. Start small and simple: Don't try to create a \"\n",
      "                          'masterpiece every day, start with simple and '\n",
      "                          'easy-to-do sketches. Focus on improving your skills '\n",
      "                          'gradually.\\n'\n",
      "                          '4. Use a variety of tools and mediums: Experiment '\n",
      "                          'with different tools like pencils, pens, markers, '\n",
      "                          'and different mediums like paper, canvas, or '\n",
      "                          'digital apps to keep your drawing practice '\n",
      "                          'interesting and engaging.\\n'\n",
      "                          '5. Take breaks and rest: Taking breaks and resting '\n",
      "                          'after some time of drawing can help you avoid '\n",
      "                          'burnout and stay motivated.\\n'\n",
      "                          '6. Challenge yourself: Set challenges like drawing '\n",
      "                          'objects from memory or a specific subject to '\n",
      "                          'improve your skills and keep your drawing practice '\n",
      "                          'interesting.\\n'\n",
      "                          '7. Track your progress: Keep a record of your daily '\n",
      "                          'drawing practice and track your progress. This can '\n",
      "                          'be a source of motivation and help you see how far '\n",
      "                          \"you've come.\\n\"\n",
      "                          '\\n'\n",
      "                          'Remember, developing a habit takes time and '\n",
      "                          'patience. Stay consistent with your drawing '\n",
      "                          'practice, be flexible and open to trying new '\n",
      "                          \"things, and with time, you'll develop a habit of \"\n",
      "                          'daily drawing that brings you joy and satisfaction.',\n",
      "               'role': 'assistant'}],\n",
      " 'prompt': 'how can i develop a habit of drawing daily',\n",
      " 'prompt_id': '086b3e24f29b8956a01059f79c56db35d118a06fb6b844b095737d042795cd43',\n",
      " 'rejected': [{'content': 'how can i develop a habit of drawing daily',\n",
      "               'role': 'user'},\n",
      "              {'content': 'As an AI language model, I cannot personally '\n",
      "                          'develop habits for you. But, here are some tips for '\n",
      "                          'developing a habit of drawing daily:\\n'\n",
      "                          '\\n'\n",
      "                          '1. Start small: Start with simple drawings or '\n",
      "                          'doodles and gradually increase the complexity of '\n",
      "                          'your drawings.\\n'\n",
      "                          '\\n'\n",
      "                          '2. Set a schedule: Set a specific time every day to '\n",
      "                          'draw, and try to stick to it as much as possible.\\n'\n",
      "                          '\\n'\n",
      "                          \"3. Make it fun: Don't pressure yourself to create \"\n",
      "                          'masterpieces every time you draw. Make it a '\n",
      "                          'relaxing and enjoyable experience.\\n'\n",
      "                          '\\n'\n",
      "                          '4. Use resources: There are many drawing tutorials '\n",
      "                          'available online. Use resources like YouTube or '\n",
      "                          'online drawing courses to help you improve your '\n",
      "                          'skills.\\n'\n",
      "                          '\\n'\n",
      "                          '5. Surround yourself with inspiration: Expose '\n",
      "                          'yourself to a variety of art forms, such as '\n",
      "                          'paintings, illustrations, and photographs, to '\n",
      "                          'inspire and motivate you.\\n'\n",
      "                          '\\n'\n",
      "                          'Remember, everyone has their own creative style and '\n",
      "                          'pace. Just keep practicing and enjoying the process '\n",
      "                          'of drawing.',\n",
      "               'role': 'assistant'}],\n",
      " 'score_chosen': 8.5,\n",
      " 'score_rejected': 8.5}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_sft[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4c9c332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ultrafeedback_to_qa(example):\n",
    "    \"\"\"\n",
    "    Convert one UltraFeedback example into your {instruction, input, output} format.\n",
    "    We'll use the 'messages' field.\n",
    "    \"\"\"\n",
    "    msgs = example[\"messages\"]\n",
    "\n",
    "    # last user message\n",
    "    user_msgs = [m[\"content\"] for m in msgs if m[\"role\"] == \"user\"]\n",
    "    # last assistant message\n",
    "    assistant_msgs = [m[\"content\"] for m in msgs if m[\"role\"] == \"assistant\"]\n",
    "\n",
    "    if not user_msgs or not assistant_msgs:\n",
    "        return {\n",
    "            \"instruction\": \"\",\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"\",\n",
    "        }\n",
    "\n",
    "    instruction = user_msgs[-1].strip()\n",
    "    output = assistant_msgs[-1].strip()\n",
    "\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": \"\",\n",
    "        \"output\": output,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b9af61cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For GPU sanity, start with a subset\n",
    "train_small = train_sft.shuffle().select(range(1000))\n",
    "eval_small = test_sft.shuffle().select(range(100))\n",
    "\n",
    "train_qa = [ultrafeedback_to_qa(ex) for ex in train_small]\n",
    "eval_qa = [ultrafeedback_to_qa(ex) for ex in eval_small]\n",
    "\n",
    "# Filter out empties\n",
    "train_qa = [ex for ex in train_qa if ex[\"instruction\"] and ex[\"output\"]]\n",
    "eval_qa = [ex for ex in eval_qa if ex[\"instruction\"] and ex[\"output\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4535d12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '',\n",
      " 'instruction': 'Given a pair of words, deduce the type of relationship '\n",
      "                \"between them. The various types of relations are: 'Entails, \"\n",
      "                'HasProperty, Synonym, Antonym, HasA, MemberOf, PartOf, '\n",
      "                \"MadeOf, IsA'. Let's denote the first word by X and the second \"\n",
      "                \"word by Y. An 'IsA' relation holds when 'X is a kind of Y'. \"\n",
      "                \"An 'Antonym' relation holds when 'X can be used as the \"\n",
      "                \"opposite of Y'. A 'Synonym' relation applies when 'X can be \"\n",
      "                \"used in place of Y, without changing the meaning'. A 'PartOf' \"\n",
      "                \"relation holds when 'X is a part of Y'. A 'MemberOf' relation \"\n",
      "                \"holds when 'X is a member of Y'. A 'MadeOf' relation holds \"\n",
      "                \"when 'X is made of Y'. An 'Entailment' relation holds when \"\n",
      "                \"'If X is true, then Y is true as well'. A 'HasA' relation \"\n",
      "                \"holds when 'X can have or contain Y'. A 'HasProperty' \"\n",
      "                \"relation holds when 'Y is to specify X'.\\n\"\n",
      "                '\\n'\n",
      "                'Example input: X: balance, Y: scale\\n'\n",
      "                'Example output: IsA\\n'\n",
      "                'Example explanation: Balance is a kind of scale.\\n'\n",
      "                'Q: X: act, Y: performance\\n'\n",
      "                'A:',\n",
      " 'output': \"Sure, I'd be happy to help! Based on the pair of words you've \"\n",
      "           'provided, the type of relationship between \"act\" and \"performance\" '\n",
      "           'is \"Entails\".\\n'\n",
      "           '\\n'\n",
      "           'The reason for this is that \"act\" can entail or imply the concept '\n",
      "           'of \"performance\". In other words, when someone performs an act, '\n",
      "           'they are engaging in a specific action or activity, and that '\n",
      "           'action or activity is what constitutes the performance. So, if '\n",
      "           '\"act\" is true, then \"performance\" is also true, as one implies the '\n",
      "           'other.\\n'\n",
      "           '\\n'\n",
      "           \"Please let me know if you have any further questions or if there's \"\n",
      "           'anything else I can help with!'}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_qa[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1f052e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(train_qa)\n",
    "eval_dataset = Dataset.from_list(eval_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0ffd41f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': '',\n",
      " 'instruction': 'Given a pair of words, deduce the type of relationship '\n",
      "                \"between them. The various types of relations are: 'Entails, \"\n",
      "                'HasProperty, Synonym, Antonym, HasA, MemberOf, PartOf, '\n",
      "                \"MadeOf, IsA'. Let's denote the first word by X and the second \"\n",
      "                \"word by Y. An 'IsA' relation holds when 'X is a kind of Y'. \"\n",
      "                \"An 'Antonym' relation holds when 'X can be used as the \"\n",
      "                \"opposite of Y'. A 'Synonym' relation applies when 'X can be \"\n",
      "                \"used in place of Y, without changing the meaning'. A 'PartOf' \"\n",
      "                \"relation holds when 'X is a part of Y'. A 'MemberOf' relation \"\n",
      "                \"holds when 'X is a member of Y'. A 'MadeOf' relation holds \"\n",
      "                \"when 'X is made of Y'. An 'Entailment' relation holds when \"\n",
      "                \"'If X is true, then Y is true as well'. A 'HasA' relation \"\n",
      "                \"holds when 'X can have or contain Y'. A 'HasProperty' \"\n",
      "                \"relation holds when 'Y is to specify X'.\\n\"\n",
      "                '\\n'\n",
      "                'Example input: X: balance, Y: scale\\n'\n",
      "                'Example output: IsA\\n'\n",
      "                'Example explanation: Balance is a kind of scale.\\n'\n",
      "                'Q: X: act, Y: performance\\n'\n",
      "                'A:',\n",
      " 'output': \"Sure, I'd be happy to help! Based on the pair of words you've \"\n",
      "           'provided, the type of relationship between \"act\" and \"performance\" '\n",
      "           'is \"Entails\".\\n'\n",
      "           '\\n'\n",
      "           'The reason for this is that \"act\" can entail or imply the concept '\n",
      "           'of \"performance\". In other words, when someone performs an act, '\n",
      "           'they are engaging in a specific action or activity, and that '\n",
      "           'action or activity is what constitutes the performance. So, if '\n",
      "           '\"act\" is true, then \"performance\" is also true, as one implies the '\n",
      "           'other.\\n'\n",
      "           '\\n'\n",
      "           \"Please let me know if you have any further questions or if there's \"\n",
      "           'anything else I can help with!'}\n"
     ]
    }
   ],
   "source": [
    "pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f7b7148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_ex(example):\n",
    "    instr = example[\"instruction\"]\n",
    "    inp = example[\"input\"]\n",
    "    out = example[\"output\"]\n",
    "\n",
    "    if inp:\n",
    "        prompt = (\n",
    "            \"Below is an instruction and additional input. \"\n",
    "            \"Write a helpful, honest, and concise response.\\n\\n\"\n",
    "            f\"### Instruction:\\n{instr}\\n\\n\"\n",
    "            f\"### Input:\\n{inp}\\n\\n\"\n",
    "            \"### Response:\\n\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            \"Below is an instruction. \"\n",
    "            \"Write a helpful, honest, and concise response.\\n\\n\"\n",
    "            f\"### Instruction:\\n{instr}\\n\\n\"\n",
    "            \"### Response:\\n\"\n",
    "        )\n",
    "\n",
    "    # For causal LM, we feed prompt + output as a single sequence\n",
    "    full_text = prompt + out\n",
    "    return {\"text\": full_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "042640fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f75e7c5c62e4332b65ccac0dc992aea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320ebe9b12104dcea6de4f415762b004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(format_ex)\n",
    "eval_dataset = eval_dataset.map(format_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b9438257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "84b7f0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948f948c9206484bafe3c067c0ea00b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ee313c8072142b0a1f6dc306c18f717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenized = train_dataset.map(\n",
    "    tokenize_fn, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "eval_tokenized = eval_dataset.map(\n",
    "    tokenize_fn, batched=True, remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "train_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "eval_tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "02162e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "033c0b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n"
     ]
    }
   ],
   "source": [
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "ft_model = prepare_model_for_kbit_training(ft_model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],  # typical for Llama-like\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "ft_model = get_peft_model(ft_model, lora_config)\n",
    "ft_model.gradient_checkpointing_enable(\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "ft_model.enable_input_require_grads()\n",
    "ft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6f207a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./tinyllama-ultrafeedback-lora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bf1f84c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # effective batch size = 8\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,  # start with 1 epoch; increase if stable\n",
    "    warmup_ratio=0.03,\n",
    "    logging_steps=20,\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    report_to=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "986e5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=ft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=eval_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7969619c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 1:27:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.805300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.600400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.500600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.539800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.568500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=1.5959888496398926, metrics={'train_runtime': 5291.6606, 'train_samples_per_second': 0.189, 'train_steps_per_second': 0.024, 'total_flos': 1595931623424000.0, 'train_loss': 1.5959888496398926, 'epoch': 1.0})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5195fd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved LoRA adapter to: ./tinyllama-ultrafeedback-lora-adapter\n"
     ]
    }
   ],
   "source": [
    "adapter_dir = \"./tinyllama-ultrafeedback-lora-adapter\"\n",
    "ft_model.save_pretrained(adapter_dir)\n",
    "tokenizer.save_pretrained(adapter_dir)\n",
    "print(\"Saved LoRA adapter to:\", adapter_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3cbbffb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_docs = [\n",
    "    f\"User: {ex['prompt']}\\n\\nAssistant: {ultrafeedback_to_qa(ex)['output']}\"\n",
    "    for ex in train_small\n",
    "]\n",
    "\n",
    "corpus_titles = [f\"UltraFeedback sample {i}\" for i in range(len(corpus_docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b26442d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472a1fb477fc4e8fb7927ad1b03d80ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute embeddings\n",
    "doc_embeddings = embedder.encode(\n",
    "    corpus_docs,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    device=device,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "doc_embeddings = np.array(doc_embeddings).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0144f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn\n",
    "nn_index = NearestNeighbors(n_neighbors=10, metric=\"cosine\")\n",
    "nn_index.fit(doc_embeddings)\n",
    "# Faiss\n",
    "faiss_emb = np.array(doc_embeddings).astype(\"float32\")\n",
    "faiss_index = faiss.IndexFlatIP(\n",
    "    faiss_emb.shape[1]\n",
    ")  # cosine similarity via inner product\n",
    "faiss.normalize_L2(faiss_emb)\n",
    "faiss_index.add(faiss_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "93da6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how can i develop a habit of drawing daily?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5597931d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, raw_answer, answer, ctx, token_count = rag_answer(\n",
    "    base_model, tokenizer, question, 3, 256, 128, \"faiss\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ab46d979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: \n",
      "    You are a helpful assistant specialized in technology news.\n",
      "    Use ONLY the context below to answer the user question.\n",
      "    If the answer is not in the context, say I don't know.\n",
      "    Answer ONE short sentence. Do NOT repeat context or question\n",
      "    \n",
      "  Question: how can i develop a habit of drawing daily?\n",
      "    \n",
      "  Context: User: Write a 1500-word report in APA format on the impact of mindfulness practices, such as meditation and breathing exercises, on enhancing creativity in individuals. Use at least three scholarly articles from peer-reviewed journals as sources, and include examples of how mindfulness has been applied in various creative fields, such as music, writing, and visual arts. Additionally, discuss potential limitations or challenges of incorporating mindfulness into a creative practice, and provide recommendations for those interested in implementing mindfulness into their own creative process.\n",
      "\n",
      "Assistant: Title: The Impact of Mindfulness Practices on Enhancing Creativity in Individuals\n",
      "\n",
      "Abstract\n",
      "\n",
      "The purpose of this paper is to explore the impact of mindfulness practices, such as meditation and breathing exercises, on enhancing creativity in individuals. The paper discusses recent research on the topic, including examples of how mindfulness has been applied in various creative fields, such as music, writing, and visual arts. Additionally, the paper will provide a summary of the current state of literature on the subject, as well as potential challenges and limitations of incorporating mindfulness into a creative practice. Finally, the paper will provide recommendations for those interested in implementing mindfulness into their own creative process.\n",
      "\n",
      "Introduction\n",
      "\n",
      "Over the past decade, mindfulness practices such as meditation and breathing exercises have gained widespread attention as ways to reduce stress and improve overall well-being. These practices have also gained attention from the creative community as a tool for enhancing creativity. The purpose of this paper is to explore the impact of mindfulness practices on creativity and how it can be applied in various creative fields.\n",
      "\n",
      "Background\n",
      "\n",
      "Mindfulness practices have been studied extensively in recent years, with many studies showing the positive impact on overall well-being and reduction in stress. Mindfulness practice has been shown to increase attention, reduce stress, and improve cognitive functioning. These benefits not only improve overall well-being but also positively impact creativity.\n",
      "\n",
      "Creativity, on the other hand, is the ability to develop new and original ideas and solutions. Creativity is considered a key element in problem-solving, innovation, and adaptability. It is important in many fields, including science, engineering, design, marketing, and the arts. The creative process is often characterized by a series of steps, including inspiration, ideation, conceptualization, and implementation. Mindfulness practices can impact each of these stages in different ways.\n",
      "\n",
      "Meditation and Breathing Exercises\n",
      "\n",
      "Meditation and breathing exercises are the most commonly practiced mindfulness techniques. Meditation involves focusing one&#39;s attention on a particular object, thought, or activity, and breathing exercises involve controlling the breath to achieve a state of relaxation. Both techniques can help to reduce stress and induce a state of calmness.\n",
      "\n",
      "Research has shown that mindfulness practices can enhance creativity in different ways. Mindfulness practices can improve cognitive flexibility, enhance attention and focus, and promote divergent thinking. These benefits make mindfulness a powerful tool for enhancing creativity.\n",
      "\n",
      "Mindfulness and Cognitive Flexibility\n",
      "\n",
      "Cognitive flexibility is the ability to switch between different mental sets and explore different perspectives. It is an essential ingredient in the creative process. Research has shown that mindfulness practice can increase cognitive flexibility by changing the activation patterns in the brain, particularly the anterior cingulate cortex, which is responsible for flexible decision-making. A study conducted by Colzato et al. (2012) found that mindfulness practice can improve cognitive flexibility in individuals.\n",
      "\n",
      "Mindfulness and Attention and Focus\n",
      "\n",
      "Research has also shown that mindfulness practice can improve attention and focus, which are essential components of the creative process. Mindfulness practice helps individuals to remain focused on a task by reducing the distraction of external stimuli and negative internal distractions. A study conducted by Dvorak and Farb (2008) showed that mindfulness practice can help improve attention and perceptual attentiveness.\n",
      "\n",
      "Mindfulness and Divergent Thinking\n",
      "\n",
      "Divergent thinking is the ability to generate a wide range of ideas and solutions to a problem. Mindfulness practice can promote divergent thinking by reducing the internal judgment that often limits creativity. Mindfulness practice helps individuals to become aware of their thoughts and feelings without judgment, which can help in generating more ideas. A study by Berkovich-Ohana et al. (2014) found that mindfulness practice can promote divergent thinking in individuals.\n",
      "\n",
      "Applications of Mindfulness in Creative Fields\n",
      "\n",
      "The impact of mindfulness on creativity can be seen in a variety of creative fields. In music, musicians have used mindfulness practice to enhance their creative process. Mindfulness practice has been found to improve focus and attention, which are essential for improving the quality of music composition. John Cage, one of the most famous composers, was known to use meditation and breathing exercises to\n",
      "    \n",
      " Answer:\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPrompt:\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0c13e50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'practices, such as meditation and breathing exercises, on enhancing creativity in individuals. The paper will analyze the research conducted by scholars on mindfulness and creativity, and provide examples of how mindfulness has been applied in various creative fields. The paper will also discuss potential limitations or challenges of incorporating mindfulness into a creative practice, and provide recommendations for those interested in implementing mindfulness into their own creative process.\\n\\nIntroduction\\n\\nCreativity is a complex and multifaceted concept that encompasses various aspects of human behavior, including'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "86a1e34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, question, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "be8ca93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "abfbf1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_answer, answer = generate_answer(ft_model, tokenizer, question, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c7a9b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (zero shot FT, no RAG):\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Answer (zero shot FT, no RAG):\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9506019b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    You are a helpful assistant specialized in technology news.\\n    Use ONLY the context below to answer the user question.\\n    If the answer is not in the context, say I don't know.\\n    Answer ONE short sentence. Do NOT repeat context or question\\n    \\n  Question: how can i develop a habit of drawing daily?\\n    \\n  Context: User: Write a 1500-word report in APA format on the impact of mindfulness practices, such as meditation and breathing exercises, on enhancing creativity in individuals. Use at least three scholarly articles from peer-reviewed journals as sources, and include examples of how mindfulness has been applied in various creative fields, such as music, writing, and visual arts. Additionally, discuss potential limitations or challenges of incorporating mindfulness into a creative practice, and provide recommendations for those interested in implementing mindfulness into their own creative process.\\n\\nAssistant: Title: The Impact of Mindfulness Practices on Enhancing Creativity in Individuals\\n\\nAbstract\\n\\nThe purpose of this paper is to explore the impact of mindfulness practices, such as meditation and breathing exercises, on enhancing creativity in individuals. The paper discusses recent research on the topic, including examples of how mindfulness has been applied in various creative fields, such as music, writing, and visual arts. Additionally, the paper will provide a summary of the current state of literature on the subject, as well as potential challenges and limitations of incorporating mindfulness into a creative practice. Finally, the paper will provide recommendations for those interested in implementing mindfulness into their own creative process.\\n\\nIntroduction\\n\\nOver the past decade, mindfulness practices such as meditation and breathing exercises have gained widespread attention as ways to reduce stress and improve overall well-being. These practices have also gained attention from the creative community as a tool for enhancing creativity. The purpose of this paper is to explore the impact of mindfulness practices on creativity and how it can be applied in various creative fields.\\n\\nBackground\\n\\nMindfulness practices have been studied extensively in recent years, with many studies showing the positive impact on overall well-being and reduction in stress. Mindfulness practice has been shown to increase attention, reduce stress, and improve cognitive functioning. These benefits not only improve overall well-being but also positively impact creativity.\\n\\nCreativity, on the other hand, is the ability to develop new and original ideas and solutions. Creativity is considered a key element in problem-solving, innovation, and adaptability. It is important in many fields, including science, engineering, design, marketing, and the arts. The creative process is often characterized by a series of steps, including inspiration, ideation, conceptualization, and implementation. Mindfulness practices can impact each of these stages in different ways.\\n\\nMeditation and Breathing Exercises\\n\\nMeditation and breathing exercises are the most commonly practiced mindfulness techniques. Meditation involves focusing one&#39;s attention on a particular object, thought, or activity, and breathing exercises involve controlling the breath to achieve a state of relaxation. Both techniques can help to reduce stress and induce a state of calmness.\\n\\nResearch has shown that mindfulness practices can enhance creativity in different ways. Mindfulness practices can improve cognitive flexibility, enhance attention and focus, and promote divergent thinking. These benefits make mindfulness a powerful tool for enhancing creativity.\\n\\nMindfulness and Cognitive Flexibility\\n\\nCognitive flexibility is the ability to switch between different mental sets and explore different perspectives. It is an essential ingredient in the creative process. Research has shown that mindfulness practice can increase cognitive flexibility by changing the activation patterns in the brain, particularly the anterior cingulate cortex, which is responsible for flexible decision-making. A study conducted by Colzato et al. (2012) found that mindfulness practice can improve cognitive flexibility in individuals.\\n\\nMindfulness and Attention and Focus\\n\\nResearch has also shown that mindfulness practice can improve attention and focus, which are essential components of the creative process. Mindfulness practice helps individuals to remain focused on a task by reducing the distraction of external stimuli and negative internal distractions. A study conducted by Dvorak and Farb (2008) showed that mindfulness practice can help improve attention and perceptual attentiveness.\\n\\nMindfulness and Divergent Thinking\\n\\nDivergent thinking is the ability to generate a wide range of ideas and solutions to a problem. Mindfulness practice can promote divergent thinking by reducing the internal judgment that often limits creativity. Mindfulness practice helps individuals to become aware of their thoughts and feelings without judgment, which can help in generating more ideas. A study by Berkovich-Ohana et al. (2014) found that mindfulness practice can promote divergent thinking in individuals.\\n\\nApplications of Mindfulness in Creative Fields\\n\\nThe impact of mindfulness on creativity can be seen in a variety of creative fields. In music, musicians have used mindfulness practice to enhance their creative process. Mindfulness practice has been found to improve focus and attention, which are essential for improving the quality of music composition. John Cage, one of the most famous composers, was known to use meditation and breathing exercises to\\n    \\n Answer:\\n    \""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2eaa2dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " practices, such as meditation and breathing exercises, on enhancing creativity in individuals. The paper will review three scholarly articles from peer-reviewed journals to provide evidence for the effectiveness of mindfulness in enhancing creativity. The paper will also discuss potential limitations and challenges of incorporating mindfulness into a creative practice, and provide recommendations for those interested in implementing mindfulness into their own creative process.\n",
      "\n",
      "Introduction\n",
      "\n",
      "Creativity is a vital aspect of human development, and it is often associated with innovation, originality, and the ability to generate\n"
     ]
    }
   ],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, prompt, 256, 128)\n",
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "86c34f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "ONLY answer the question below. Do NOT repeat the question below in the answer.\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e7afd0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " 1. Start small: Begin by drawing a simple sketch or drawing a few lines.\n",
      "2. Make it a habit: Once you have started drawing, make it a habit. Try to draw every day, even if it's just for a few minutes.\n",
      "3. Choose a specific time: Choose a specific time of day, such as before bed, during lunch break, or during your commute.\n",
      "4. Make it a part of your routine: Try to draw every day, even if it's just for a few minutes, as it becomes a habit.\n",
      "5. Visualize the result:\n"
     ]
    }
   ],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, prompt, 256, 128)\n",
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "71104bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"{question}:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b9fc1cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how can i develop a habit of drawing daily?:'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d0ade622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Answer (no RAG, no fine-tuning):\n",
      " I'm not a big fan of drawing. I've never been able to draw a straight line. But I can draw a picture. I can draw a picture of a picture. I can draw a picture of a picture of a picture. I can draw a picture of a picture of a picture of a picture. I can draw a picture of a picture of a picture of a picture of a picture. I can draw a picture of a picture of a picture of a picture of a picture of a picture of a picture of a picture of a picture of a picture of a picture of a picture of a picture of a picture of\n"
     ]
    }
   ],
   "source": [
    "raw_answer, answer = generate_answer(base_model, tokenizer, prompt, 256, 128)\n",
    "print(\"Baseline Answer (no RAG, no fine-tuning):\\n\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "250b6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_answer, answer = generate_answer(ft_model, tokenizer, question, 256, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ff1b7ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "83a15cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RAG (no FT) ---\n",
      "practices, such as meditation and breathing exercises, on enhancing creativity in individuals. The paper will draw on three scholarly articles from peer-reviewed journals to provide a comprehensive overview of the topic. The paper will discuss the potential benefits of mindfulness on creativity, including increased focus, improved problem-solving, and enhanced emotional regulation. The paper will also explore the challenges of incorporating mindfulness into a creative practice, including the need for self-awareness, mindfulness training, and the potential for distraction. The paper will provide\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- RAG (no FT) ---\")\n",
    "prompt, full_output, gen_output, contexts, token_count = rag_answer(\n",
    "    base_model, tokenizer, question, 3, 256, 128, 'faiss'\n",
    ")\n",
    "print(gen_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9d2c1da4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how can i develop a habit of drawing daily'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e6c738c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-tuned + RAG ---\n",
      "practices. Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below Below\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Fine-tuned + RAG ---\")\n",
    "_, raw_answer, answer, _, _ = rag_answer(\n",
    "    ft_model, tokenizer, question, 3, 256, 128, \"faiss\"\n",
    ")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
