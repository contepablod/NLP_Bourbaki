{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0099c896",
   "metadata": {},
   "source": [
    "![Colegio Bourbaki](./Images/Bourbaki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4c1df8",
   "metadata": {},
   "source": [
    "## Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a38423",
   "metadata": {},
   "source": [
    "### Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b346f3dd",
   "metadata": {},
   "source": [
    "El objetivo de este notebook es hacer una demostración de la creación de chatbots estilo ChatGPT con conocimiento de datos específicos.\n",
    "\n",
    "En primer lugar, enseñaremos cómo conectar con el API de Hugging Face para utilizar modelos guardados en ese Hub desde código.\n",
    "\n",
    "Después, veremos cómo podemos introducir material a la base de conocimiento del chatbot, para así obtener respuestas más personalizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611461cf",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5665c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP / LLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "# LangChain\n",
    "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Utils\n",
    "import os\n",
    "from pdfminer.high_level import extract_text\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7850115",
   "metadata": {},
   "source": [
    "Primero, definimos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845da704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo open-weights\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa0ebfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración 4-bit para que corra localmente\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670ab7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1564fe20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10398ca71b541e8b44d2a1c94b5fc96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa435b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Pipeline de generación \"crudo\"\n",
    "generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b068122a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explicame en pocas palabras qué es el fine-tuning con LoRA.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6112754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explicame en pocas palabras qué es el fine-tuning con LoRA.\n",
      "\n",
      "El fine-tuning con LoRA (Low-Rank Adaptation) es una técnica utilizada para ajustar modelos de aprendizaje automático pre-entrenados, especialmente en el contexto de redes neuronales profundas, sin alterar\n"
     ]
    }
   ],
   "source": [
    "outputs = generation_pipeline(prompt)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f969567c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pregunta 1 ---\n",
      "¿Qué es un modelo de lenguaje grande (LLM)? Respondé en menos de 3 líneas.\n",
      "\n",
      "Un modelo de lenguaje grande (LLM) es un sistema de inteligencia artificial avanzado entrenado en vastas cantidades de datos textuales para generar, comprender y producir lenguaje humano con un nivel de sofisticación que imita el proces\n",
      "\n",
      "--- Pregunta 2 ---\n",
      "¿Qué es RAG (Retrieval-Augmented Generation)? Explicalo con un ejemplo.\n",
      "\n",
      "# Answer\n",
      "RAG (Retrieval-Augmented Generation) es una técnica en el campo de la inteligencia artificial y el procesamiento de lenguaje natural que combina la generación de texto con la recuperación de información de una base de datos externa para produ\n",
      "\n",
      "--- Pregunta 3 ---\n",
      "¿En qué se diferencia fine-tuning clásico de LoRA?\n",
      "\n",
      "Fine-tuning clásico y LoRA (Low-Rank Adaptation) son técnicas utilizadas para ajustar modelos pre-entrenados, pero difieren en su enfoque y eficiencia.\n",
      "\n",
      "1. Fine-tuning clás\n"
     ]
    }
   ],
   "source": [
    "preguntas = [\n",
    "    \"¿Qué es un modelo de lenguaje grande (LLM)? Respondé en menos de 3 líneas.\",\n",
    "    \"¿Qué es RAG (Retrieval-Augmented Generation)? Explicalo con un ejemplo.\",\n",
    "    \"¿En qué se diferencia fine-tuning clásico de LoRA?\",\n",
    "]\n",
    "\n",
    "for i, q in enumerate(preguntas, start=1):\n",
    "    print(f\"\\n--- Pregunta {i} ---\")\n",
    "    outputs = generation_pipeline(q)\n",
    "    print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e0705",
   "metadata": {},
   "source": [
    "Podemos lograr mejores respuestas del modelo si modificamos el atributo **system**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d20a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chat_prompt(system_msg, user_msg):\n",
    "    return f\"<|system|>\\n{system_msg}\\n<|user|>\\n{user_msg}\\n<|assistant|>\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c600eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sistema = \"Eres un asistente de poetas, habilidoso en explicar conceptos complejos de programación creativamente.\"\n",
    "usuario = \"Compón un poema que explique el concepto de recursión en programación.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cdf766c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_chat_prompt(sistema, usuario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73eabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "Eres un asistente de poetas, habilidoso en explicar conceptos complejos de programación creativamente.\n",
      "<|user|>\n",
      "Compón un poema que explique el concepto de recursión en programación.\n",
      "<|assistant|>\n",
      " En las profundidades de la programación, donde los algoritmos se encuentran,\n",
      "Existe un concepto, un giro, donde la intuición se vuelve.\n",
      "Recursión, el término, que suena y recuerda,\n",
      "En líneas de código, una historia que escribe.\n",
      "\n",
      "Una función en llamada, a sí misma, se dirige,\n",
      "Mientras se avanza hacia el reino del reto.\n",
      "En la esencia misma, encuentra su propio hilo,\n",
      "Envolviéndose en sí, en un atajo elegante.\n",
      "\n",
      "Un problema, una vez grande, como un monte extenso,\n",
      "Se divide en partes, para su resolución enmendarse.\n",
      "A través de sí misma, se desplaza, sin temor ni desesperación,\n",
      "En cada nivel, una parte, un pequeño paso.\n",
      "\n",
      "Una recursión, como una torre, se construye,\n",
      "Por cada capa que añade, un nodo más.\n",
      "Cada llamada, una capa, un paso más allá,\n",
      "En la cima, el problema\n"
     ]
    }
   ],
   "source": [
    "out = generation_pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc2126",
   "metadata": {},
   "source": [
    "### Creación de un asistente especializado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964ff854",
   "metadata": {},
   "source": [
    "A su vez, podemos aprovechar aún más las capacidades de loss LLM haciendo una especie de fine-tuning. La idea consiste en alimentar al modelos con documentos propios para así lograr respuestas informadas sobre ellos.\n",
    "\n",
    "Esto es posible a través de los encajes y la generación de una base de datos vectorizada.\n",
    "\n",
    "Primero, extraemos texto desde archivos pdfs...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0968838d",
   "metadata": {},
   "source": [
    "... y extramos el texto de un pdf para después guardarlo en .txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d59e373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto extraído y guardado en ./Data/output.txt\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"/media/pdconte/hdd/Pablo/Personal/Colegio_Bourbaki/Natural_Language_Processsing/Semana5/Data/Feynman1982_Article_SimulatingPhysicsWithComputers.pdf\"\n",
    "output_txt_path = \"./Data/output.txt\"\n",
    "\n",
    "# Extraer texto del PDF a un archivo de texto\n",
    "text = extract_text(pdf_path)\n",
    "\n",
    "os.makedirs(os.path.dirname(output_txt_path), exist_ok=True)\n",
    "with open(output_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text)\n",
    "\n",
    "print(\"Texto extraído y guardado en\", output_txt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5d077fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TextLoader(\"./Data/output.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100, separator=\"\\n\"\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77bd8fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': './Data/output.txt'}, page_content=\"International Journal of Theoretical Physics, VoL 21, Nos. 6/7,  1982 \\nSimulating Physics with Computers \\nRichard P. Feynman \\nDepartment of Physics, California Institute of Technology, Pasadena, California 91107 \\nReceived May 7, 1981 \\n1.  I N T R O D U C T I O N  \\nOn  the  program  it  says  this  is  a  keynote  speech--and  I  don't  know \\nwhat  a  keynote speech is.  I  do not intend  in  any way to suggest what  should \\nbe  in  this  meeting  as  a  keynote of  the  subjects  or  anything  like  that.  I  have \\nmy  own  things  to  say  and  to  talk  about  and  there's  no  implication  that \\nanybody  needs  to  talk  about  the  same  thing  or  anything  like  it.  So  what  I \\nwant  to  talk  about  is  what  Mike  Dertouzos  suggested  that  nobody  would \\ntalk  about.  I  want  to  talk  about  the  problem  of  simulating  physics  with \\ncomputers  and  I  mean  that  in  a  specific way  which  I  am  going  to  explain.\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b4db61a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ccb63",
   "metadata": {},
   "source": [
    "En seguida, cargamos el documento a la base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f49bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings HuggingFace (sin costo de API)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Crear / persistir base vectorial con Chroma\n",
    "db = Chroma.from_documents(texts, embeddings, collection_name=\"docs_rag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38152e8",
   "metadata": {},
   "source": [
    "Este modelo ahora se puede utilizar como un ChatGPT con conocimiento especializado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f603690",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dd19bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generation_pipeline_db = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.2,\n",
    "    return_full_text=False,\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=generation_pipeline_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24c75d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=local_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "# Ejemplo de consulta\n",
    "query = \"¿De qué trata el documento? Resume los puntos principales.\"\n",
    "result = qa.invoke({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ad3157e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEl documento trata sobre una conferencia titulada \"Simulando Física con Computadores\" presentada por Richard P. Feynman. Feynman explica que su discurso no tiene la intención de establecer un tema o keynote para el evento, sino que pretende abordar el tema sugerido por Mike Dertouzos, que no está especificado en el texto proporcionado. Los puntos principales incluyen la clarificación de que el discurso de Feynman no es un keynote, y su intención es disc'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "respuesta = result[\"result\"]\n",
    "respuesta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d94a376e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Photons are polarized such that they either go into the ordinary ray or the extraordinary one, but not both. The probability of a photon being in either ray is always 100%, meaning a photon will always be found in one or the other, never both.\\n\\nContext: places  out,  where  the  photon  can  go.  (See  Figure  2.) \\nIf you put  a  polarized photon  in,  then  it will  go  to  one beam called the \\nordinary  ray, or  another,  the  extraordinary'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"How photons are polarized?\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c53e38fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nNegative probabilities are not a concept that exists in the real world or in standard probability theory. Probabilities, by definition, are non-negative values that represent the likelihood of an event occurring, ranging from 0 (the event will not occur) to 1 (the event will certainly occur). \\n\\nIn the context of quantum mechanics, the \"negative probabilities\" you\\'re referring to might be a misunderstanding or misrepresentation of the complex probability amplitudes. In quantum mechanics, probabilities are derived from the square of the magnitude of a complex number (probability'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Expand the concept of negative probabilities\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d92db0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A quantum computer is a type of computing device that uses the principles of quantum mechanics to process information. Unlike classical computers, which use bits as the basic unit of information, quantum computers use quantum bits, or qubits, which can exist in multiple states simultaneously due to the phenomenon of superposition. This allows quantum computers to potentially solve complex problems much faster than classical computers.\\n\\n\\nThe context provided discusses the author\\'s struggle to understand quantum mechanics and the concept of a \"world view\" associated with it. The author expresses difficulty in grasping the principles of quantum mechanics, and even though they suspect'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is a quantum computer?\"\n",
    "result = qa.invoke({\"query\": query})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd51153",
   "metadata": {},
   "source": [
    "Podemos comparar la salida con el modelo original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abd08d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a quantum computer? A quantum computer is a type of computing device that uses the principles of quantum mechanics to perform calculations. It operates on quantum bits, or qubits, which can exist in multiple states simultaneously due to the concept of superposition. This allows quantum computers to process a vast number of possibilities simultaneously, potentially solving complex problems much faster than classical computers.\n",
      "\n",
      "In contrast to classical bits, which can either be a 0 or a 1, qubits can represent a 0, a 1, or any quantum superposition of these states. This ability, combined with another quantum property called entanglement, where the state of\n"
     ]
    }
   ],
   "source": [
    "out = generation_pipeline(\n",
    "    query,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "print(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff77d55",
   "metadata": {},
   "source": [
    "### Preguntas\n",
    "\n",
    "**Uso de LLM:**\n",
    "\n",
    "4) ¿Qué ventajas ofrece usar un LLM para la creación de chatbots comparado con versiones anteriores?\n",
    "5) ¿Cómo se formulan las peticiones al modelo para generar respuestas coherentes y relevantes?\n",
    "6) ¿Qué limitaciones tienes estos modelos y cómo puedes mitigarlas?\n",
    "\n",
    "**Introducción de Material a la Base de Conocimiento:**\n",
    "\n",
    "7) ¿Cómo puedes personalizar las respuestas del modelo utilizando información específica?\n",
    "8) ¿Cuál es la importancia de la relevancia y precisión del material que se introduce en la base de conocimientos del bot?\n",
    "9) ¿Qué estrategias se pueden utilizar para mantener actualizada la base de conocimientos del chatbot?\n",
    "\n",
    "**Personalización y Respuestas del Chatbot:**\n",
    "\n",
    "10) ¿De qué manera se puede ajustar el tono o el estilo de las respuestas que genera el modelo?\n",
    "11) ¿Cómo afecta el contexto proporcionado a las respuestas generadas por el chatbot?\n",
    "    Describe un método para evaluar la precisión y utilidad de las respuestas del chatbot.\n",
    "\n",
    "**Problemas Éticos y de Privacidad:**\n",
    "\n",
    "12) ¿Cuáles son las consideraciones éticas al utilizar modelos de lenguaje generativos en un chatbot?\n",
    "13) ¿Cómo debería manejar un chatbot las solicitudes de datos personales o sensibles de los usuarios?\n",
    "14) ¿Qué medidas se pueden tomar para garantizar la privacidad y la seguridad de los usuarios al interactuar con un chatbot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4b8154",
   "metadata": {},
   "source": [
    "![Lenguaje Matemático](./Images/Matematicas.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
