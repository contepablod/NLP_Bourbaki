{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a236957",
   "metadata": {},
   "source": [
    "## Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e9c17",
   "metadata": {},
   "source": [
    "![Colegio Bourbaki](./Images/Bourbaki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee09bfcc",
   "metadata": {},
   "source": [
    "## Fast Text: Representación de palabras mediante subpalabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe273e",
   "metadata": {},
   "source": [
    "Link de Interés: https://fasttext.cc/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf12e6d",
   "metadata": {},
   "source": [
    "FastText es un método de embedding de palabras desarrollado por **Facebook AI Research (FAIR)**. Es una extensión del modelo Word2Vec, pero introduce una mejora clave: en lugar de aprender vectores solo para palabras completas, representa cada palabra como un conjunto de n-gramas de caracteres.\n",
    "\n",
    "- Representación mediante n-gramas\n",
    "\n",
    "Por ejemplo, si tomamos la palabra “artificial” y definimos \n",
    "n=3\n",
    "n=3:\n",
    "\n",
    "Los n-gramas de caracteres serían:\n",
    "\n",
    "<ar, art, rti, tif, ifi, fic, ici, cia, ial, al>\n",
    "\n",
    "(A menudo se añaden los delimitadores de inicio y fin, como < y >, para preservar el contexto).\n",
    "\n",
    "Así, la representación vectorial de la palabra se obtiene como la suma o promedio de los vectores de sus n-gramas.\n",
    "\n",
    "- Ventajas sobre Word2Vec\n",
    "\n",
    "Generalización a palabras desconocidas (OOV):\n",
    "Si una palabra no se vio durante el entrenamiento, FastText puede descomponerla en sus subpalabras (n-gramas) y construir un vector aproximado, lo que Word2Vec no puede hacer.\n",
    "\n",
    "Comprensión morfológica:\n",
    "Al basarse en subpalabras, captura información sobre prefijos, sufijos y raíces, lo que mejora la representación de palabras con similitudes morfológicas (por ejemplo, “correr”, “corriendo”, “corredor”).\n",
    "\n",
    "- Modelo de entrenamiento\n",
    "\n",
    "El modelo subyacente es similar a Skip-gram o CBOW (como en Word2Vec), pero cada palabra se trata como la suma de sus n-gramas de caracteres.\n",
    "Aunque internamente no modela explícitamente la estructura lingüística de la oración, utiliza una ventana deslizante de contexto sobre las palabras, como un modelo de bolsa de palabras (bag of words)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ff4c9",
   "metadata": {},
   "source": [
    "![fastText](./Images/cbow-sg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c080b6a6",
   "metadata": {},
   "source": [
    "**Aplicaciones prácticas**\n",
    "\n",
    "FastText se puede aplicar a numerosos problemas relacionados con el lenguaje natural:\n",
    "\n",
    "- Corrección ortográfica y autocompletado.\n",
    "\n",
    "- Sistemas de recomendación y búsqueda: mejores sugerencias de productos o consultas.\n",
    "\n",
    "- Chatbots y atención al cliente: comprensión de entradas con errores tipográficos.\n",
    "\n",
    "- Análisis de sentimientos y reseñas.\n",
    "\n",
    "Se pueden usar conjuntos de datos como:\n",
    "\n",
    "- Consultas de búsqueda de usuarios.\n",
    "\n",
    "- Conversaciones de chatbots.\n",
    "\n",
    "- Reseñas y valoraciones.\n",
    "\n",
    "Estos modelos pueden mejorar la experiencia del cliente al ofrecer sugerencias más precisas, autocorrección, o resultados más relevantes.\n",
    "\n",
    "\n",
    "FastText es un modelo de embedding basado en subpalabras (n-gramas) que mejora a Word2Vec al poder representar palabras raras o no vistas y captar mejor la morfología del idioma.\n",
    "Gracias a esto, es especialmente útil en aplicaciones prácticas donde la entrada del usuario puede ser ruidosa o diversa.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f9520",
   "metadata": {},
   "source": [
    "### Corrector ortográfico con FastText\n",
    "\n",
    "En este notebook construimos un **corrector ortográfico inteligente** combinando técnicas clásicas de NLP con **embeddings FastText**.\n",
    "\n",
    "### ¿Qué hace el código?\n",
    "1. **Tokenización del texto**  \n",
    "   Dividimos las oraciones en palabras, espacios y signos de puntuación.  \n",
    "   Solo intentamos corregir las palabras alfabéticas, manteniendo el resto intacto.\n",
    "\n",
    "2. **Generación de candidatos (edits)**  \n",
    "   Para cada palabra mal escrita, generamos posibles variantes aplicando operaciones de edición:\n",
    "   - Eliminación de una letra  \n",
    "   - Inserción de una letra  \n",
    "   - Sustitución de una letra  \n",
    "   - Transposición de dos letras seguidas  \n",
    "\n",
    "   Además, consideramos los errores más comunes de teclado (QWERTY), donde confundir teclas adyacentes tiene menor costo.\n",
    "\n",
    "3. **Distancia de edición ponderada**  \n",
    "   Calculamos qué tan diferente es cada candidato de la palabra original, asignando costos menores a errores típicos de tipeo.\n",
    "\n",
    "4. **Embeddings de FastText**  \n",
    "   Usamos vectores de palabras (FastText) para comparar la similitud semántica entre la palabra mal escrita y cada candidato.  \n",
    "   Esto permite corregir incluso palabras fuera del vocabulario (OOV) gracias a los sub-tokens de FastText.\n",
    "\n",
    "5. **Ranking de candidatos**  \n",
    "   Combinamos:\n",
    "   - Similitud de embeddings  \n",
    "   - Frecuencia de la palabra en el corpus  \n",
    "   - Distancia de edición  \n",
    "\n",
    "   El mejor candidato según esta puntuación se elige como corrección.\n",
    "\n",
    "6. **Reconstrucción del texto**  \n",
    "   Reemplazamos solo las palabras corregidas y dejamos intactos los espacios y la puntuación, devolviendo la oración completa.\n",
    "\n",
    "---\n",
    "\n",
    "En resumen: el sistema propone correcciones ortográficas basadas no solo en la cercanía de edición, sino también en la **probabilidad de uso real** (frecuencia) y la **cercanía semántica**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e934ea4",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c204d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from collections.abc import Iterable\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "# Wordfreq for word frequencies\n",
    "from wordfreq import zipf_frequency \n",
    "\n",
    "# Gensim for FastText\n",
    "import gensim.downloader as api\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb019b6",
   "metadata": {},
   "source": [
    "### Funciones de ayuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d80521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex for tokenization\n",
    "TOKEN_RE = re.compile(r\"([A-Za-z]+|[^A-Za-z\\s]+|\\s+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3db368",
   "metadata": {},
   "source": [
    "Usamos esa regex para separar el texto en tokens manteniendo todo, de manera que al corregir podamos reconstruir la oración sin perder espacios o símbolos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1063ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Split into tokens: words, whitespace, punctuation blocks. Keep everything to reconstruct.\"\"\"\n",
    "    return TOKEN_RE.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3794d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word(tok: str) -> bool:\n",
    "    '''Returns true if token is a word (only letters).'''\n",
    "    return tok.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a02e1",
   "metadata": {},
   "source": [
    "Importamos todas las letras minúsculas del alfabeto inglés desde la librería estándar string.\n",
    "Esto se usa para probar reemplazos e inserciones de letras en las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab16ab6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALPHABET = string.ascii_lowercase\n",
    "ALPHABET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55e964",
   "metadata": {},
   "source": [
    "Construimos un diccionario de adyacencia de teclado (layout QWERTY en inglés).\n",
    "\n",
    "Ejemplo: la letra s tiene como adyacentes a, d, w, x.\n",
    "\n",
    "Esto sirve para que errores como “s → a” o “s → d” cuesten menos en la distancia de edición (porque son errores de tipeo comunes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23369fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple keyboard adjacency map (US QWERTY). Extend as needed.\n",
    "KEY_ADJ: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "_adj_rows = [\n",
    "    \"qwertyuiop\",\n",
    "    \"asdfghjkl\",\n",
    "    \"zxcvbnm\",\n",
    "]\n",
    "for row in _adj_rows:\n",
    "    for i, ch in enumerate(row):\n",
    "        if i > 0:\n",
    "            KEY_ADJ[ch].add(row[i-1])\n",
    "        if i < len(row)-1:\n",
    "            KEY_ADJ[ch].add(row[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ddd0c",
   "metadata": {},
   "source": [
    "Generamos todas las palabras que están a una edición de distancia de la original:\n",
    "\n",
    "**deletes** → eliminar una letra.\n",
    "\n",
    "**transposes** → intercambiar dos letras consecutivas.\n",
    "\n",
    "**replaces** → reemplazar una letra por otra del alfabeto.\n",
    "\n",
    "**inserts** → insertar una letra extra en cualquier posición.\n",
    "\n",
    "Estas son las candidatas para corrección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c903c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits(word: str) -> set[str]:\n",
    "    word = word.lower()\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in ALPHABET]\n",
    "    inserts  = [L + c + R for L, R in splits for c in ALPHABET]\n",
    "    return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa29247",
   "metadata": {},
   "source": [
    "Implementamos una variante del algoritmo de Damerau-Levenshtein para calcular la distancia entre dos palabras:\n",
    "\n",
    "Inserción = costo 1\n",
    "\n",
    "Eliminación = costo 1\n",
    "\n",
    "Sustitución = costo 1 (pero si la sustitución es entre teclas adyacentes → costo 0.6)\n",
    "\n",
    "Transposición = costo 0.8\n",
    "\n",
    "Así modelamos errores más realistas de escritura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30a8b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_edit_distance(src: str, dst: str) -> float:\n",
    "    \"\"\"Damerau-Levenshtein style with lower cost for adjacent-key substitutions.\"\"\"\n",
    "    src, dst = src.lower(), dst.lower()\n",
    "    n, m = len(src), len(dst)\n",
    "    dp = np.zeros((n+1, m+1), dtype=float)\n",
    "    for i in range(n+1):\n",
    "        dp[i,0] = i\n",
    "    for j in range(m+1):\n",
    "        dp[0,j] = j\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            cost_sub = 0 if src[i-1]==dst[j-1] else (0.6 if dst[j-1] in KEY_ADJ[src[i-1]] else 1.0)\n",
    "            dp[i,j] = min(\n",
    "                dp[i-1,j] + 1,         # deletion\n",
    "                dp[i, j-1] + 1,        # insertion\n",
    "                dp[i-1,j-1] + cost_sub # substitution\n",
    "            )\n",
    "            if i>1 and j>1 and src[i-1]==dst[j-2] and src[i-2]==dst[j-1]:\n",
    "                dp[i,j] = min(dp[i,j], dp[i-2,j-2] + 0.8)  # transposition\n",
    "    return float(dp[n,m])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b00073",
   "metadata": {},
   "source": [
    "Vamos a definir un conjunto de parámetros ajustables para balancear los criterios de correción.\n",
    "\n",
    "Por ejemplo, cuando evaluamos un candidato, el sistema calcula algo por el estilo:\n",
    "\n",
    "score = sim_w * similitud + freq_w * log_frecuencia - dist_w * distancia\n",
    "\n",
    "El candidato con mayor score es el que elegimos como corrección.\n",
    "\n",
    "* sim_w → controla cuánto valoramos la similitud semántica entre la palabra original y el candidato, medida con FastText (cosine similarity).\n",
    "\n",
    "* freq_w → controla cuánto valoramos que el candidato sea una palabra frecuente en el vocabulario.\n",
    "\n",
    "* dist_w → controla cuánto penalizamos candidatos que están muy lejos de la palabra original en términos de operaciones de edición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cd31cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RankWeights:\n",
    "    sim_w: float = 2.0  # peso para la similitud de embeddings (coseno)\n",
    "    freq_w: float = 1.2  # peso para la frecuencia de la palabra en el corpus\n",
    "    dist_w: float = 1.0  # penalización por distancia de edición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa91d5",
   "metadata": {},
   "source": [
    "Vamos a definir la clase principal del corrector ortográfico utilizando FastText. \n",
    "Esta clase incluirá métodos para entrenar el modelo, generar candidatos de corrección y seleccionar la mejor corrección basada en una puntuación compuesta, es decir,\n",
    "la tokenización, la verificación de palabras conocidas, la generación de ediciones, el cálculo de similitud de embeddings y la distancia de edición ponderada, para entrenar, cargar, generar candidatos y corregir texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0eb9e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellChecker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,  \n",
    "        vocab: Counter | None = None,  \n",
    "        weights: RankWeights | None = None, \n",
    "    ):\n",
    "        self.model = model  # modelo FastText (entrenado o cargado)\n",
    "        self.vocab = vocab or Counter()  # frecuencias de palabras\n",
    "        self.weights = weights or RankWeights()  # pesos de ranking\n",
    "        self._wordset: set[str] = (\n",
    "            set()\n",
    "        )  # conjunto rápido para saber si una palabra existe\n",
    "        try:\n",
    "            zipf_frequency,\n",
    "            # escala ~0..7 (más alto = más frecuente)\n",
    "            self.zipf_frequency = zipf_frequency\n",
    "        except Exception:\n",
    "            self.zipf_frequency = None\n",
    "\n",
    "    # Model & vocab\n",
    "    def train_model(\n",
    "        self,\n",
    "        corpus: Iterable[str],\n",
    "        size: int = 100,\n",
    "        window: int = 5,\n",
    "        min_count: int = 1,\n",
    "        epochs: int = 10,\n",
    "    ) -> None:\n",
    "        sentences = [re.findall(r\"[A-Za-z]+\", s.lower()) for s in corpus]\n",
    "        self.model = FastText(vector_size=size, window=window, min_count=min_count)\n",
    "        self.model.build_vocab(sentences)\n",
    "        self.model.train(sentences, total_examples=len(sentences), epochs=epochs)\n",
    "        self.vocab = Counter(w for sen in sentences for w in sen)\n",
    "        self._wordset = set(self.vocab)\n",
    "\n",
    "    def load_pretrained(self, path: str) -> None:\n",
    "        if path.endswith(\".bin\"):\n",
    "            try:\n",
    "                self.model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "            except Exception:\n",
    "                self.model = load_facebook_vectors(path)\n",
    "        else:\n",
    "            self.model = KeyedVectors.load_word2vec_format(path, binary=False)\n",
    "        self._wordset = set(self.get_vocab_words())\n",
    "\n",
    "    def set_vocab_from_corpus(self, corpus: Iterable[str]) -> None:\n",
    "        words = [w.lower() for s in corpus for w in re.findall(r\"[A-Za-z]+\", s)]\n",
    "        self.vocab = Counter(words)\n",
    "        self._wordset = set(self.vocab)\n",
    "\n",
    "    def get_vocab_words(self) -> Iterable[str]:\n",
    "        m = self.model\n",
    "        if hasattr(m, \"key_to_index\"):\n",
    "            return list(m.key_to_index.keys())\n",
    "        if hasattr(m, \"wv\") and hasattr(m.wv, \"key_to_index\"):\n",
    "            return list(m.wv.key_to_index.keys())\n",
    "        return list(self.vocab.keys())\n",
    "\n",
    "    # Scoring\n",
    "    def _freq_prior(self, w):\n",
    "        # Prior por frecuencia de palabra:\n",
    "        # - Si wordfreq: usa Zipf (0..7).\n",
    "        # - Si no, usa log-frecuencia del vocab manual.\n",
    "        if self.zipf_frequency is not None:\n",
    "            return self.zipf_frequency(w.lower(), \"en\")  # cambiar \"en\" por otro idioma\n",
    "        return math.log(self.vocab.get(w.lower(), 0) + 1.0)\n",
    "\n",
    "    def _cosine(self, a, b):\n",
    "        # Similitud coseno entre embeddings de palabras\n",
    "        m = self.model\n",
    "        if m is None:\n",
    "            return 0.0\n",
    "        try:\n",
    "            if hasattr(m, \"wv\"):\n",
    "                va, vb = m.wv[a], m.wv[b]\n",
    "            else:\n",
    "                va, vb = m[a], m[b]\n",
    "            denom = (np.linalg.norm(va) * np.linalg.norm(vb))\n",
    "            if denom == 0:\n",
    "                return 0.0\n",
    "            return float(np.dot(va, vb) / denom)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def score(self, src, cand, prev=None, nxt=None):\n",
    "        # Componentes\n",
    "        d   = weighted_edit_distance(src, cand)\n",
    "        sim = self._cosine(src.lower(), cand.lower())\n",
    "        fr  = self._freq_prior(cand)\n",
    "\n",
    "        # Contexto (promedio de similitud con vecinos alfabéticos si existen)\n",
    "        ctx = 0.0\n",
    "        cnt = 0\n",
    "        if prev and prev.isalpha():\n",
    "            ctx += self._cosine(cand.lower(), prev.lower()); cnt += 1\n",
    "        if nxt and nxt.isalpha():\n",
    "            ctx += self._cosine(cand.lower(), nxt.lower());  cnt += 1\n",
    "        if cnt:\n",
    "            ctx /= cnt\n",
    "\n",
    "        # Pequeño boost si el candidato ya es igual (evita cambios innecesarios)\n",
    "        ident_bonus = 0.15 if src.lower() == cand.lower() else 0.0\n",
    "\n",
    "        # Cálculo final\n",
    "        return ident_bonus + (2.5 * sim) + (1.4 * fr) + (1.0 * ctx) - (1.2 * d)\n",
    "\n",
    "    # Candidates\n",
    "    def known(self, words: Iterable[str]) -> set[str]:\n",
    "        if self._wordset:\n",
    "            return {w for w in words if w in self._wordset}\n",
    "        return {w for w in words if self.vocab.get(w, 0) > 0}\n",
    "\n",
    "    def candidates(self, word, max_edits=2, k_from_embeddings=200):\n",
    "        wl = word.lower()\n",
    "        cands = set()\n",
    "\n",
    "        # a) vecinos por edición\n",
    "        cands |= self.known({wl})\n",
    "        e1 = self.known(edits(wl))\n",
    "        cands |= e1\n",
    "        if max_edits >= 2 and e1:\n",
    "            for w1 in list(e1)[:300]:  # limitar expansión para no explotar el espacio\n",
    "                cands |= self.known(edits(w1))\n",
    "\n",
    "        # b) vecinos por embeddings (si hay modelo)\n",
    "        try:\n",
    "            m = self.model.wv if hasattr(self.model, \"wv\") else self.model\n",
    "            if hasattr(m, \"most_similar\"):\n",
    "                for w, _ in m.most_similar(wl, topn=k_from_embeddings):\n",
    "                    cands.add(w)\n",
    "        except Exception:\n",
    "            pass\n",
    "        if not cands:\n",
    "            cands.add(wl)\n",
    "\n",
    "        # c) filtro suave por frecuencia (descarta palabras muy raras)\n",
    "        #    umbral ajustable: si existe wordfreq, pedimos Zipf>=2.5; si no, al menos que aparezca en vocab\n",
    "        filtered = set()\n",
    "        for w in cands:\n",
    "            if self.zipf_frequency is not None:\n",
    "                if self.zipf_frequency(w, \"en\") >= 2.5:\n",
    "                    filtered.add(w)\n",
    "            else:\n",
    "                if self.vocab.get(w, 0) > 0:\n",
    "                    filtered.add(w)\n",
    "        if filtered:\n",
    "            cands = filtered\n",
    "        return cands\n",
    "\n",
    "    # Correction\n",
    "    def correct_word(self, word, prev=None, nxt=None, max_edits=2):\n",
    "        if len(word) <= 2:\n",
    "            return word\n",
    "        cands = self.candidates(word, max_edits=max_edits)\n",
    "        best = max(cands, key=lambda c: self.score(word, c, prev=prev, nxt=nxt))\n",
    "        if word.istitle():\n",
    "            return best.title()\n",
    "        if word.isupper():\n",
    "            return best.upper()\n",
    "        return best\n",
    "\n",
    "    def correct(self, text, max_edits=2):\n",
    "        toks = tokenize(text)\n",
    "        out = []\n",
    "        # Para cada token palabra, miramos palabra previa/siguiente (alfabéticas) como contexto\n",
    "        for i, tok in enumerate(toks):\n",
    "            if tok.isalpha():\n",
    "                prev = None\n",
    "                nxt  = None\n",
    "                # buscar prev\n",
    "                j = i - 1\n",
    "                while j >= 0:\n",
    "                    if toks[j].isalpha(): prev = toks[j]; break\n",
    "                    j -= 1\n",
    "                # buscar next\n",
    "                j = i + 1\n",
    "                while j < len(toks):\n",
    "                    if toks[j].isalpha(): nxt = toks[j]; break\n",
    "                    j += 1\n",
    "                out.append(self.correct_word(tok, prev=prev, nxt=nxt, max_edits=max_edits))\n",
    "            else:\n",
    "                out.append(tok)\n",
    "        return \"\".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506de45",
   "metadata": {},
   "source": [
    "* Constructor __init__\n",
    "\n",
    "Se inicializa el corrector con un modelo de embeddings, un vocabulario y los parámetros de peso.\n",
    "\n",
    "\n",
    "* Sección “Model & vocab”\n",
    "\n",
    "train_model → entrena un modelo FastText desde cero sobre un corpus dado.\n",
    "\n",
    "load_pretrained → carga un modelo ya entrenado desde archivo (.bin o texto).\n",
    "\n",
    "set_vocab_from_corpus → construye un contador de frecuencias a partir de un corpus.\n",
    "\n",
    "get_vocab_words → obtiene la lista de palabras conocidas según el modelo o vocabulario.\n",
    "\n",
    "Esto define la base de conocimiento del corrector.\n",
    "\n",
    "\n",
    "* Sección “Scoring”\n",
    "\n",
    "_cosine → calcula la similitud de coseno entre embeddings de dos palabras.\n",
    "\n",
    "_log_freq → devuelve el logaritmo de la frecuencia de una palabra (más frecuente = más probable).\n",
    "\n",
    "score → combina: bonus de identidad (si ya coincide la palabra), similitud de embeddings, frecuencia, penalización por distancia de edición.\n",
    "\n",
    "Da un puntaje total para ordenar candidatos.\n",
    "\n",
    "* Sección “Candidates”\n",
    "\n",
    "known → filtra una lista de palabras dejando solo las que están en el vocabulario/modelo.\n",
    "\n",
    "candidates → genera posibles correcciones:\n",
    "\n",
    "Variantes por ediciones (borrar, insertar, sustituir, transponer).\n",
    "\n",
    "Vecinos semánticos según embeddings (most_similar).\n",
    "\n",
    "Si no encuentra nada, devuelve la palabra original.\n",
    "\n",
    "Aquí obtenemos qué opciones existen para corregir una palabra.\n",
    "\n",
    "* Sección “Correction”\n",
    "\n",
    "correct_word → evalúa todos los candidatos de una palabra y se queda con el de mayor score. Respeta mayúsculas/minúsculas.\n",
    "\n",
    "correct → tokeniza el texto completo, corrige solo las palabras alfabéticas, y reconstruye el texto final.\n",
    "\n",
    "Esta es la función que usamos directamente para corregir frases enteras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176217f1",
   "metadata": {},
   "source": [
    "Vamos a implementar una función que nos permita construir un corrector. Tenemos 3 modos de uso:\n",
    "\n",
    "* Modo pretrained (pretrained no es None)\n",
    "\n",
    "Carga un modelo FastText ya entrenado (ej. cc.en.300.bin).\n",
    "Si el vocabulario está vacío, se inicializa con una lista de hasta 200k palabras del modelo.\n",
    "\n",
    "* Modo demo (demo=True)\n",
    "\n",
    "Entrena un mini-modelo FastText sobre el TOY_CORPUS.\n",
    "Esto permite probar el pipeline sin descargar nada.\n",
    "\n",
    "* Modo fallback (ni pretrained ni demo)\n",
    "\n",
    "Usa solo el vocabulario del TOY_CORPUS, sin embeddings.\n",
    "Es el modo más básico, útil si no hay FastText disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86d4db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOY_CORPUS = [\n",
    "    \"This is a simple sentence.\",\n",
    "    \"Another simple example sentence.\",\n",
    "    \"We write code to correct spelling errors.\",\n",
    "    \"Fasttext uses subword embeddings.\",\n",
    "    \"Spell checking benefits from language models\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36ecaa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_checker(pretrained=None, demo: bool = True) -> SpellChecker:\n",
    "    \"\"\"\n",
    "    Construye un SpellChecker:\n",
    "      - Si pretrained es un str -> se asume ruta a archivo .bin/.txt\n",
    "      - Si pretrained es un objeto gensim (KeyedVectors/FastText) -> se usa directamente\n",
    "      - Si demo=True -> entrena mini-modelo en TOY_CORPUS\n",
    "      - Sino -> usa solo vocabulario de TOY_CORPUS (sin embeddings)\n",
    "    \"\"\"\n",
    "    checker = SpellChecker(weights=RankWeights(sim_w=2.0, freq_w=1.2, dist_w=1.0))\n",
    "\n",
    "    if pretrained is not None:\n",
    "        # Caso 1: ruta a archivo\n",
    "        if isinstance(pretrained, str):\n",
    "            checker.load_pretrained(pretrained)\n",
    "        else:\n",
    "            # Caso 2: objeto de modelo gensim ya cargado (ej. api.load)\n",
    "            checker.model = pretrained\n",
    "\n",
    "        # inicializar vocabulario si está vacío\n",
    "        if not checker.vocab:\n",
    "            words = list(checker.get_vocab_words())[:200000]\n",
    "            checker.vocab = Counter({w: 1 for w in words})\n",
    "            checker._wordset = set(words)\n",
    "\n",
    "    elif demo:\n",
    "        checker.train_model(TOY_CORPUS, size=100, window=5, min_count=1, epochs=10)\n",
    "\n",
    "    else:\n",
    "        checker.vocab = Counter(w for s in TOY_CORPUS for w in s.split())\n",
    "        checker._wordset = set(checker.vocab)\n",
    "\n",
    "    return checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e71ff",
   "metadata": {},
   "source": [
    "Veamos cómo usarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b829f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Ths is a smple sentnce with speling erors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a2da90e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : Ths is a smple sentnce with speling erors.\n",
      "OUTPUT: This is a simple sentence to spelling to.\n"
     ]
    }
   ],
   "source": [
    "# Build the checker with a tiny demo FastText model (no external downloads)\n",
    "checker = build_checker(pretrained=None, demo=True)\n",
    "print(\"INPUT :\", sample)\n",
    "print(\"OUTPUT:\", checker.correct(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6eb52a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bebc7c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : Ths is a smple sentnce with speling erors.\n",
      "OUTPUT: The is a simple sentence with spelling errors.\n"
     ]
    }
   ],
   "source": [
    "checker = build_checker(pretrained=model, demo=False)\n",
    "sample = \"Ths is a smple sentnce with speling erors.\"\n",
    "print(\"INPUT :\", sample)\n",
    "print(\"OUTPUT:\", checker.correct(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "490c07d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : Ths is a smple sentnce with speling erors.\n",
      "OUTPUT: Ths is a simple sentnce with spelling erors.\n"
     ]
    }
   ],
   "source": [
    "checker = build_checker(pretrained=None, demo=False)\n",
    "sample = \"Ths is a smple sentnce with speling erors.\"\n",
    "print(\"INPUT :\", sample)\n",
    "print(\"OUTPUT:\", checker.correct(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a366f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/pdconte/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27699c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The is a simple sentence with spelling errors.\n"
     ]
    }
   ],
   "source": [
    "corpus = [\" \".join(sent) for sent in brown.sents()]\n",
    "checker = build_checker(demo=False)\n",
    "checker.train_model(corpus)\n",
    "print(checker.correct(\"Ths is a smple sentnce with speling erors.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e911a9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : Ths is a smple sentnce with speling erors.\n",
      "OUTPUT: The is a simple sentence with spelling errors.\n",
      "------------------------------------------------------------\n",
      "INPUT : Shee liks to read boks in the librery.\n",
      "OUTPUT: She like to read books in the library.\n",
      "------------------------------------------------------------\n",
      "INPUT : We are lernig how to corect mispelled wrds.\n",
      "OUTPUT: We are lerner how to correct spelled words.\n",
      "------------------------------------------------------------\n",
      "INPUT : The quik borwn fox jmps ovr the lazi dog.\n",
      "OUTPUT: The quick born for jumps over the lazy dog.\n",
      "------------------------------------------------------------\n",
      "INPUT : Natrual langauge procesing is intersting.\n",
      "OUTPUT: Natural language processing is interesting.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"Ths is a smple sentnce with speling erors.\",\n",
    "    \"Shee liks to read boks in the librery.\",\n",
    "    \"We are lernig how to corect mispelled wrds.\",\n",
    "    \"The quik borwn fox jmps ovr the lazi dog.\",\n",
    "    \"Natrual langauge procesing is intersting.\",\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(\"INPUT :\", s)\n",
    "    print(\"OUTPUT:\", checker.correct(s))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b604a510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spelling'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker.correct_word(\"speling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7eedc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simple'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker.correct_word(\"smple\", prev=\"a\", nxt=\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc809cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidatos para 'speling': ['ailing', 'baffling', 'boiling', 'bombing', 'briefing', 'bubbling', 'bursting', 'calming', 'chafing', 'charging', 'charming', 'chatting', 'chugging', 'circling', 'clawing', 'cling', 'copying', 'creaking', 'cringing', 'crippling', 'crumbling', 'curbing', 'curling', 'curving', 'dabbling', 'dangling', 'debunking', 'deeming', 'despairing', 'dialing', 'digging', 'dreaming', 'dressing', 'drugging', 'dueling', 'dwindling', 'embezzling', 'embodying', 'emitting', 'fairing', 'fawning', 'fencing', 'filing', 'fleming', 'fooling', 'fumbling', 'grappling', 'grieving', 'growling', 'humming', 'hurtling', 'hustling', 'idling', 'inkling', 'juggling', 'kipling', 'labeling', 'lingering', 'linking', 'meddling', 'mingling', 'modeling', 'mulling', 'mumbling', 'muttering', 'nailing', 'naming', 'paging', 'paneling', 'patting', 'peeling', 'piling', 'pleading', 'polling', 'pooling', 'portraying', 'purging', 'puzzling', 'rambling', 'reeling', 'ribbing', 'ridiculing', 'ruling', 'rumbling', 'rummaging', 'sampling', 'scrambling', 'screaming', 'scrubbing', 'sealing', 'selling', 'sewing', 'sharing', 'shuffling', 'sibling', 'signaling', 'singling', 'sizzling', 'smelling', 'sniffing', 'soaring', 'sobbing', 'spelling', 'spewing', 'spilling', 'spiraling', 'splitting', 'spoiling', 'spying', 'squealing', 'stealing', 'sticking', 'stifling', 'sting', 'stinging', 'stinking', 'stirling', 'striking', 'stuffing', 'stumbling', 'styling', 'subbing', 'suing', 'surging', 'swelling', 'taming', 'teeming', 'terrifying', 'tingling', 'trampling', 'traveling', 'trembling', 'trifling', 'trimming', 'trucking', 'tumbling', 'viewing', 'wobbling', 'wrapping', 'wrestling']\n"
     ]
    }
   ],
   "source": [
    "cands = checker.candidates(\"speling\")\n",
    "print(\"Candidatos para 'speling':\", sorted(list(cands)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b574d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "digging → 3.7052169713974\n",
      "trucking → 1.6046089363098144\n",
      "paneling → 2.954586882591247\n",
      "spelling → 6.863473677635192\n",
      "wrapping → 1.9276108903884888\n",
      "sealing → 4.942210045576095\n",
      "baffling → 2.500096553087234\n",
      "stinking → 2.380250285625457\n",
      "linking → 3.6681769719123833\n",
      "scrambling → 1.43938517165184\n",
      "briefing → 2.8762612447738647\n",
      "screaming → 3.7351446533203125\n",
      "ruling → 4.867081147193909\n",
      "hurtling → 1.7070206418037408\n",
      "pleading → 2.82184118103981\n",
      "signaling → 2.760330406427383\n",
      "reeling → 4.3842892656326296\n",
      "stirling → 4.200774427175522\n",
      "despairing → 0.12614150714874217\n",
      "grappling → 2.624631917953491\n",
      "naming → 3.0762742764949795\n",
      "lingering → 1.2601444263458248\n",
      "purging → 2.2320652267932886\n",
      "inkling → 2.903462163209915\n",
      "spiraling → 3.35547372508049\n",
      "surging → 3.5404468657970423\n",
      "curving → 2.407335652351379\n",
      "ribbing → 1.1845391852855682\n",
      "fooling → 3.9569705739021295\n",
      "sizzling → 3.218874657869339\n",
      "calming → 2.3263598227500912\n",
      "dressing → 3.970941685914992\n",
      "portraying → -0.33065012383460957\n",
      "sniffing → 2.1781785161495204\n",
      "bursting → 1.8075159225463873\n",
      "boiling → 4.5661489713191985\n",
      "embodying → -0.5298656589984896\n",
      "copying → 3.2847111649513243\n",
      "wrestling → 3.463875331640243\n",
      "growling → 2.7578857355117803\n",
      "taming → 1.7232641403675082\n",
      "wobbling → 1.801395277261733\n",
      "idling → 2.79539294052124\n",
      "bombing → 3.847471219062804\n",
      "spoiling → 4.6171472609043125\n",
      "charming → 2.6173464624881753\n",
      "muttering → 0.5668970096111297\n",
      "stealing → 5.957131669044495\n",
      "shuffling → 1.9257777359485626\n",
      "singling → 2.631353073120117\n",
      "sticking → 3.751324476480484\n",
      "rumbling → 1.7835740032196048\n",
      "chugging → 0.39532975959777783\n",
      "paging → 2.971757768392563\n",
      "clawing → 1.430129389762878\n",
      "debunking → 0.061947927474975906\n",
      "charging → 2.765317540645599\n",
      "terrifying → 0.054869130849839465\n",
      "meddling → 2.259387589693069\n",
      "polling → 4.681695298671722\n",
      "circling → 2.785605065345764\n",
      "piling → 4.656309605836867\n",
      "cling → 3.738339977025986\n",
      "spying → 5.146525374650954\n",
      "puzzling → 2.025820834159851\n",
      "dueling → 4.524841075897216\n",
      "trifling → 1.417501786708832\n",
      "rummaging → -1.1465648417472831\n",
      "crumbling → 1.046344989299774\n",
      "sibling → 5.112243519067764\n",
      "stuffing → 2.496799738883972\n",
      "sting → 4.161311925649643\n",
      "modeling → 4.857303025484085\n",
      "scrubbing → 1.2405260858535767\n",
      "swelling → 5.1418683488368995\n",
      "pooling → 3.489768815517426\n",
      "chafing → 1.3170732834339143\n",
      "fumbling → 1.6074795703887936\n",
      "stinging → 1.9430463056564324\n",
      "dreaming → 3.634753992319107\n",
      "sampling → 4.089585030078888\n",
      "crippling → 2.372861788511276\n",
      "trembling → 2.2247740952968593\n",
      "viewing → 4.552701127052307\n",
      "humming → 2.1758344144821162\n",
      "creaking → 2.036813712120056\n",
      "curbing → 2.0717375366687767\n",
      "dangling → 2.720272170066833\n",
      "trampling → 0.752825633049012\n",
      "dabbling → 1.761358566284179\n",
      "squealing → 2.6847974629402156\n",
      "sharing → 5.360070740222932\n",
      "spewing → 5.503056472301482\n",
      "mumbling → 1.6727083675861358\n",
      "suing → 3.7512012302875517\n",
      "tingling → 1.9911796236038208\n",
      "cringing → 0.2701815710067752\n",
      "splitting → 2.927406744480133\n",
      "selling → 6.765751191616056\n",
      "styling → 4.913114485263824\n",
      "rambling → 2.6122653832435603\n",
      "teeming → 2.960947361946106\n",
      "drugging → 0.5699203875064853\n",
      "peeling → 4.61135079717636\n",
      "dialing → 3.6032803013324735\n",
      "striking → 4.476165809631347\n",
      "tumbling → 2.172309605836868\n",
      "smelling → 5.006767105579376\n",
      "fawning → 1.8321061575412738\n",
      "sewing → 5.191067573785782\n",
      "embezzling → -0.026451972246170463\n",
      "ailing → 3.6342466754913323\n",
      "soaring → 4.131922622680664\n",
      "stumbling → 2.3248649382591244\n",
      "trimming → 0.9644600601196283\n",
      "fairing → 1.4336891281604762\n",
      "sobbing → 4.18115430521965\n",
      "patting → 1.7236826529502869\n",
      "curling → 4.229739189386367\n",
      "dwindling → 1.2497908828258515\n",
      "mulling → 2.560255304574966\n",
      "nailing → 3.0746559290885926\n",
      "grieving → 2.469723163843155\n",
      "fleming → 3.8364505436420444\n",
      "fencing → 2.633541836977005\n",
      "bubbling → 2.1610061199665065\n",
      "filing → 4.5154075098037705\n",
      "spilling → 4.738339882373809\n",
      "stifling → 3.0234139578342445\n",
      "chatting → 1.729228946447372\n",
      "subbing → 2.725394281625748\n",
      "labeling → 4.31724396944046\n",
      "kipling → 2.970769252300263\n",
      "juggling → 2.173114952564239\n",
      "deeming → 3.0163651058673855\n",
      "emitting → 0.9329286339282987\n",
      "hustling → 1.6724861912727356\n",
      "traveling → 4.093550093650816\n",
      "ridiculing → -0.6418247418403622\n",
      "mingling → 1.665280971765518\n"
     ]
    }
   ],
   "source": [
    "word = \"speling\"\n",
    "for cand in checker.candidates(word):\n",
    "    print(cand, \"→\", checker.score(word, cand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c112e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ths', ' ', 'is', ' ', 'a', ' ', 'smple', ' ', 'sentnce', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ths is a smple sentnce.\"\n",
    "print(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa6f62ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de vocabulario: 41433\n",
      "Palabras más frecuentes: [('the', 70003), ('of', 36473), ('and', 28935), ('to', 26247), ('a', 23517), ('in', 21422), ('that', 10789), ('is', 10109), ('was', 9815), ('he', 9801)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño de vocabulario:\", len(checker.vocab))\n",
    "print(\"Palabras más frecuentes:\", checker.vocab.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a020de5e",
   "metadata": {},
   "source": [
    "### Referencias\n",
    "\n",
    "- Word Representations: https://fasttext.cc/docs/en/unsupervised-tutorial.html\n",
    "\n",
    "- FastText GitHub: https://github.com/facebookresearch/fastText/\n",
    "\n",
    "- Building a spelling correction/word suggestion module: https://github.com/Svantevith/spelling-correction-module-using-fasttext/blob/master/Spelling_corrector_word_suggestion_module_using_fastText.ipynb\n",
    "\n",
    "- system_transcription_error_detection_and_correction: https://github.com/kmariael/system_transcription_error_detection_and_correction\n",
    "\n",
    "- Misspelling-Correction-with-fastText-sequence-labeling: https://github.com/leechehao/Misspelling-Correction-with-fastText-sequence-labeling\n",
    "\n",
    "- Enriching Word Vectors with Subword Information: https://arxiv.org/pdf/1607.04606\n",
    "\n",
    "- Bag of Tricks for Efficient Text Classification: https://arxiv.org/abs/1607.01759\n",
    "\n",
    "- FastText.zip: Compressing text classification models: https://arxiv.org/abs/1612.03651"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
