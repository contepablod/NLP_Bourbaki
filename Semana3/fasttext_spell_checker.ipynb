{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a236957",
   "metadata": {},
   "source": [
    "## Procesamiento de Lenguaje Natural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e9c17",
   "metadata": {},
   "source": [
    "![Colegio Bourbaki](./Images/Bourbaki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b317ecc",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "## Corrector ortográfico con FastText\n",
    "\n",
    "En este notebook construimos un **corrector ortográfico inteligente** combinando técnicas clásicas de NLP con **embeddings FastText**.\n",
    "\n",
    "### ¿Qué hace el código?\n",
    "1. **Tokenización del texto**  \n",
    "   Dividimos las oraciones en palabras, espacios y signos de puntuación.  \n",
    "   Solo intentamos corregir las palabras alfabéticas, manteniendo el resto intacto.\n",
    "\n",
    "2. **Generación de candidatos (edits)**  \n",
    "   Para cada palabra mal escrita, generamos posibles variantes aplicando operaciones de edición:\n",
    "   - Eliminación de una letra  \n",
    "   - Inserción de una letra  \n",
    "   - Sustitución de una letra  \n",
    "   - Transposición de dos letras seguidas  \n",
    "\n",
    "   Además, consideramos los errores más comunes de teclado (QWERTY), donde confundir teclas adyacentes tiene menor costo.\n",
    "\n",
    "3. **Distancia de edición ponderada**  \n",
    "   Calculamos qué tan diferente es cada candidato de la palabra original, asignando costos menores a errores típicos de tipeo.\n",
    "\n",
    "4. **Embeddings de FastText**  \n",
    "   Usamos vectores de palabras (FastText) para comparar la similitud semántica entre la palabra mal escrita y cada candidato.  \n",
    "   Esto permite corregir incluso palabras fuera del vocabulario (OOV) gracias a los sub-tokens de FastText.\n",
    "\n",
    "5. **Ranking de candidatos**  \n",
    "   Combinamos:\n",
    "   - Similitud de embeddings  \n",
    "   - Frecuencia de la palabra en el corpus  \n",
    "   - Distancia de edición  \n",
    "\n",
    "   El mejor candidato según esta puntuación se elige como corrección.\n",
    "\n",
    "6. **Reconstrucción del texto**  \n",
    "   Reemplazamos solo las palabras corregidas y dejamos intactos los espacios y la puntuación, devolviendo la oración completa.\n",
    "\n",
    "---\n",
    "\n",
    "En resumen: el sistema propone correcciones ortográficas basadas no solo en la cercanía de edición, sino también en la **probabilidad de uso real** (frecuencia) y la **cercanía semántica**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbe273e",
   "metadata": {},
   "source": [
    "Link de Interés: https://fasttext.cc/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e934ea4",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c204d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "from __future__ import annotations\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from collections.abc import Iterable\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "from wordfreq import zipf_frequency \n",
    "\n",
    "# Gensim for FastText\n",
    "import gensim.downloader as api\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.fasttext import load_facebook_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb019b6",
   "metadata": {},
   "source": [
    "### Funciones de ayuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d80521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r\"([A-Za-z]+|[^A-Za-z\\s]+|\\s+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3db368",
   "metadata": {},
   "source": [
    "Usamos esa regex para separar el texto en tokens manteniendo todo, de manera que al corregir podamos reconstruir la oración sin perder espacios o símbolos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1063ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"Split into tokens: words, whitespace, punctuation blocks. Keep everything to reconstruct.\"\"\"\n",
    "    return TOKEN_RE.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794d3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_word(tok: str) -> bool:\n",
    "    '''Returns true if token is a word (only letters).'''\n",
    "    return tok.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5a02e1",
   "metadata": {},
   "source": [
    "Importamos todas las letras minúsculas del alfabeto inglés desde la librería estándar string.\n",
    "Esto se usa para probar reemplazos e inserciones de letras en las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab16ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55e964",
   "metadata": {},
   "source": [
    "Construimos un diccionario de adyacencia de teclado (layout QWERTY en inglés).\n",
    "\n",
    "Ejemplo: la letra s tiene como adyacentes a, d, w, x.\n",
    "\n",
    "Esto sirve para que errores como “s → a” o “s → d” cuesten menos en la distancia de edición (porque son errores de tipeo comunes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23369fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple keyboard adjacency map (US QWERTY). Extend as needed.\n",
    "KEY_ADJ: dict[str, set[str]] = defaultdict(set)\n",
    "\n",
    "_adj_rows = [\n",
    "    \"qwertyuiop\",\n",
    "    \"asdfghjkl\",\n",
    "    \"zxcvbnm\",\n",
    "]\n",
    "for row in _adj_rows:\n",
    "    for i, ch in enumerate(row):\n",
    "        if i > 0:\n",
    "            KEY_ADJ[ch].add(row[i-1])\n",
    "        if i < len(row)-1:\n",
    "            KEY_ADJ[ch].add(row[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ddd0c",
   "metadata": {},
   "source": [
    "Generamos todas las palabras que están a una edición de distancia de la original:\n",
    "\n",
    "deletes → eliminar una letra.\n",
    "\n",
    "transposes → intercambiar dos letras consecutivas.\n",
    "\n",
    "replaces → reemplazar una letra por otra del alfabeto.\n",
    "\n",
    "inserts → insertar una letra extra en cualquier posición.\n",
    "\n",
    "Estas son las candidatas para corrección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c903c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits(word: str) -> set[str]:\n",
    "    word = word.lower()\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in ALPHABET]\n",
    "    inserts  = [L + c + R for L, R in splits for c in ALPHABET]\n",
    "    return set(deletes + transposes + replaces + inserts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa29247",
   "metadata": {},
   "source": [
    "Implementamos una variante del algoritmo de Damerau-Levenshtein para calcular la distancia entre dos palabras:\n",
    "\n",
    "Inserción = costo 1\n",
    "\n",
    "Eliminación = costo 1\n",
    "\n",
    "Sustitución = costo 1 (pero si la sustitución es entre teclas adyacentes → costo 0.6)\n",
    "\n",
    "Transposición = costo 0.8\n",
    "\n",
    "Así modelamos errores más realistas de escritura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a8b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_edit_distance(src: str, dst: str) -> float:\n",
    "    \"\"\"Damerau-Levenshtein style with lower cost for adjacent-key substitutions.\"\"\"\n",
    "    src, dst = src.lower(), dst.lower()\n",
    "    n, m = len(src), len(dst)\n",
    "    dp = np.zeros((n+1, m+1), dtype=float)\n",
    "    for i in range(n+1):\n",
    "        dp[i,0] = i\n",
    "    for j in range(m+1):\n",
    "        dp[0,j] = j\n",
    "    for i in range(1, n+1):\n",
    "        for j in range(1, m+1):\n",
    "            cost_sub = 0 if src[i-1]==dst[j-1] else (0.6 if dst[j-1] in KEY_ADJ[src[i-1]] else 1.0)\n",
    "            dp[i,j] = min(\n",
    "                dp[i-1,j] + 1,         # deletion\n",
    "                dp[i, j-1] + 1,        # insertion\n",
    "                dp[i-1,j-1] + cost_sub # substitution\n",
    "            )\n",
    "            if i>1 and j>1 and src[i-1]==dst[j-2] and src[i-2]==dst[j-1]:\n",
    "                dp[i,j] = min(dp[i,j], dp[i-2,j-2] + 0.8)  # transposition\n",
    "    return float(dp[n,m])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b00073",
   "metadata": {},
   "source": [
    "Vamos a definir un conjunto de parámetros ajustables para balancear los criterios de correción.\n",
    "\n",
    "Por ejemplo, cuando evaluamos un candidato, el sistema calcula algo por el estilo:\n",
    "\n",
    "score = sim_w * similitud + freq_w * log_frecuencia - dist_w * distancia\n",
    "\n",
    "El candidato con mayor score es el que elegimos como corrección.\n",
    "\n",
    "* sim_w → controla cuánto valoramos la similitud semántica entre la palabra original y el candidato, medida con FastText (cosine similarity).\n",
    "\n",
    "* freq_w → controla cuánto valoramos que el candidato sea una palabra frecuente en el vocabulario.\n",
    "\n",
    "* dist_w → controla cuánto penalizamos candidatos que están muy lejos de la palabra original en términos de operaciones de edición."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cd31cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RankWeights:\n",
    "    sim_w: float = 2.0  # peso para la similitud de embeddings (coseno)\n",
    "    freq_w: float = 1.2  # peso para la frecuencia de la palabra en el corpus\n",
    "    dist_w: float = 1.0  # penalización por distancia de edición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa91d5",
   "metadata": {},
   "source": [
    "Vamos a definir la clase principal del corrector ortográfico utilizando FastText. \n",
    "Esta clase incluirá métodos para entrenar el modelo, generar candidatos de corrección y seleccionar la mejor corrección basada en una puntuación compuesta, es decir,\n",
    "la tokenización, la verificación de palabras conocidas, la generación de ediciones, el cálculo de similitud de embeddings y la distancia de edición ponderada, para entrenar, cargar, generar candidatos y corregir texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eb9e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellChecker:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model=None,  \n",
    "        vocab: Counter | None = None,  \n",
    "        weights: RankWeights | None = None, \n",
    "    ):\n",
    "        self.model = model  # modelo FastText (entrenado o cargado)\n",
    "        self.vocab = vocab or Counter()  # frecuencias de palabras\n",
    "        self.weights = weights or RankWeights()  # pesos de ranking\n",
    "        self._wordset: set[str] = (\n",
    "            set()\n",
    "        )  # conjunto rápido para saber si una palabra existe\n",
    "        try:\n",
    "            zipf_frequency,\n",
    "            # escala ~0..7 (más alto = más frecuente)\n",
    "            self.zipf_frequency = zipf_frequency\n",
    "        except Exception:\n",
    "            self.zipf_frequency = None\n",
    "\n",
    "    # Model & vocab\n",
    "    def train_model(\n",
    "        self,\n",
    "        corpus: Iterable[str],\n",
    "        size: int = 100,\n",
    "        window: int = 5,\n",
    "        min_count: int = 1,\n",
    "        epochs: int = 10,\n",
    "    ) -> None:\n",
    "        sentences = [re.findall(r\"[A-Za-z]+\", s.lower()) for s in corpus]\n",
    "        self.model = FastText(vector_size=size, window=window, min_count=min_count)\n",
    "        self.model.build_vocab(sentences)\n",
    "        self.model.train(sentences, total_examples=len(sentences), epochs=epochs)\n",
    "        self.vocab = Counter(w for sent in sentences for w in sent)\n",
    "        self._wordset = set(self.vocab)\n",
    "\n",
    "    def load_pretrained(self, path: str) -> None:\n",
    "        if path.endswith(\".bin\"):\n",
    "            try:\n",
    "                self.model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "            except Exception:\n",
    "                self.model = load_facebook_vectors(path)\n",
    "        else:\n",
    "            self.model = KeyedVectors.load_word2vec_format(path, binary=False)\n",
    "        self._wordset = set(self.get_vocab_words())\n",
    "\n",
    "    def set_vocab_from_corpus(self, corpus: Iterable[str]) -> None:\n",
    "        words = [w.lower() for s in corpus for w in re.findall(r\"[A-Za-z]+\", s)]\n",
    "        self.vocab = Counter(words)\n",
    "        self._wordset = set(self.vocab)\n",
    "\n",
    "    def get_vocab_words(self) -> Iterable[str]:\n",
    "        m = self.model\n",
    "        if hasattr(m, \"key_to_index\"):\n",
    "            return list(m.key_to_index.keys())\n",
    "        if hasattr(m, \"wv\") and hasattr(m.wv, \"key_to_index\"):\n",
    "            return list(m.wv.key_to_index.keys())\n",
    "        return list(self.vocab.keys())\n",
    "\n",
    "    # Scoring\n",
    "    def _freq_prior(self, w):\n",
    "\n",
    "        # Prior por frecuencia de palabra:\n",
    "        # - Si está wordfreq: usa Zipf (0..7).\n",
    "        # - Si no, usa log-frecuencia de tu vocab.\n",
    "        if self.zipf_frequency is not None:\n",
    "            return self.zipf_frequency(w.lower(), \"en\")  # cambiá \"en\" por tu idioma si querés\n",
    "        return math.log(self.vocab.get(w.lower(), 0) + 1.0)\n",
    "\n",
    "    def _cosine(self, a, b):\n",
    "        m = self.model\n",
    "        if m is None:\n",
    "            return 0.0\n",
    "        try:\n",
    "            if hasattr(m, \"wv\"):\n",
    "                va, vb = m.wv[a], m.wv[b]\n",
    "            else:\n",
    "                va, vb = m[a], m[b]\n",
    "            denom = (np.linalg.norm(va) * np.linalg.norm(vb))\n",
    "            if denom == 0:\n",
    "                return 0.0\n",
    "            return float(np.dot(va, vb) / denom)\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    def score(self, src, cand, prev=None, nxt=None):\n",
    "        # Señales\n",
    "        d   = weighted_edit_distance(src, cand)\n",
    "        sim = self._cosine(src.lower(), cand.lower())\n",
    "        fr  = self._freq_prior(cand)\n",
    "\n",
    "        # Contexto (promedio de similitud con vecinos alfabéticos si existen)\n",
    "        ctx = 0.0\n",
    "        cnt = 0\n",
    "        if prev and prev.isalpha():\n",
    "            ctx += self._cosine(cand.lower(), prev.lower()); cnt += 1\n",
    "        if nxt and nxt.isalpha():\n",
    "            ctx += self._cosine(cand.lower(), nxt.lower());  cnt += 1\n",
    "        if cnt:\n",
    "            ctx /= cnt\n",
    "\n",
    "        # Pequeño bono si el candidato ya es igual (evita cambios innecesarios)\n",
    "        ident_bonus = 0.15 if src.lower() == cand.lower() else 0.0\n",
    "\n",
    "        # Mezcla final (aumenté un poco el peso de similitud y el contexto)\n",
    "        return ident_bonus + (2.5 * sim) + (1.4 * fr) + (1.0 * ctx) - (1.2 * d)\n",
    "\n",
    "    # Candidates\n",
    "    def known(self, words: Iterable[str]) -> set[str]:\n",
    "        if self._wordset:\n",
    "            return {w for w in words if w in self._wordset}\n",
    "        return {w for w in words if self.vocab.get(w, 0) > 0}\n",
    "\n",
    "    def candidates(self, word, max_edits=2, k_from_embeddings=200):\n",
    "        wl = word.lower()\n",
    "        cands = set()\n",
    "\n",
    "        # 3 fuentes: identidad/edits, vecinos de edits->edits (edits2), y vecinos por embeddings\n",
    "        # a) identidad + edits1 + edits2 (pruned)\n",
    "        cands |= self.known({wl})\n",
    "        e1 = self.known(edits(wl))\n",
    "        cands |= e1\n",
    "        if max_edits >= 2 and e1:\n",
    "            for w1 in list(e1)[:300]:  # limitar expansión para no explotar el espacio\n",
    "                cands |= self.known(edits(w1))\n",
    "\n",
    "        # b) vecinos por embeddings (si hay modelo)\n",
    "        try:\n",
    "            m = self.model.wv if hasattr(self.model, \"wv\") else self.model\n",
    "            if hasattr(m, \"most_similar\"):\n",
    "                for w, _ in m.most_similar(wl, topn=k_from_embeddings):\n",
    "                    cands.add(w)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        if not cands:\n",
    "            cands.add(wl)\n",
    "\n",
    "        # c) filtro suave por frecuencia (descartar muy raras)\n",
    "        #    umbral ajustable: si tenés wordfreq, pedí Zipf>=2.5; si no, al menos que aparezca en vocab\n",
    "        filtered = set()\n",
    "        for w in cands:\n",
    "            if self.zipf_frequency is not None:\n",
    "                if self.zipf_frequency(w, \"en\") >= 2.5:\n",
    "                    filtered.add(w)\n",
    "            else:\n",
    "                if self.vocab.get(w, 0) > 0:\n",
    "                    filtered.add(w)\n",
    "        if filtered:\n",
    "            cands = filtered\n",
    "\n",
    "        return cands\n",
    "\n",
    "    # Correction\n",
    "    def correct_word(self, word, prev=None, nxt=None, max_edits=2):\n",
    "        if len(word) <= 2:\n",
    "            return word\n",
    "        cands = self.candidates(word, max_edits=max_edits)\n",
    "        best = max(cands, key=lambda c: self.score(word, c, prev=prev, nxt=nxt))\n",
    "        if word.istitle():\n",
    "            return best.title()\n",
    "        if word.isupper():\n",
    "            return best.upper()\n",
    "        return best\n",
    "\n",
    "    def correct(self, text, max_edits=2):\n",
    "        toks = tokenize(text)\n",
    "        out = []\n",
    "        # Para cada token palabra, miramos palabra previa/siguiente (alfabéticas) como contexto\n",
    "        for i, tok in enumerate(toks):\n",
    "            if tok.isalpha():\n",
    "                prev = None\n",
    "                nxt  = None\n",
    "                # buscar prev\n",
    "                j = i - 1\n",
    "                while j >= 0:\n",
    "                    if toks[j].isalpha(): prev = toks[j]; break\n",
    "                    j -= 1\n",
    "                # buscar next\n",
    "                j = i + 1\n",
    "                while j < len(toks):\n",
    "                    if toks[j].isalpha(): nxt = toks[j]; break\n",
    "                    j += 1\n",
    "                out.append(self.correct_word(tok, prev=prev, nxt=nxt, max_edits=max_edits))\n",
    "            else:\n",
    "                out.append(tok)\n",
    "        return \"\".join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1506de45",
   "metadata": {},
   "source": [
    "* Constructor __init__\n",
    "\n",
    "Se inicializa el corrector con un modelo de embeddings, un vocabulario y los parámetros de peso.\n",
    "\n",
    "\n",
    "* Sección “Model & vocab”\n",
    "\n",
    "train_model → entrena un modelo FastText desde cero sobre un corpus dado.\n",
    "\n",
    "load_pretrained → carga un modelo ya entrenado desde archivo (.bin o texto).\n",
    "\n",
    "set_vocab_from_corpus → construye un contador de frecuencias a partir de un corpus.\n",
    "\n",
    "get_vocab_words → obtiene la lista de palabras conocidas según el modelo o vocabulario.\n",
    "\n",
    "Esto define la base de conocimiento del corrector.\n",
    "\n",
    "\n",
    "* Sección “Scoring”\n",
    "\n",
    "_cosine → calcula la similitud de coseno entre embeddings de dos palabras.\n",
    "\n",
    "_log_freq → devuelve el logaritmo de la frecuencia de una palabra (más frecuente = más probable).\n",
    "\n",
    "score → combina: bonus de identidad (si ya coincide la palabra), similitud de embeddings, frecuencia, penalización por distancia de edición.\n",
    "\n",
    "Da un puntaje total para ordenar candidatos.\n",
    "\n",
    "* Sección “Candidates”\n",
    "\n",
    "known → filtra una lista de palabras dejando solo las que están en el vocabulario/modelo.\n",
    "\n",
    "candidates → genera posibles correcciones:\n",
    "\n",
    "Variantes por ediciones (borrar, insertar, sustituir, transponer).\n",
    "\n",
    "Vecinos semánticos según embeddings (most_similar).\n",
    "\n",
    "Si no encuentra nada, devuelve la palabra original.\n",
    "\n",
    "Aquí obtenemos qué opciones existen para corregir una palabra.\n",
    "\n",
    "* Sección “Correction”\n",
    "\n",
    "correct_word → evalúa todos los candidatos de una palabra y se queda con el de mayor score. Respeta mayúsculas/minúsculas.\n",
    "\n",
    "correct → tokeniza el texto completo, corrige solo las palabras alfabéticas, y reconstruye el texto final.\n",
    "\n",
    "Esta es la función que usamos directamente para corregir frases enteras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176217f1",
   "metadata": {},
   "source": [
    "Vamos a implementar una función que nos permita construir un corrector. Tenemos 3 modos de uso:\n",
    "\n",
    "* Modo pretrained (pretrained no es None)\n",
    "\n",
    "Carga un modelo FastText ya entrenado (ej. cc.en.300.bin).\n",
    "Si el vocabulario está vacío, se inicializa con una lista de hasta 200k palabras del modelo.\n",
    "\n",
    "* Modo demo (demo=True)\n",
    "\n",
    "Entrena un mini-modelo FastText sobre el TOY_CORPUS.\n",
    "Esto permite probar el pipeline sin descargar nada.\n",
    "\n",
    "* Modo fallback (ni pretrained ni demo)\n",
    "\n",
    "Usa solo el vocabulario del TOY_CORPUS, sin embeddings.\n",
    "Es el modo más básico, útil si no hay FastText disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d4db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOY_CORPUS = [\n",
    "    \"this is a simple sentence\",\n",
    "    \"another simple example sentence\",\n",
    "    \"we write code to correct spelling errors\",\n",
    "    \"fasttext uses subword embeddings\",\n",
    "    \"spell checking benefits from language models\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36ecaa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_checker(pretrained=None, demo: bool = True) -> SpellChecker:\n",
    "    \"\"\"\n",
    "    Construye un SpellChecker:\n",
    "      - Si pretrained es un str -> se asume ruta a archivo .bin/.txt\n",
    "      - Si pretrained es un objeto gensim (KeyedVectors/FastText) -> se usa directamente\n",
    "      - Si demo=True -> entrena mini-modelo en TOY_CORPUS\n",
    "      - Sino -> usa solo vocabulario de TOY_CORPUS (sin embeddings)\n",
    "    \"\"\"\n",
    "    checker = SpellChecker(weights=RankWeights(sim_w=2.0, freq_w=1.2, dist_w=1.0))\n",
    "\n",
    "    if pretrained is not None:\n",
    "        # Caso 1: ruta a archivo\n",
    "        if isinstance(pretrained, str):\n",
    "            checker.load_pretrained(pretrained)\n",
    "        else:\n",
    "            # Caso 2: objeto de modelo gensim ya cargado (ej. api.load)\n",
    "            checker.model = pretrained\n",
    "\n",
    "        # inicializar vocabulario si está vacío\n",
    "        if not checker.vocab:\n",
    "            words = list(checker.get_vocab_words())[:200000]\n",
    "            checker.vocab = Counter({w: 1 for w in words})\n",
    "            checker._wordset = set(words)\n",
    "\n",
    "    elif demo:\n",
    "        checker.train_model(TOY_CORPUS, size=100, window=5, min_count=1, epochs=10)\n",
    "\n",
    "    else:\n",
    "        checker.vocab = Counter(w for s in TOY_CORPUS for w in s.split())\n",
    "        checker._wordset = set(checker.vocab)\n",
    "\n",
    "    return checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21e71ff",
   "metadata": {},
   "source": [
    "Veamos cómo usarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b829f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"Ths is a smple sentnce with speling erors.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2da90e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : Ths is a smple sentnce with speling erors.\n",
      "OUTPUT: This is a simple sentence to spelling to.\n"
     ]
    }
   ],
   "source": [
    "# Build the checker with a tiny demo FastText model (no external downloads)\n",
    "checker = build_checker(pretrained=None, demo=True)\n",
    "print(\"INPUT :\", sample)\n",
    "print(\"OUTPUT:\", checker.correct(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb52a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load(\"fasttext-wiki-news-subwords-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bebc7c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : Ths is a smple sentnce with speling erors.\n",
      "OUTPUT: The is a simple sentence with spelling errors.\n"
     ]
    }
   ],
   "source": [
    "checker = build_checker(pretrained=model, demo=False)\n",
    "sample = \"Ths is a smple sentnce with speling erors.\"\n",
    "print(\"INPUT :\", sample)\n",
    "print(\"OUTPUT:\", checker.correct(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "490c07d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : Ths is a smple sentnce with speling erors.\n",
      "OUTPUT: This is a simple sentence with spelling errors.\n"
     ]
    }
   ],
   "source": [
    "checker = build_checker(pretrained=None, demo=False)\n",
    "sample = \"Ths is a smple sentnce with speling erors.\"\n",
    "print(\"INPUT :\", sample)\n",
    "print(\"OUTPUT:\", checker.correct(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a366f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/pdconte/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27699c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The is a simple sentence with spelling errors.\n"
     ]
    }
   ],
   "source": [
    "corpus = [\" \".join(sent) for sent in brown.sents()]\n",
    "checker = build_checker(demo=False)\n",
    "checker.train_model(corpus)\n",
    "print(checker.correct(\"Ths is a smple sentnce with speling erors.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e911a9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT : Ths is a smple sentnce with speling erors.\n",
      "OUTPUT: The is a simple sentence with spelling errors.\n",
      "------------------------------------------------------------\n",
      "INPUT : Shee liks to read boks in the librery.\n",
      "OUTPUT: She like to read books in the library.\n",
      "------------------------------------------------------------\n",
      "INPUT : We are lernig how to corect mispelled wrds.\n",
      "OUTPUT: We are lena how to correct spelled words.\n",
      "------------------------------------------------------------\n",
      "INPUT : The quik borwn fox jmps ovr the lazi dog.\n",
      "OUTPUT: The quick born for jumps over the lazy dog.\n",
      "------------------------------------------------------------\n",
      "INPUT : Natrual langauge procesing is intersting.\n",
      "OUTPUT: Natural language processing is interesting.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"Ths is a smple sentnce with speling erors.\",\n",
    "    \"Shee liks to read boks in the librery.\",\n",
    "    \"We are lernig how to corect mispelled wrds.\",\n",
    "    \"The quik borwn fox jmps ovr the lazi dog.\",\n",
    "    \"Natrual langauge procesing is intersting.\",\n",
    "]\n",
    "\n",
    "for s in examples:\n",
    "    print(\"INPUT :\", s)\n",
    "    print(\"OUTPUT:\", checker.correct(s))\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b604a510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spelling'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker.correct_word(\"speling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7eedc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'simple'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checker.correct_word(\"smple\", prev=\"a\", nxt=\"sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fc809cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidatos para 'speling': ['ailing', 'aiming', 'bing', 'briefing', 'bulging', 'bursting', 'bustling', 'calming', 'chafing', 'chatting', 'chugging', 'circling', 'cleaning', 'cling', 'coping', 'copying', 'creaking', 'crippling', 'cruising', 'curbing', 'curling', 'curving', 'damaging', 'dancing', 'darting', 'debunking', 'decking', 'deeming', 'delaying', 'denying', 'dialing', 'digging', 'dressing', 'drugging', 'dueling', 'embezzling', 'embodying', 'emitting', 'emptying', 'ensuing', 'escaping', 'facing', 'failing', 'fairing', 'falsifying', 'favoring', 'fawning', 'fencing', 'filing', 'fleming', 'fooling', 'fumbling', 'fuming', 'grieving', 'growling', 'humming', 'hurtling', 'hustling', 'idling', 'inkling', 'juggling', 'kipling', 'labeling', 'lifting', 'limping', 'lingering', 'linking', 'logging', 'meddling', 'mingling', 'modeling', 'mulling', 'mumbling', 'muttering', 'nailing', 'naming', 'napping', 'nudging', 'obeying', 'pacing', 'paging', 'patting', 'peeling', 'pleading', 'polling', 'pooling', 'portraying', 'puffing', 'puzzling', 'rambling', 'reeling', 'ribbing', 'ridiculing', 'ruling', 'rummaging', 'sampling', 'sealing', 'selling', 'sewing', 'sibling', 'signaling', 'singing', 'singling', 'sizzling', 'smelling', 'sniffing', 'sobbing', 'spelling', 'spewing', 'spilling', 'spiraling', 'splicing', 'spoiling', 'spraying', 'spying', 'squealing', 'sticking', 'stifling', 'sting', 'stinging', 'stinking', 'stirling', 'striking', 'stuffing', 'subbing', 'suing', 'sulking', 'surging', 'swelling', 'taming', 'teeming', 'terrifying', 'ticking', 'tingling', 'trampling', 'traveling', 'trembling', 'trifling', 'trimming', 'uttering', 'waging', 'wobbling', 'wrapping', 'wrestling']\n"
     ]
    }
   ],
   "source": [
    "cands = checker.candidates(\"speling\")\n",
    "print(\"Candidatos para 'speling':\", sorted(list(cands)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b574d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curving → 2.4114257230758662\n",
      "spilling → 4.740142028808593\n",
      "sticking → 3.7549466507434843\n",
      "curbing → 2.076314577341079\n",
      "obeying → 3.076804737567901\n",
      "mumbling → 1.6649324946403503\n",
      "puffing → 1.6046232278347015\n",
      "pooling → 3.4934511904716494\n",
      "squealing → 2.6789249153137202\n",
      "favoring → 1.3417812445163726\n",
      "sizzling → 3.21653785777092\n",
      "circling → 2.775640658855438\n",
      "dueling → 4.5210155007839194\n",
      "juggling → 2.1666570873260493\n",
      "wrapping → 1.9288007481098175\n",
      "polling → 4.677364127159119\n",
      "peeling → 4.606453977584838\n",
      "falsifying → -1.1403046538829802\n",
      "escaping → 2.85735041809082\n",
      "limping → 1.7810463192462924\n",
      "cling → 3.7253400549888616\n",
      "curling → 4.233124584197998\n",
      "growling → 2.7522533946037298\n",
      "sniffing → 2.1842918665409083\n",
      "spewing → 5.501380836725234\n",
      "taming → 1.7268560652732852\n",
      "facing → 3.9567791819572458\n",
      "tingling → 1.9798583173751831\n",
      "fencing → 2.6352004852294924\n",
      "selling → 6.764867254734037\n",
      "swelling → 5.152778234004975\n",
      "embezzling → -0.030254003524780693\n",
      "sewing → 5.182606098413467\n",
      "coping → 3.2936266517639154\n",
      "wobbling → 1.7935004930496206\n",
      "spraying → 4.091174916267395\n",
      "fuming → 1.7670875954627991\n",
      "humming → 2.1774860591888423\n",
      "denying → 3.650963238477707\n",
      "patting → 1.7112851848602295\n",
      "bustling → 2.059444891452789\n",
      "reeling → 4.380249262809754\n",
      "stinking → 2.380788068532943\n",
      "bursting → 1.8031650815010076\n",
      "chafing → 1.32296892786026\n",
      "ailing → 3.635914264440536\n",
      "cruising → 1.365560111284256\n",
      "ensuing → 2.5934993145465848\n",
      "suing → 3.7497053027153013\n",
      "hurtling → 1.7018502368926995\n",
      "chatting → 1.7328004567623134\n",
      "stifling → 3.02103677558899\n",
      "hustling → 1.6676851861476898\n",
      "stirling → 4.195767637014389\n",
      "signaling → 2.763423142433166\n",
      "debunking → 0.0713684415817264\n",
      "terrifying → 0.057284162044526354\n",
      "singing → 5.260667199850081\n",
      "pleading → 2.8223041601181027\n",
      "fooling → 3.9618143453598016\n",
      "puzzling → 2.0305106766223906\n",
      "fairing → 1.4350835788249965\n",
      "subbing → 2.726300272226334\n",
      "naming → 3.078905672550201\n",
      "spying → 5.156543127298354\n",
      "surging → 3.5363220753669733\n",
      "digging → 3.705267337322235\n",
      "napping → 2.34493858385086\n",
      "smelling → 5.0157596583366395\n",
      "sting → 4.162140877246856\n",
      "darting → 2.3457433278560638\n",
      "ribbing → 1.1779671771526337\n",
      "fawning → 1.8279649758338916\n",
      "trembling → 2.220001104354858\n",
      "delaying → 2.868521452903747\n",
      "wrestling → 3.4620672247409816\n",
      "meddling → 2.2611837756633753\n",
      "waging → 1.9192826178073883\n",
      "spelling → 6.8634538590908045\n",
      "paging → 2.972114055156708\n",
      "nudging → 1.2617849359512325\n",
      "logging → 3.38124763393402\n",
      "stuffing → 2.506579370975494\n",
      "traveling → 4.095504529953001\n",
      "sibling → 5.105941519975662\n",
      "labeling → 4.309008544683456\n",
      "modeling → 4.853976043224335\n",
      "rambling → 2.6073645403385157\n",
      "muttering → 0.5753604221343993\n",
      "aiming → 3.563197864055633\n",
      "spiraling → 3.356178251981735\n",
      "emptying → 1.9454709346294399\n",
      "kipling → 2.968153800487519\n",
      "damaging → 2.3893460967540747\n",
      "sobbing → 4.1793312971591945\n",
      "idling → 2.7884476583003996\n",
      "splicing → 2.8474228241443633\n",
      "striking → 4.477634915113448\n",
      "filing → 4.5210072171688065\n",
      "chugging → 0.39309145617485\n",
      "emitting → 0.9299740316867826\n",
      "teeming → 2.969734874725342\n",
      "crippling → 2.363810674190521\n",
      "pacing → 3.523788776397705\n",
      "failing → 4.8593026530742645\n",
      "sealing → 4.934627291679382\n",
      "drugging → 0.5713970925807956\n",
      "bing → 2.716042990207672\n",
      "fleming → 3.8306951191425327\n",
      "fumbling → 1.5987115781307217\n",
      "embodying → -0.5325582988262179\n",
      "singling → 2.6243450570106504\n",
      "ridiculing → -0.6471097366809841\n",
      "linking → 3.6660502781867974\n",
      "nailing → 3.0717664449214936\n",
      "decking → 2.358385622024536\n",
      "trimming → 0.9682498724460595\n",
      "spoiling → 4.619352036714554\n",
      "mingling → 1.6492160298824308\n",
      "trifling → 1.4187576565742495\n",
      "trampling → 0.7533516440391548\n",
      "grieving → 2.4715394663810732\n",
      "deeming → 3.0319269845485683\n",
      "mulling → 2.5621794915199274\n",
      "rummaging → -1.1438759272098533\n",
      "inkling → 2.9082350051403045\n",
      "portraying → -0.3365524737834926\n",
      "uttering → 1.4021746013164522\n",
      "sulking → 2.9287474019527435\n",
      "lingering → 1.2571969766616817\n",
      "dancing → 4.418071519136429\n",
      "calming → 2.3315741860866543\n",
      "creaking → 2.047930425405502\n",
      "sampling → 4.097539567947388\n",
      "stinging → 1.9431653659343713\n",
      "lifting → 3.327410605192184\n",
      "dialing → 3.6086935951709744\n",
      "dressing → 3.966076456785201\n",
      "briefing → 2.8807421729564666\n",
      "ticking → 2.9629152228832236\n",
      "cleaning → 3.781192316770553\n",
      "copying → 3.286297840595245\n",
      "bulging → 1.8762639744281762\n",
      "ruling → 4.863763105630875\n"
     ]
    }
   ],
   "source": [
    "word = \"speling\"\n",
    "for cand in checker.candidates(word):\n",
    "    print(cand, \"→\", checker.score(word, cand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c112e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ths', ' ', 'is', ' ', 'a', ' ', 'smple', ' ', 'sentnce', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"Ths is a smple sentnce.\"\n",
    "print(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa6f62ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño de vocabulario: 41433\n",
      "Palabras más frecuentes: [('the', 70003), ('of', 36473), ('and', 28935), ('to', 26247), ('a', 23517), ('in', 21422), ('that', 10789), ('is', 10109), ('was', 9815), ('he', 9801)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamaño de vocabulario:\", len(checker.vocab))\n",
    "print(\"Palabras más frecuentes:\", checker.vocab.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
