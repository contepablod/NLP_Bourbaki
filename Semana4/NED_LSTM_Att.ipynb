{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f1f620",
   "metadata": {},
   "source": [
    "## Procesamiento de Lenguaje Natural\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524413e",
   "metadata": {},
   "source": [
    "![Colegio Bourbaki](./Images/Bourbaki.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8e711",
   "metadata": {},
   "source": [
    "## Named Entity Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b578b7",
   "metadata": {},
   "source": [
    "La desambiguación de entidades nombradas (Named Entity Disambiguation, **NED**) es una tarea fundamental en el campo del Procesamiento del Lenguaje Natural. Su objetivo es identificar correctamente a qué entidad del mundo real se refiere una mención ambigua en un texto.\n",
    "\n",
    "Por ejemplo, cuando un texto menciona **“Amazon”**, esta palabra puede referirse a:\n",
    "\n",
    "- La empresa Amazon Inc.\n",
    "\n",
    "- El río Amazonas\n",
    "\n",
    "- La selva amazónica\n",
    "\n",
    "La **NED** busca resolver esa ambigüedad seleccionando la entidad correcta según el contexto.\n",
    "\n",
    "La NED suele ir después del proceso de Reconocimiento de Entidades Nombradas (NER): NER detecta las menciones de entidades en el texto (por ejemplo: “Amazon” es una Organización). NED determina a qué “Amazon” se refiere, vinculando esa mención con una entrada específica en una base de conocimiento, como Wikidata, DBpedia o Wikipedia.\n",
    "\n",
    "\n",
    "El proceso de NED se puede dividir en tres pasos principales:\n",
    "\n",
    "- Identificación de la mención:\n",
    "Se detecta la palabra o frase que podría ser una entidad (por ejemplo, “Paris”).\n",
    "\n",
    "- Generación de candidatos:\n",
    "Se buscan todas las posibles entidades que podrían corresponder a esa mención.\n",
    "Ejemplo: “Paris” → {Paris, France; Paris Hilton; Paris, Texas}.\n",
    "\n",
    "- Desambiguación (selección del candidato correcto):\n",
    "Se usa el contexto del texto, las relaciones semánticas y la frecuencia para decidir cuál de las opciones es la correcta.\n",
    "\n",
    "Por ejemplo, en la frase “Paris es una de las ciudades más visitadas del mundo”, el contexto “ciudad” ayuda a escoger Paris, France.\n",
    "\n",
    "Existen varios métodos para realizar NED:\n",
    "\n",
    "- Basados en reglas y diccionarios:\n",
    "Usan coincidencias exactas con nombres en bases de datos o listas de entidades.\n",
    "\n",
    "- Basados en similitud semántica:\n",
    "Calculan qué tan similar es el contexto del texto con las descripciones de cada entidad candidata.\n",
    "\n",
    "- Modelos de aprendizaje automático o profundo (Deep Learning):\n",
    "Utilizan modelos entrenados sobre grandes corpus (como BERT, ELMo o spaCy transformers) para entender el contexto y predecir la entidad correcta de manera más precisa.\n",
    "\n",
    "- Sistemas híbridos:\n",
    "Combinan reglas, información semántica y modelos neuronales para mejorar la precisión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48360fe8",
   "metadata": {},
   "source": [
    "![NED](./Images/NED_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f58a13",
   "metadata": {},
   "source": [
    "**Ejemplo**\n",
    "\n",
    "“Apple presentó el nuevo iPhone en su evento anual.”\n",
    "\n",
    "NER: Detecta “Apple” como una entidad del tipo Organización.\n",
    "\n",
    "NED: Decide que “Apple” se refiere a Apple Inc., y no a “apple” (la fruta).\n",
    "\n",
    "El sistema puede vincular esta mención con una base de conocimiento:\n",
    "\n",
    "Apple Inc. → Wikidata ID: Q312\n",
    "\n",
    "**Aplicaciones**\n",
    "\n",
    "- Motores de búsqueda semántica: mejoran la comprensión de consultas ambiguas.\n",
    "\n",
    "- Análisis de noticias y redes sociales: para identificar correctamente a personas o empresas mencionadas.\n",
    "\n",
    "- Sistemas de preguntas y respuestas (QA): permiten enlazar menciones a entidades reales.\n",
    "\n",
    "- Desambiguación en bases de conocimiento: ayudan a mantener consistencia entre datos textuales y estructurados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d4058d",
   "metadata": {},
   "source": [
    "### Base de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bfd7ce",
   "metadata": {},
   "source": [
    "La base de datos que utilizaremos proviene de los sistemas **AIDA-YAGO**\n",
    "\n",
    "AIDA-YAGO es un conjunto de datos y sistema de referencia ampliamente utilizado en investigación de Named Entity Disambiguation (NED).\n",
    "Combina dos componentes principales:\n",
    "\n",
    "AIDA → un sistema de desambiguación de entidades.\n",
    "\n",
    "YAGO → una base de conocimiento semántico (similar a Wikidata o DBpedia).\n",
    "\n",
    "En conjunto, AIDA-YAGO se usa para evaluar, entrenar y comparar algoritmos de NED, proporcionando textos con menciones ya enlazadas a entidades concretas del mundo real.\n",
    "\n",
    "**YAGO: la base de conocimiento**\n",
    "\n",
    "YAGO (Yet Another Great Ontology) es una base de conocimiento semántico creada en la Universidad de Múnich (Max Planck Institute for Informatics).\n",
    "Su propósito es unir la estructura de WordNet (una base léxica del inglés) con la información de Wikipedia.\n",
    "\n",
    "Características clave de YAGO:\n",
    "\n",
    "-Contiene millones de entidades: personas, lugares, organizaciones, eventos, etc.\n",
    "\n",
    "- Cada entidad está enlazada con una página de Wikipedia.\n",
    "\n",
    "- Incluye tipos semánticos y relaciones (por ejemplo, “Barack Obama —isA→ Person”, “Obama —bornIn→ Hawaii”).\n",
    "\n",
    "- Tiene alta precisión (≈95%), porque los datos son verificados automáticamente y por reglas consistentes.\n",
    "\n",
    "En pocas palabras, YAGO es una gran red de conocimiento estructurado que representa hechos sobre el mundo real, usada como “referencia” para enlazar menciones de texto.\n",
    "\n",
    "**AIDA: el sistema de desambiguación**\n",
    "\n",
    "AIDA (Accurate Online Disambiguation of Entities) es un sistema automático de desambiguación desarrollado por el mismo grupo de investigación del Max Planck Institute. Recibe un texto con menciones detectadas (por ejemplo, de un sistema NER). Genera candidatos de entidades usando YAGO. Utiliza características contextuales y semánticas (coherencia entre entidades, similitud de contexto, relaciones en YAGO) para seleccionar la mejor entidad. Puede procesar texto en línea y enlazarlo automáticamente con YAGO o Wikipedia.\n",
    "\n",
    "**AIDA-YAGO Dataset**\n",
    "\n",
    "El dataset AIDA-YAGO es un conjunto de artículos de noticias en inglés (extraídos de Reuters) donde:\n",
    "\n",
    "- Se han anotado manualmente las menciones de entidades (personas, lugares, organizaciones).\n",
    "\n",
    "- Cada mención está enlazada a una entidad en YAGO (y por tanto a Wikipedia).\n",
    "\n",
    "Este corpus es muy valioso porque:\n",
    "\n",
    "- Permite evaluar la precisión de sistemas de NED.\n",
    "\n",
    "- Contiene textos reales, no ejemplos sintéticos.\n",
    "\n",
    "El dataset AIDA-YAGO es uno de los estándares más usados para evaluar modelos de entity linking y NED. Se emplea para:\n",
    "\n",
    "- Entrenar modelos de aprendizaje profundo (como BERT, BLINK, REL, GENRE).\n",
    "\n",
    "- Comparar resultados entre diferentes enfoques.\n",
    "\n",
    "- Servir como benchmark público reproducible.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Texto del dataset:\n",
    "\n",
    "“Apple CEO Steve Jobs introduced the iPhone at Macworld in San Francisco.”\n",
    "\n",
    "En el corpus AIDA-YAGO:\n",
    "\n",
    "“Apple” → Apple Inc. (YAGO entity)\n",
    "\n",
    "“Steve Jobs” → Steve_Jobs\n",
    "\n",
    "“Macworld” → Macworld_Conference_and_Expo\n",
    "\n",
    "“San Francisco” → San_Francisco\n",
    "\n",
    "Cada una de estas menciones está etiquetada con su correspondiente entidad YAGO, lo que permite a los modelos aprender a desambiguar.\n",
    "\n",
    "\n",
    "Link: https://resources.mpi-inf.mpg.de/yago-naga/aida/downloads.html?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2d508e",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b65ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7320a3ee",
   "metadata": {},
   "source": [
    "### Configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf03cd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5178aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5888c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "torch.backends.cuda.matmul.fp32_precision = 'ieee'  # torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.conv.fp32_precision = 'tf32'# torch.backends.cudnn.allow_tf32 = True\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b99eba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__Python VERSION: 3.12.11 (main, Sep  5 2025, 19:35:43) [GCC 13.3.0]\n",
      "__pyTorch VERSION: 2.9.0+cu128\n",
      "__CUDA VERSION\n",
      "Thu Nov  6 19:24:12 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1650 Ti     Off |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   64C    P5              9W /   50W |     350MiB /   4096MiB |     32%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      8444      G   /usr/bin/gnome-shell                           81MiB |\n",
      "|    0   N/A  N/A     56372      G   /proc/self/exe                                206MiB |\n",
      "|    0   N/A  N/A     81942      G   ...scord/258/usr/share/discord/Discord         55MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "__CUDNN VERSION: 91002\n",
      "__Number CUDA Devices: 1\n",
      "__Devices\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  1\n",
      "Current cuda device  0\n"
     ]
    }
   ],
   "source": [
    "print(\"__Python VERSION:\", sys.version)\n",
    "print(\"__pyTorch VERSION:\", torch.__version__)\n",
    "print(\n",
    "    \"__CUDA VERSION\",\n",
    ")\n",
    "! nvidia-smi\n",
    "print(\"__CUDNN VERSION:\", torch.backends.cudnn.version())\n",
    "print(\"__Number CUDA Devices:\", torch.cuda.device_count())\n",
    "print(\"__Devices\")\n",
    "print(\"Active CUDA Device: GPU\", torch.cuda.current_device())\n",
    "print(\"Available devices \", torch.cuda.device_count())\n",
    "print(\"Current cuda device \", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c4548b",
   "metadata": {},
   "source": [
    "### Funciones de ayuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23b5f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93cb2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(seqs: list[list[int]], pad_idx: int):\n",
    "    ''' Pads a list of sequences to the same length with pad_idx.\n",
    "    Returns a tensor of shape (batch_size, max_seq_length) and a tensor of lengths.\n",
    "    '''\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    max_len = int(lengths.max()) if lengths.numel() > 0 else 0\n",
    "    out = torch.full((len(seqs), max_len), pad_idx, dtype=torch.long)\n",
    "    for i, s in enumerate(seqs):\n",
    "        if len(s) > 0:\n",
    "            out[i, : len(s)] = torch.tensor(s, dtype=torch.long)\n",
    "    return out, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daccb22",
   "metadata": {},
   "source": [
    "Vamos a construir una clase Vocab simple para manejar el vocabulario y la codificación de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ecc32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    ''' A simple vocabulary class that builds a mapping from tokens to indices and vice versa.\n",
    "        It also keeps track of token frequencies and allows filtering by minimum frequency.\n",
    "    '''\n",
    "    def __init__(self, min_freq: int = 1, specials: list[str] = None):\n",
    "        if specials is None:\n",
    "            specials = [\"<pad>\", \"<unk>\"]\n",
    "        self.min_freq = min_freq\n",
    "        self.freqs: dict[str, int] = {}\n",
    "        self.itos: list[str] = []\n",
    "        self.stoi: dict[str, int] = {}\n",
    "        self.specials = specials\n",
    "\n",
    "    def build(self, texts: Iterable[str]):\n",
    "        ''' Build the vocabulary from an iterable of texts.'''\n",
    "        for t in texts:\n",
    "            for tok in t.strip().split():\n",
    "                self.freqs[tok] = self.freqs.get(tok, 0) + 1\n",
    "        self.itos = list(self.specials)\n",
    "        for i, sp in enumerate(self.specials):\n",
    "            self.stoi[sp] = i\n",
    "        for tok, f in sorted(self.freqs.items(), key=lambda x: (-x[1], x[0])):\n",
    "            if f >= self.min_freq and tok not in self.stoi:\n",
    "                self.stoi[tok] = len(self.itos)\n",
    "                self.itos.append(tok)\n",
    "\n",
    "    @property\n",
    "    def pad_idx(self):\n",
    "        return self.stoi[\"<pad>\"]\n",
    "\n",
    "    @property\n",
    "    def unk_idx(self):\n",
    "        return self.stoi[\"<unk>\"]\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        return [self.stoi.get(tok, self.unk_idx) for tok in text.strip().split()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85ef6c4",
   "metadata": {},
   "source": [
    "Veamos como funciona el vocabulario con un ejemplo sencillo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bafad337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8\n",
      "Text: this is a test\n",
      "Encoded: [7, 6, 3, 2]\n",
      "Label: 1\n",
      "Text: another test example\n",
      "Encoded: [4, 2, 5]\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "toy_data = [\n",
    "    (\"this is a test\", 1),\n",
    "    (\"another test example\", 0)\n",
    "]\n",
    "toy_vocab = Vocab(min_freq=1)\n",
    "toy_vocab.build([text for text, _ in toy_data])\n",
    "print(\"Vocab size:\", len(toy_vocab))\n",
    "for text, label in toy_data:\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Encoded:\", toy_vocab.encode(text))\n",
    "    print(\"Label:\", label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48cf714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8\n",
      "Text: this is a test\n",
      "Encoded: [7, 6, 3, 2]\n",
      "Label: 1\n",
      "Text: another test example\n",
      "Encoded: [4, 2, 5]\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "toy_data = [(\"this is a test\", 1), (\"another test example\", 0)]\n",
    "toy_vocab = Vocab(min_freq=1)\n",
    "toy_vocab.build([text for text, _ in toy_data])\n",
    "print(\"Vocab size:\", len(toy_vocab))\n",
    "for text, label in toy_data:\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Encoded:\", toy_vocab.encode(text))\n",
    "    print(\"Label:\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08006a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'test': 2,\n",
       " 'a': 3,\n",
       " 'another': 4,\n",
       " 'example': 5,\n",
       " 'is': 6,\n",
       " 'this': 7}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_vocab.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655941f6",
   "metadata": {},
   "source": [
    "Ahora, crearemos 3 clases para manejar la base de Datos:\n",
    "\n",
    "- **NEDExample**: representa un ejemplo individual con mención, contexto, candidato y etiqueta.\n",
    "- **NEDDataset**: maneja la carga y almacenamiento de múltiples ejemplos desde un archivo CSV.\n",
    "- **NEDCollator**: prepara lotes de datos para el entrenamiento, incluyendo el padding de secuencias. En definitiva, convierte una lista de NEDExamples en tensores de Pytorch. Primero, recibe una clase Vocabulario que se encarga de codificar, entonces por cada ejemplo en el batch va a codificar el texto, truncarlo y definir la etiqueta segun la base de datos. También realizará padding de secuencias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d21062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NEDExample:\n",
    "    ''' A single example for Named Entity Disambiguation.\n",
    "    mention: str\n",
    "    context: str\n",
    "    candidate: str\n",
    "    label: int\n",
    "    ''' \n",
    "    mention: str\n",
    "    context: str\n",
    "    candidate: str\n",
    "    label: int\n",
    "\n",
    "\n",
    "class NEDDataset(Dataset):\n",
    "    def __init__(self, path: str, delimiter: str | None = None):\n",
    "        self.examples: list[NEDExample] = []\n",
    "        with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            sample = f.read(4096)\n",
    "            f.seek(0)\n",
    "\n",
    "            # Prefer explicit delimiter; otherwise auto-pick (tabs vs commas)\n",
    "            if delimiter is None:\n",
    "                delimiter = \"\\t\" if sample.count(\"\\t\") >= sample.count(\",\") else \",\"\n",
    "\n",
    "            reader = csv.DictReader(f, delimiter=delimiter)\n",
    "            if not reader.fieldnames:\n",
    "                raise ValueError(\"File has no header row.\")\n",
    "\n",
    "            # Normalize header names (and strip BOM)\n",
    "            headers = [h.lstrip(\"\\ufeff\").strip() for h in reader.fieldnames]\n",
    "            lower = [h.lower() for h in headers]\n",
    "\n",
    "            required = {\"mention\", \"context\", \"candidate\", \"label\"}\n",
    "            missing = required - set(lower)\n",
    "            if missing:\n",
    "                raise ValueError(f\"Missing required columns: {missing}. Got: {lower}\")\n",
    "\n",
    "            header_map = {orig: low for orig, low in zip(headers, lower)}\n",
    "\n",
    "            for raw_row in reader:\n",
    "                # Only use declared headers; ignore DictReader’s None/overflow bucket\n",
    "                row = {header_map[h]: (raw_row.get(h) or \"\") for h in headers}\n",
    "                self.examples.append(\n",
    "                    NEDExample(\n",
    "                        mention=row[\"mention\"].strip(),\n",
    "                        context=row[\"context\"].strip(),\n",
    "                        candidate=row[\"candidate\"].strip(),\n",
    "                        label=int(str(row[\"label\"]).strip() or 0),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "class NEDCollator:\n",
    "    \"\"\"Collate function for NED examples.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab: Vocab, ctx_max_len: int, cand_max_len: int):\n",
    "        self.vocab = vocab\n",
    "        self.ctx_max_len = ctx_max_len\n",
    "        self.cand_max_len = cand_max_len\n",
    "\n",
    "    def truncate(self, ids: list[int], max_len: int):\n",
    "        return ids[:max_len]\n",
    "\n",
    "    def __call__(self, batch: list[NEDExample]):\n",
    "        assert batch and len(batch) > 0, \"Empty batch!\"\n",
    "        ctx_ids, cand_ids, labels = [], [], []\n",
    "        # prefer UNK if available, else PAD\n",
    "        unk_idx = getattr(self.vocab, \"unk_idx\", self.vocab.pad_idx)\n",
    "\n",
    "        for ex in batch:\n",
    "            ctx_text = (getattr(ex, \"context\", \"\") or \"\").strip()\n",
    "            cand_text = (getattr(ex, \"candidate\", \"\") or \"\").strip()\n",
    "\n",
    "            ctx = self.vocab.encode(ctx_text)\n",
    "            cand = self.vocab.encode(cand_text)\n",
    "\n",
    "            # --- SAFETY: avoid zero-length sequences ---\n",
    "            if not ctx:\n",
    "                ctx = [unk_idx]\n",
    "            if not cand:\n",
    "                cand = [unk_idx]\n",
    "            # ------------------------------------------\n",
    "\n",
    "            ctx_ids.append(self.truncate(ctx, self.ctx_max_len))\n",
    "            cand_ids.append(self.truncate(cand, self.cand_max_len))\n",
    "            labels.append(int(getattr(ex, \"label\", 0) or 0))\n",
    "\n",
    "        ctx_pad, ctx_len = pad_sequences(ctx_ids, self.vocab.pad_idx)\n",
    "        cand_pad, cand_len = pad_sequences(cand_ids, self.vocab.pad_idx)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        return ctx_pad, ctx_len, cand_pad, cand_len, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec094e26",
   "metadata": {},
   "source": [
    "### Atención y Arquitectura Transfomer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a848d6e",
   "metadata": {},
   "source": [
    "![S2S](./Images/Seq2Seq.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb960490",
   "metadata": {},
   "source": [
    "![ex](./Images/Attn_ex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd553b5a",
   "metadata": {},
   "source": [
    "![dot](./Images/dot_prod.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df79181",
   "metadata": {},
   "source": [
    "![MHA](./Images/MHA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081d315a",
   "metadata": {},
   "source": [
    "![MHA_ex](./Images/MHA_economy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c29b11",
   "metadata": {},
   "source": [
    "![Trans](./Images/Original_Transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6a5a0",
   "metadata": {},
   "source": [
    "Fuente: Chapter 7 - Python Deep Learning - Ivan Vasilev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d087a",
   "metadata": {},
   "source": [
    "Por consiguiente, vamos a construir 2 clases (**LSTMEncoder** & **SelfAttentionEncoder**) que convierten un batch de tokens en vectores de tamano por secuencia, es decir, un vector por contexto o candidato:\n",
    "1. token_ids -> vectores (embeddings)\n",
    "2. codifica la secuencia de vectores con LSTM o atención\n",
    "3. Mean Pooling y proyección, para obtener una representación fija por secuencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e96b5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEncoder(nn.Module):\n",
    "    def forward(self, ids, lengths):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class LSTMEncoder(SequenceEncoder):\n",
    "    \"\"\"BiLSTM + mean pooling over non-pad tokens.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, emb_dim, hidden_dim, num_layers=1, dropout=0.1, pad_idx=0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.proj = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, ids, lengths):\n",
    "        emb = self.emb(ids)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            emb, lengths.to(\"cpu\"), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_out, batch_first=True\n",
    "        )  # [B, T', 2H]\n",
    "        # mask over actual tokens (use pad_idx, not hardcoded 0)\n",
    "        mask = (ids[:, : out.size(1)] != self.pad_idx).unsqueeze(-1).float()\n",
    "        summed = (out * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1.0)\n",
    "        pooled = summed / denom\n",
    "        return self.dropout(torch.tanh(self.proj(pooled)))\n",
    "\n",
    "\n",
    "class SelfAttentionEncoder(SequenceEncoder):\n",
    "    \"\"\"TransformerEncoder + mean pooling over non-pad tokens.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        emb_dim,\n",
    "        hidden_dim,\n",
    "        n_heads=4,\n",
    "        n_layers=2,\n",
    "        dropout=0.1,\n",
    "        pad_idx=0,\n",
    "        max_len=1024,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_len = max_len\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.pos = nn.Embedding(max_len, emb_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=hidden_dim * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=n_layers,\n",
    "            enable_nested_tensor=False,\n",
    "        )\n",
    "        self.proj = nn.Linear(emb_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, ids, lengths):\n",
    "        B, T = ids.size()\n",
    "        # clip or wrap positions if needed\n",
    "        if T > self.max_len:\n",
    "            pos_ids = torch.arange(T, device=ids.device) % self.max_len\n",
    "        else:\n",
    "            pos_ids = torch.arange(T, device=ids.device)\n",
    "        pos_ids = pos_ids.unsqueeze(0).expand(B, T)  # [B, T]\n",
    "\n",
    "        x = self.emb(ids) + self.pos(pos_ids)  # [B, T, D]\n",
    "        kpm = ids == self.pad_idx  # True where padding\n",
    "        x = self.encoder(x, src_key_padding_mask=kpm)\n",
    "        mask = (~kpm).unsqueeze(-1).float()  # non-pad mask\n",
    "        pooled = (x * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1.0)\n",
    "        return self.dropout(torch.tanh(self.proj(pooled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2771623",
   "metadata": {},
   "source": [
    "Vamos a proceder con la lógica de entrenamiento. Tenemos dos posibilidades:\n",
    "\n",
    "- **Entrenamiento par-a-par (pairwise):**\n",
    "\n",
    " Lo que le enseñas al modelo:\n",
    "\n",
    " «Para este par (contexto, candidato), ¿es la entidad correcta? Sí (1) / No (0)». Para cada mención, comparamos el candidato correcto contra uno incorrecto.\n",
    " \n",
    " Forma de los datos:\n",
    "\n",
    " Filas como: (mención = «Apple», contexto = «Me comí una manzana madura...», candidato = «Apple, la fruta», etiqueta = 1) (mención = «Apple», contexto = «Me comí una manzana madura...», candidato = «Apple Inc., la empresa», etiqueta = 0)\n",
    "\n",
    "Salida y pérdida del modelo:\n",
    "\n",
    "Un logit por fila → pasar por BCEWithLogitsLoss contra la etiqueta 0/1. Cada candidato se aprende de forma independiente. \n",
    "\n",
    "Inferencia (cómo elegir una entidad):\n",
    "\n",
    "para una mención con K candidatos, puntúa cada par de forma independiente y elige el logit más alto (o softmax sobre esos logits para obtener una sensación probabilística). \n",
    "\n",
    "Ventajas: creación de datos sencilla, flexible (se puede reutilizar como puntuador de plausibilidad general). \n",
    "\n",
    "Desventajas: no hay competencia explícita entre los candidatos; puede producir puntuaciones mal calibradas entre los candidatos.\n",
    "\n",
    "- **Entrenamiento por lista (listwise):**\n",
    "\n",
    "Lo que le enseñas al modelo:\n",
    "\n",
    "«Dados todos los candidatos para esta mención a la vez, haz que el candidato ideal supere al resto». Para cada mención, consideramos todos los  candidatos juntos y optimizamos para que el correcto supere a los demás.\n",
    "\n",
    "Forma de los datos:\n",
    "\n",
    "Agrupa a los candidatos por mención (un identificador), por ejemplo, para la misma (mención, contexto): candidatos = [fruta (etiqueta=1), empresa (etiqueta=0), …]\n",
    "\n",
    "Resultado y pérdida del modelo:\n",
    "\n",
    "El modelo produce una puntuación por candidato en el grupo. Aplica softmax dentro del grupo y minimiza NLL del candidato ideal: -log p(candidato ideal). Los candidatos compiten directamente.\n",
    "\n",
    "Inferencia:\n",
    "\n",
    "Compara siempre el grupo en su conjunto y elige la puntuación argmax (o probabilidad softmax).\n",
    "\n",
    "Ventajas: optimiza la decisión real (clasificación dentro de un conjunto) → normalmente mejor precisión top-1.\n",
    "\n",
    "Desventajas: necesita datos agrupados (≥1 positivo + ≥1 negativo por mención). Si un grupo solo tiene un candidato (Ki=1), la pérdida se reduce a 0 → no hay aprendizaje.\n",
    "\n",
    "**Ejemplo**\n",
    "\n",
    "Mención: «Manzana» en «Hoy me comí una manzana madura».\n",
    "\n",
    "Candidatos: [fruta SI, empresa NO]\n",
    "\n",
    "Por pares: Entrena fruta=1, empresa=0 de forma independiente. En el momento de la prueba, calcula ambas puntuaciones y elige la mayor.\n",
    "\n",
    "Por lista: Entrena ambas juntas para esa mención, de modo que softmax(puntuación_fruta, puntuación_empresa) otorgue mayor probabilidad a la fruta.\n",
    "\n",
    "Esa es la diferencia fundamental: plausibilidad independiente (por pares) frente a competencia directa (por lista)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f0c68",
   "metadata": {},
   "source": [
    "Para esto, primero vamos a instanciar una clase que permita realizar el scoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87703fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseScorer(nn.Module):\n",
    "    \"\"\"Pairwise scorer model for NED.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder: SequenceEncoder, hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 4, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, ctx_ids, ctx_len, cand_ids, cand_len):\n",
    "        ctx_vec = self.encoder(ctx_ids, ctx_len)\n",
    "        cand_vec = self.encoder(cand_ids, cand_len)\n",
    "        feat = torch.cat(\n",
    "            [ctx_vec, cand_vec, torch.abs(ctx_vec - cand_vec), ctx_vec * cand_vec],\n",
    "            dim=-1,\n",
    "        )\n",
    "        logits = self.mlp(feat).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b088c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    total = correct = 0\n",
    "    loss_sum = 0.0\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    for batch in loader:\n",
    "        ctx_ids, ctx_len, cand_ids, cand_len, labels = [\n",
    "            x.to(device) if torch.is_tensor(x) else x for x in batch\n",
    "        ]\n",
    "        logits = model(ctx_ids, ctx_len, cand_ids, cand_len)\n",
    "        loss = bce(logits, labels)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.numel()\n",
    "        loss_sum += loss.item() * labels.size(0)\n",
    "    return correct / max(total, 1), loss_sum / max(total, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060fe2e",
   "metadata": {},
   "source": [
    "* PairwiseScorer: \n",
    "\n",
    "El modelo aprende una función\n",
    "\n",
    "f(contexto,candidato)→logit\n",
    "\n",
    "que indica qué tan compatible es una pareja.\n",
    "\n",
    "Durante el entrenamiento, se usa un dataset con etiquetas 1 (correcto) o 0 (incorrecto).\n",
    "Se optimiza con BCEWithLogitsLoss, como en evaluate().\n",
    "\n",
    "Luego de entrenarlo, el modelo sabe producir un puntaje alto para contextos y entidades correctas.\n",
    "\n",
    "* evaluate: evaluación en lote\n",
    "\n",
    "Esta función se usa para medir el rendimiento general del modelo (accuracy y pérdida promedio) sobre un conjunto de validación o prueba.\n",
    "\n",
    "Procesa miles de ejemplos en batches.\n",
    "\n",
    "Calcula métricas globales.\n",
    "\n",
    "No sirve para uso interactivo, sino para comparar modelos o hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a36ed29",
   "metadata": {},
   "source": [
    "Entonces, podemos armar los entrenamientos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a571e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pairwise(args):\n",
    "    ''' Trains a pairwise NED model based on the provided arguments. '''\n",
    "    set_seed(args[\"seed\"])\n",
    "    train_ds = NEDDataset(args[\"train\"])\n",
    "    dev_ds = NEDDataset(args[\"dev\"]) if args.get(\"dev\") else None\n",
    "\n",
    "    # Optionally mark mentions in context\n",
    "    def mark_mention(ex):\n",
    "        m = (ex.mention or \"\").strip()\n",
    "        if m:\n",
    "            ex.context = f\"[MENTION] {m} [/MENTION] \" + ex.context\n",
    "        return ex\n",
    "\n",
    "    train_ds.examples = [mark_mention(ex) for ex in train_ds.examples]\n",
    "    if dev_ds:\n",
    "        dev_ds.examples = [mark_mention(ex) for ex in dev_ds.examples]\n",
    "    \n",
    "    # Vocab and DataLoaders\n",
    "    vocab = Vocab(min_freq=args[\"min_freq\"])\n",
    "    vocab.build(\n",
    "        [ex.context for ex in train_ds.examples]\n",
    "        + [ex.candidate for ex in train_ds.examples]\n",
    "    )\n",
    "    collate = NEDCollator(vocab, args[\"ctx_max_len\"], args[\"cand_max_len\"])\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=args[\"batch_size\"], shuffle=True, collate_fn=collate\n",
    "    )\n",
    "    dev_loader = (\n",
    "        DataLoader(\n",
    "            dev_ds, batch_size=args[\"batch_size\"], shuffle=False, collate_fn=collate\n",
    "        )\n",
    "        if dev_ds\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    if args[\"encoder\"] == \"lstm\":\n",
    "        enc = LSTMEncoder(\n",
    "            len(vocab),\n",
    "            args[\"emb_dim\"],\n",
    "            args[\"hidden_dim\"],\n",
    "            args[\"lstm_layers\"],\n",
    "            args[\"dropout\"],\n",
    "            pad_idx=vocab.pad_idx,\n",
    "        )\n",
    "    else:\n",
    "        enc = SelfAttentionEncoder(\n",
    "            len(vocab),\n",
    "            args[\"emb_dim\"],\n",
    "            args[\"hidden_dim\"],\n",
    "            args[\"attn_heads\"],\n",
    "            args[\"attn_layers\"],\n",
    "            args[\"dropout\"],\n",
    "            pad_idx=vocab.pad_idx,\n",
    "            max_len=max(args[\"ctx_max_len\"], args[\"cand_max_len\"]),\n",
    "        )\n",
    "    model = PairwiseScorer(\n",
    "        enc, hidden_dim=args[\"hidden_dim\"], dropout=args[\"dropout\"]\n",
    "    ).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"]\n",
    "    )\n",
    "    bce = nn.BCEWithLogitsLoss() #BCELoss\n",
    "    best_dev = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, args[\"epochs\"] + 1):\n",
    "        model.train()\n",
    "        total = correct = 0\n",
    "        loss_sum = 0.0\n",
    "        for _, batch in enumerate(train_loader, 1):\n",
    "            ctx_ids, ctx_len, cand_ids, cand_len, labels = [\n",
    "                x.to(DEVICE) if torch.is_tensor(x) else x for x in batch\n",
    "            ]\n",
    "            # Forward pass\n",
    "            opt.zero_grad()\n",
    "            logits = model(ctx_ids, ctx_len, cand_ids, cand_len)\n",
    "            loss = bce(logits, labels)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            # Compute metrics\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.numel()\n",
    "            loss_sum += loss.item() * labels.size(0)\n",
    "        if dev_loader is not None:\n",
    "            dev_acc, dev_loss = evaluate(model, dev_loader, DEVICE)\n",
    "            print(f\"Epoch {epoch} DONE | pairwise dev_loss={dev_loss:.4f} pairwise dev_acc={dev_acc:.4f}\")\n",
    "            best_dev = max(best_dev, dev_acc)\n",
    "\n",
    "    print(\"Training complete. Best dev acc:\", best_dev)\n",
    "    \n",
    "    return model, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f8bd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_listwise(args, augment_singletons=True):\n",
    "\n",
    "    def _read_groups_from_csv(path):\n",
    "        groups = defaultdict(lambda: {\"context\": None, \"mention\": None, \"cands\": []})\n",
    "        with open(path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "            rdr = csv.DictReader(f)\n",
    "            for r in rdr:\n",
    "                gid = str(r[\"id\"])\n",
    "                if groups[gid][\"context\"] is None:\n",
    "                    groups[gid][\"context\"] = r[\"context\"]\n",
    "                if groups[gid][\"mention\"] is None:\n",
    "                    groups[gid][\"mention\"] = r.get(\"mention\", \"\")\n",
    "                groups[gid][\"cands\"].append((r[\"candidate\"], int(r[\"label\"])))\n",
    "        return [{\"id\": k, **v} for k, v in groups.items()]\n",
    "\n",
    "    set_seed(args[\"seed\"])\n",
    "    train_groups = _read_groups_from_csv(args[\"train\"])\n",
    "    dev_groups = _read_groups_from_csv(args[\"dev\"]) if args.get(\"dev\") else []\n",
    "\n",
    "    # Mark mention in context\n",
    "    def mark(g):\n",
    "        m = (g.get(\"mention\", \"\") or \"\").strip()\n",
    "        if m:\n",
    "            g[\"context\"] = f\"[MENTION] {m} [/MENTION] \" + g[\"context\"]\n",
    "        return g\n",
    "\n",
    "    train_groups = [mark(g) for g in train_groups]\n",
    "    dev_groups = [mark(g) for g in dev_groups]\n",
    "\n",
    "    # Ensure Ki >= 2 (augment singletons with a negative)\n",
    "    def ensure_two(groups):\n",
    "        kept = []\n",
    "        # pool of candidate strings to sample negatives from\n",
    "        pool = list({c for g in groups for c, _ in g[\"cands\"]})\n",
    "        for g in groups:\n",
    "            Ki = len(g[\"cands\"])\n",
    "            pos_idx = next((i for i, (_, y) in enumerate(g[\"cands\"]) if y == 1), None)\n",
    "            if Ki >= 2:\n",
    "                kept.append(g)\n",
    "                continue\n",
    "            if not augment_singletons:\n",
    "                continue\n",
    "            gold = g[\"cands\"][pos_idx][0] if pos_idx is not None else None\n",
    "            # pick any different candidate as negative\n",
    "            neg = next((c for c in pool if c != gold), None)\n",
    "            if neg is None:\n",
    "                neg = (gold or \"DUMMY_ENTITY\") + \"_NEG\"\n",
    "            if pos_idx is None:\n",
    "                base = g[\"cands\"][0][0] if g[\"cands\"] else \"SOME_ENTITY\"\n",
    "                g[\"cands\"] = [(base, 1), (neg, 0)]\n",
    "            else:\n",
    "                g[\"cands\"].append((neg, 0))\n",
    "            kept.append(g)\n",
    "        return kept\n",
    "\n",
    "    train_groups = ensure_two(train_groups)\n",
    "    dev_groups = ensure_two(dev_groups)\n",
    "\n",
    "    # Build vocab\n",
    "    texts = []\n",
    "    for g in train_groups:\n",
    "        texts.append(g[\"context\"])\n",
    "        texts.extend(c for c, _ in g[\"cands\"])\n",
    "    vocab = Vocab(min_freq=args[\"min_freq\"])\n",
    "    vocab.build(texts)\n",
    "    # Model\n",
    "    if args[\"encoder\"] == \"lstm\":\n",
    "        enc = LSTMEncoder(\n",
    "            len(vocab),\n",
    "            args[\"emb_dim\"],\n",
    "            args[\"hidden_dim\"],\n",
    "            args[\"lstm_layers\"],\n",
    "            args[\"dropout\"],\n",
    "            pad_idx=vocab.pad_idx,\n",
    "        )\n",
    "    else:\n",
    "        enc = SelfAttentionEncoder(\n",
    "            len(vocab),\n",
    "            args[\"emb_dim\"],\n",
    "            args[\"hidden_dim\"],\n",
    "            args[\"attn_heads\"],\n",
    "            args[\"attn_layers\"],\n",
    "            args[\"dropout\"],\n",
    "            pad_idx=vocab.pad_idx,\n",
    "            max_len=max(args[\"ctx_max_len\"], args[\"cand_max_len\"]),\n",
    "        )\n",
    "    model = PairwiseScorer(\n",
    "        enc, hidden_dim=args[\"hidden_dim\"], dropout=args[\"dropout\"]\n",
    "    ).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(), lr=args[\"lr\"], weight_decay=args[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    def encode_texts(vocab, texts, max_len):\n",
    "        unk = getattr(vocab, \"unk_idx\", vocab.pad_idx)\n",
    "        ids = []\n",
    "        for t in texts:\n",
    "            seq = vocab.encode((t or \"\").strip())\n",
    "            if not seq:            # <-- guard: ensure length >= 1\n",
    "                seq = [unk]\n",
    "            ids.append(seq[:max_len])\n",
    "        pad, lengths = pad_sequences(ids, vocab.pad_idx)\n",
    "        return pad.to(DEVICE), lengths.to(DEVICE)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, args[\"epochs\"] + 1):\n",
    "        random.shuffle(train_groups)\n",
    "        model.train()\n",
    "        total_loss = steps = 0\n",
    "        for i in range(0, len(train_groups), args[\"batch_size\"]):\n",
    "            batch = [\n",
    "                g\n",
    "                for g in train_groups[i : i + args[\"batch_size\"]]\n",
    "                if len(g[\"cands\"]) >= 2\n",
    "            ]\n",
    "            if not batch:\n",
    "                continue\n",
    "            ctx_texts = [g[\"context\"] for g in batch]\n",
    "            cand_texts = list(\n",
    "                itertools.chain.from_iterable(\n",
    "                    [[c for c, _ in g[\"cands\"]] for g in batch]\n",
    "                )\n",
    "            )\n",
    "            ctx_ids, ctx_len = encode_texts(vocab, ctx_texts, args[\"ctx_max_len\"])\n",
    "            cand_ids, cand_len = encode_texts(vocab, cand_texts, args[\"cand_max_len\"])\n",
    "\n",
    "            ctx_vec = model.encoder(ctx_ids, ctx_len)  # [B, H]\n",
    "            cand_vec = model.encoder(cand_ids, cand_len)  # [sumK, H]\n",
    "\n",
    "            # Per-group scores and NLL on local gold index\n",
    "            scores = []\n",
    "            start = 0\n",
    "            for bi, g in enumerate(batch):\n",
    "                Ki = len(g[\"cands\"])\n",
    "                c_i = cand_vec[start : start + Ki]\n",
    "                ctx_i = ctx_vec[bi].unsqueeze(0).expand(Ki, -1)\n",
    "                feat = torch.cat(\n",
    "                    [ctx_i, c_i, torch.abs(ctx_i - c_i), ctx_i * c_i], dim=-1\n",
    "                )\n",
    "                scores.append(model.mlp(feat).squeeze(-1))  # [Ki]\n",
    "                start += Ki\n",
    "\n",
    "            losses = []\n",
    "            for s, g in zip(scores, batch):\n",
    "                logp = torch.log_softmax(s, dim=0)\n",
    "                gold_local = next(\n",
    "                    (j for j, (_, y) in enumerate(g[\"cands\"]) if y == 1), 0\n",
    "                )\n",
    "                losses.append(-logp[gold_local])\n",
    "            loss = torch.stack(losses).mean()\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            total_loss += float(loss.item())\n",
    "            steps += 1\n",
    "\n",
    "        print(f\"Epoch {epoch}: listwise loss={total_loss/max(steps,1):.4f}\")\n",
    "\n",
    "    return model, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10a8613",
   "metadata": {},
   "source": [
    "**score_pair_in_memory**\tPuntúa una pareja (contexto, candidato)\t(prob, logit)\n",
    "\n",
    "**choose_among_candidates**\tCompara varios candidatos y elige el mejor\t(best_index, softmax_probs, logits)\n",
    "\n",
    "**compare_two\tCompara dos (uno correcto y otro incorrecto)**\tDiccionario con resultados detallados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fe5dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def score_pair_in_memory(\n",
    "    model: PairwiseScorer,\n",
    "    vocab: Vocab,\n",
    "    context: str,\n",
    "    candidate: str,\n",
    "    ctx_max_len: int = 64,\n",
    "    cand_max_len: int = 64,\n",
    ") -> tuple[float, float]:\n",
    "    model.eval()\n",
    "    ctx_ids = torch.tensor(\n",
    "        [vocab.encode(context)[:ctx_max_len]], dtype=torch.long, device=DEVICE\n",
    "    )\n",
    "    ctx_len = torch.tensor([ctx_ids.size(1)], dtype=torch.long, device=DEVICE)\n",
    "    cand_ids = torch.tensor(\n",
    "        [vocab.encode(candidate)[:cand_max_len]], dtype=torch.long, device=DEVICE\n",
    "    )\n",
    "    cand_len = torch.tensor([cand_ids.size(1)], dtype=torch.long, device=DEVICE)\n",
    "    logit = model(ctx_ids, ctx_len, cand_ids, cand_len)  # shape [1]\n",
    "    prob = torch.sigmoid(logit).item()\n",
    "    return prob, logit.item()  # (sigmoid prob, raw logit)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def choose_among_candidates(\n",
    "    model: PairwiseScorer,\n",
    "    vocab: Vocab,\n",
    "    context: str,\n",
    "    candidates: list[str],\n",
    "    ctx_max_len: int = 64,\n",
    "    cand_max_len: int = 64,\n",
    "):\n",
    "    \"\"\"Return (best_index, softmax_probs[list], logits[list]) for a set of candidates.\"\"\"\n",
    "    model.eval()\n",
    "    K = len(candidates)\n",
    "    if K == 0:\n",
    "        return None, [], []\n",
    "    # encode one context, tile to K\n",
    "    ctx_enc = vocab.encode(context)[:ctx_max_len]\n",
    "    ctx_ids = torch.tensor([ctx_enc], dtype=torch.long, device=DEVICE).expand(K, -1)\n",
    "    ctx_len = torch.tensor([len(ctx_enc)], dtype=torch.long, device=DEVICE).expand(K)\n",
    "\n",
    "    # encode candidates with padding\n",
    "    cand_seqs = [vocab.encode(c)[:cand_max_len] for c in candidates]\n",
    "    maxL = max((len(s) for s in cand_seqs), default=0)\n",
    "    pad = vocab.pad_idx\n",
    "    cand_ids = torch.full((K, maxL), pad, dtype=torch.long, device=DEVICE)\n",
    "    cand_len = torch.tensor(\n",
    "        [len(s) for s in cand_seqs], dtype=torch.long, device=DEVICE\n",
    "    )\n",
    "    for i, s in enumerate(cand_seqs):\n",
    "        if s:\n",
    "            cand_ids[i, : len(s)] = torch.tensor(s, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    logits = model(ctx_ids, ctx_len, cand_ids, cand_len)  # [K]\n",
    "    probs = torch.softmax(logits, dim=0)  # [K]\n",
    "    best = int(torch.argmax(logits).item())\n",
    "    return best, probs.cpu().tolist(), logits.cpu().tolist()\n",
    "\n",
    "\n",
    "def compare_two(\n",
    "    model: PairwiseScorer,\n",
    "    vocab: Vocab,\n",
    "    context: str,\n",
    "    correct_candidate: str,\n",
    "    incorrect_candidate: str,\n",
    "):\n",
    "    best, probs, logits = choose_among_candidates(\n",
    "        model, vocab, context, [correct_candidate, incorrect_candidate]\n",
    "    )\n",
    "    return {\n",
    "        \"chosen\": \"correct\" if best == 0 else \"incorrect\",\n",
    "        \"softmax_probs\": {\"correct\": probs[0], \"incorrect\": probs[1]},\n",
    "        \"logits\": {\"correct\": logits[0], \"incorrect\": logits[1]},\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef0eb8",
   "metadata": {},
   "source": [
    "Entonces, armaremos un toy example para definir los hiperparámetros y entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34f164e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [\n",
    "    # Apple, company context\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"mention\": \"Apple\",\n",
    "        \"context\": \"I love my new Apple laptop\",\n",
    "        \"candidate\": \"Apple Inc. is a technology company\",\n",
    "        \"label\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"mention\": \"Apple\",\n",
    "        \"context\": \"I love my new Apple laptop\",\n",
    "        \"candidate\": \"Apple is a fruit often red or green\",\n",
    "        \"label\": 0,\n",
    "    },\n",
    "    # Apple, fruit context\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"mention\": \"Apple\",\n",
    "        \"context\": \"I ate a ripe apple today\",\n",
    "        \"candidate\": \"Apple is a fruit often red or green\",\n",
    "        \"label\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"mention\": \"Apple\",\n",
    "        \"context\": \"I ate a ripe apple today\",\n",
    "        \"candidate\": \"Apple Inc. is a technology company\",\n",
    "        \"label\": 0,\n",
    "    },\n",
    "    # Amazon, cloud context\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"mention\": \"Amazon\",\n",
    "        \"context\": \"Amazon released new cloud features\",\n",
    "        \"candidate\": \"Amazon.com is an e-commerce and cloud company\",\n",
    "        \"label\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"mention\": \"Amazon\",\n",
    "        \"context\": \"Amazon released new cloud features\",\n",
    "        \"candidate\": \"Amazon rainforest is in South America\",\n",
    "        \"label\": 0,\n",
    "    },\n",
    "    # Amazon, rainforest context\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"mention\": \"Amazon\",\n",
    "        \"context\": \"Tree species in the Amazon are diverse\",\n",
    "        \"candidate\": \"Amazon rainforest is in South America\",\n",
    "        \"label\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"mention\": \"Amazon\",\n",
    "        #\"candidate\":\n",
    "        \"context\": \"Amazon.com is an e-commerce and cloud company\",\n",
    "        \"label\": 0,\n",
    "    },\n",
    "    # Amazon, AWS context\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"mention\": \"Amazon\",\n",
    "        \"context\": \"AWS is a part of Amazon services\",\n",
    "        \"candidate\": \"Amazon.com is an e-commerce and cloud company\",\n",
    "        \"label\": 1,\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"mention\": \"Amazon\",\n",
    "        \"context\": \"Amazon rainforest is in South America\",\n",
    "        \"label\": 0,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd311ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 1,\n",
       "  'mention': 'Apple',\n",
       "  'context': 'I love my new Apple laptop',\n",
       "  'candidate': 'Apple Inc. is a technology company',\n",
       "  'label': 1},\n",
       " {'id': 1,\n",
       "  'mention': 'Apple',\n",
       "  'context': 'I love my new Apple laptop',\n",
       "  'candidate': 'Apple is a fruit often red or green',\n",
       "  'label': 0},\n",
       " {'id': 2,\n",
       "  'mention': 'Apple',\n",
       "  'context': 'I ate a ripe apple today',\n",
       "  'candidate': 'Apple is a fruit often red or green',\n",
       "  'label': 1},\n",
       " {'id': 2,\n",
       "  'mention': 'Apple',\n",
       "  'context': 'I ate a ripe apple today',\n",
       "  'candidate': 'Apple Inc. is a technology company',\n",
       "  'label': 0},\n",
       " {'id': 3,\n",
       "  'mention': 'Amazon',\n",
       "  'context': 'Amazon released new cloud features',\n",
       "  'candidate': 'Amazon.com is an e-commerce and cloud company',\n",
       "  'label': 1},\n",
       " {'id': 3,\n",
       "  'mention': 'Amazon',\n",
       "  'context': 'Amazon released new cloud features',\n",
       "  'candidate': 'Amazon rainforest is in South America',\n",
       "  'label': 0},\n",
       " {'id': 4,\n",
       "  'mention': 'Amazon',\n",
       "  'context': 'Tree species in the Amazon are diverse',\n",
       "  'candidate': 'Amazon rainforest is in South America',\n",
       "  'label': 1},\n",
       " {'id': 4,\n",
       "  'mention': 'Amazon',\n",
       "  'context': 'Amazon.com is an e-commerce and cloud company',\n",
       "  'label': 0},\n",
       " {'id': 5,\n",
       "  'mention': 'Amazon',\n",
       "  'context': 'AWS is a part of Amazon services',\n",
       "  'candidate': 'Amazon.com is an e-commerce and cloud company',\n",
       "  'label': 1},\n",
       " {'id': 5,\n",
       "  'mention': 'Amazon',\n",
       "  'context': 'Amazon rainforest is in South America',\n",
       "  'label': 0}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25f1f0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toy data written to data/train.csv and data/dev.csv\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"data\", exist_ok=True)\n",
    "for split in [\"train\", \"dev\"]:\n",
    "    with open(f\"data/{split}.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(\n",
    "            f, fieldnames=[\"id\", \"mention\", \"context\", \"candidate\", \"label\"]\n",
    "        )\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "print(\"Toy data written to data/train.csv and data/dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6745b4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'pw',\n",
       " 'train': 'data/train.csv',\n",
       " 'dev': 'data/dev.csv',\n",
       " 'save_dir': 'models/',\n",
       " 'encoder': 'lstm',\n",
       " 'emb_dim': 256,\n",
       " 'hidden_dim': 512,\n",
       " 'dropout': 0.2,\n",
       " 'lstm_layers': 1,\n",
       " 'attn_heads': 4,\n",
       " 'attn_layers': 2,\n",
       " 'batch_size': 1,\n",
       " 'epochs': 30,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.001,\n",
       " 'log_every': 5,\n",
       " 'min_freq': 1,\n",
       " 'ctx_max_len': 64,\n",
       " 'cand_max_len': 64,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARGS = {\n",
    "    'mode': 'pw',\n",
    "    \"train\": \"data/train.csv\",\n",
    "    \"dev\": \"data/dev.csv\",\n",
    "    \"save_dir\": \"models/\",\n",
    "    \"encoder\": \"lstm\",  # 'lstm' or 'attn'\n",
    "    \"emb_dim\": 256,\n",
    "    \"hidden_dim\": 512,\n",
    "    \"dropout\": 0.2,\n",
    "    \"lstm_layers\": 1,\n",
    "    \"attn_heads\": 4,\n",
    "    \"attn_layers\": 2,\n",
    "    \"batch_size\": 1,\n",
    "    \"epochs\": 30,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"log_every\": 5,\n",
    "    \"min_freq\": 1,\n",
    "    \"ctx_max_len\": 64,\n",
    "    \"cand_max_len\": 64,\n",
    "    \"seed\": 42,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d32d16e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'pw',\n",
       " 'train': 'data/train.csv',\n",
       " 'dev': 'data/dev.csv',\n",
       " 'save_dir': 'models/',\n",
       " 'encoder': 'lstm',\n",
       " 'emb_dim': 256,\n",
       " 'hidden_dim': 512,\n",
       " 'dropout': 0.2,\n",
       " 'lstm_layers': 1,\n",
       " 'attn_heads': 4,\n",
       " 'attn_layers': 2,\n",
       " 'batch_size': 1,\n",
       " 'epochs': 30,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.001,\n",
       " 'log_every': 5,\n",
       " 'min_freq': 1,\n",
       " 'ctx_max_len': 64,\n",
       " 'cand_max_len': 64,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1edffccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 DONE | pairwise dev_loss=0.6800 pairwise dev_acc=0.8000\n",
      "Epoch 2 DONE | pairwise dev_loss=0.6642 pairwise dev_acc=0.7000\n",
      "Epoch 3 DONE | pairwise dev_loss=0.6443 pairwise dev_acc=0.7000\n",
      "Epoch 4 DONE | pairwise dev_loss=0.6153 pairwise dev_acc=0.8000\n",
      "Epoch 5 DONE | pairwise dev_loss=0.5768 pairwise dev_acc=0.9000\n",
      "Epoch 6 DONE | pairwise dev_loss=0.5321 pairwise dev_acc=0.9000\n",
      "Epoch 7 DONE | pairwise dev_loss=0.4844 pairwise dev_acc=0.9000\n",
      "Epoch 8 DONE | pairwise dev_loss=0.4328 pairwise dev_acc=0.8000\n",
      "Epoch 9 DONE | pairwise dev_loss=0.3906 pairwise dev_acc=0.8000\n",
      "Epoch 10 DONE | pairwise dev_loss=0.3509 pairwise dev_acc=0.9000\n",
      "Epoch 11 DONE | pairwise dev_loss=0.3177 pairwise dev_acc=0.9000\n",
      "Epoch 12 DONE | pairwise dev_loss=0.2839 pairwise dev_acc=0.9000\n",
      "Epoch 13 DONE | pairwise dev_loss=0.2441 pairwise dev_acc=0.9000\n",
      "Epoch 14 DONE | pairwise dev_loss=0.2044 pairwise dev_acc=1.0000\n",
      "Epoch 15 DONE | pairwise dev_loss=0.1619 pairwise dev_acc=1.0000\n",
      "Epoch 16 DONE | pairwise dev_loss=0.1179 pairwise dev_acc=1.0000\n",
      "Epoch 17 DONE | pairwise dev_loss=0.0838 pairwise dev_acc=1.0000\n",
      "Epoch 18 DONE | pairwise dev_loss=0.0490 pairwise dev_acc=1.0000\n",
      "Epoch 19 DONE | pairwise dev_loss=0.0234 pairwise dev_acc=1.0000\n",
      "Epoch 20 DONE | pairwise dev_loss=0.0101 pairwise dev_acc=1.0000\n",
      "Epoch 21 DONE | pairwise dev_loss=0.0051 pairwise dev_acc=1.0000\n",
      "Epoch 22 DONE | pairwise dev_loss=0.0030 pairwise dev_acc=1.0000\n",
      "Epoch 23 DONE | pairwise dev_loss=0.0020 pairwise dev_acc=1.0000\n",
      "Epoch 24 DONE | pairwise dev_loss=0.0016 pairwise dev_acc=1.0000\n",
      "Epoch 25 DONE | pairwise dev_loss=0.0012 pairwise dev_acc=1.0000\n",
      "Epoch 26 DONE | pairwise dev_loss=0.0010 pairwise dev_acc=1.0000\n",
      "Epoch 27 DONE | pairwise dev_loss=0.0009 pairwise dev_acc=1.0000\n",
      "Epoch 28 DONE | pairwise dev_loss=0.0007 pairwise dev_acc=1.0000\n",
      "Epoch 29 DONE | pairwise dev_loss=0.0007 pairwise dev_acc=1.0000\n",
      "Epoch 30 DONE | pairwise dev_loss=0.0006 pairwise dev_acc=1.0000\n",
      "Training complete. Best dev acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "model_pw_lstm, vocab_pw_lstm = train_pairwise(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "64a579ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(candidate | context) = 0.9976624250411987 logit: 6.056290626525879\n"
     ]
    }
   ],
   "source": [
    "prob, logit = score_pair_in_memory(\n",
    "    model_pw_lstm,\n",
    "    vocab_pw_lstm,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    candidate=\"Apple is a fruit often red or green\",\n",
    ")\n",
    "print(\"P(candidate | context) =\", prob, \"logit:\", logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53e74858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax probs: [0.9999979734420776, 1.9989972770417808e-06]\n",
      "chosen: fruit\n"
     ]
    }
   ],
   "source": [
    "best_idx, probs, logits = choose_among_candidates(\n",
    "    model_pw_lstm,\n",
    "    vocab_pw_lstm,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    candidates=[\n",
    "        \"Apple is a fruit often red or green\",  # correct\n",
    "        \"Apple Inc. is a technology company\",  # incorrect\n",
    "    ],\n",
    ")\n",
    "print(\"softmax probs:\", probs)\n",
    "print(\"chosen:\", [\"fruit\", \"company\"][best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff600d48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'lw',\n",
       " 'train': 'data/train.csv',\n",
       " 'dev': 'data/dev.csv',\n",
       " 'save_dir': 'models/',\n",
       " 'encoder': 'lstm',\n",
       " 'emb_dim': 256,\n",
       " 'hidden_dim': 512,\n",
       " 'dropout': 0.2,\n",
       " 'lstm_layers': 1,\n",
       " 'attn_heads': 4,\n",
       " 'attn_layers': 2,\n",
       " 'batch_size': 1,\n",
       " 'epochs': 30,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.001,\n",
       " 'log_every': 5,\n",
       " 'min_freq': 1,\n",
       " 'ctx_max_len': 64,\n",
       " 'cand_max_len': 64,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARGS.update({\"mode\": \"lw\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ed5b06f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'lw',\n",
       " 'train': 'data/train.csv',\n",
       " 'dev': 'data/dev.csv',\n",
       " 'save_dir': 'models/',\n",
       " 'encoder': 'lstm',\n",
       " 'emb_dim': 256,\n",
       " 'hidden_dim': 512,\n",
       " 'dropout': 0.2,\n",
       " 'lstm_layers': 1,\n",
       " 'attn_heads': 4,\n",
       " 'attn_layers': 2,\n",
       " 'batch_size': 1,\n",
       " 'epochs': 30,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.001,\n",
       " 'log_every': 5,\n",
       " 'min_freq': 1,\n",
       " 'ctx_max_len': 64,\n",
       " 'cand_max_len': 64,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6dae814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: listwise loss=0.7017\n",
      "Epoch 2: listwise loss=0.6738\n",
      "Epoch 3: listwise loss=0.6464\n",
      "Epoch 4: listwise loss=0.6258\n",
      "Epoch 5: listwise loss=0.5884\n",
      "Epoch 6: listwise loss=0.5527\n",
      "Epoch 7: listwise loss=0.5035\n",
      "Epoch 8: listwise loss=0.4596\n",
      "Epoch 9: listwise loss=0.4101\n",
      "Epoch 10: listwise loss=0.3695\n",
      "Epoch 11: listwise loss=0.3215\n",
      "Epoch 12: listwise loss=0.2865\n",
      "Epoch 13: listwise loss=0.2573\n",
      "Epoch 14: listwise loss=0.2245\n",
      "Epoch 15: listwise loss=0.2020\n",
      "Epoch 16: listwise loss=0.1669\n",
      "Epoch 17: listwise loss=0.1360\n",
      "Epoch 18: listwise loss=0.1085\n",
      "Epoch 19: listwise loss=0.0906\n",
      "Epoch 20: listwise loss=0.0586\n",
      "Epoch 21: listwise loss=0.0318\n",
      "Epoch 22: listwise loss=0.0207\n",
      "Epoch 23: listwise loss=0.0148\n",
      "Epoch 24: listwise loss=0.0087\n",
      "Epoch 25: listwise loss=0.0042\n",
      "Epoch 26: listwise loss=0.0044\n",
      "Epoch 27: listwise loss=0.0022\n",
      "Epoch 28: listwise loss=0.0023\n",
      "Epoch 29: listwise loss=0.0015\n",
      "Epoch 30: listwise loss=0.0014\n"
     ]
    }
   ],
   "source": [
    "model_lw_lstm, vocab_lw_lstm = train_listwise(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8f4edaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': 'correct',\n",
       " 'softmax_probs': {'correct': 0.9989790916442871,\n",
       "  'incorrect': 0.0010209004394710064},\n",
       " 'logits': {'correct': 2.348507881164551, 'incorrect': -4.537540912628174}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_two(\n",
    "    model_lw_lstm,\n",
    "    vocab_lw_lstm,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    correct_candidate=\"Apple is a fruit often red or green\",\n",
    "    incorrect_candidate=\"Apple Inc. is a technology company\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5db75311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax probs: [0.9964815378189087, 0.003518466604873538]\n",
      "chosen idx: 0\n"
     ]
    }
   ],
   "source": [
    "best_idx, probs, logits = choose_among_candidates(\n",
    "    model_lw_lstm,\n",
    "    vocab_lw_lstm,\n",
    "    context=\"Amazon released new cloud features\",\n",
    "    candidates=[\n",
    "        \"Amazon.com is an e-commerce and cloud company\",  # correct\n",
    "        \"Amazon rainforest is in South America\",  # incorrect\n",
    "    ],\n",
    ")\n",
    "print(\"softmax probs:\", probs)\n",
    "print(\"chosen idx:\", best_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd277cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'pw',\n",
       " 'train': 'data/train.csv',\n",
       " 'dev': 'data/dev.csv',\n",
       " 'save_dir': 'models/',\n",
       " 'encoder': 'attn',\n",
       " 'emb_dim': 256,\n",
       " 'hidden_dim': 512,\n",
       " 'dropout': 0.2,\n",
       " 'lstm_layers': 1,\n",
       " 'attn_heads': 4,\n",
       " 'attn_layers': 2,\n",
       " 'batch_size': 1,\n",
       " 'epochs': 30,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.001,\n",
       " 'log_every': 5,\n",
       " 'min_freq': 1,\n",
       " 'ctx_max_len': 64,\n",
       " 'cand_max_len': 64,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARGS.update({\"mode\": \"pw\", \"encoder\": \"attn\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ae6e64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'pw',\n",
       " 'train': 'data/train.csv',\n",
       " 'dev': 'data/dev.csv',\n",
       " 'save_dir': 'models/',\n",
       " 'encoder': 'attn',\n",
       " 'emb_dim': 256,\n",
       " 'hidden_dim': 512,\n",
       " 'dropout': 0.2,\n",
       " 'lstm_layers': 1,\n",
       " 'attn_heads': 4,\n",
       " 'attn_layers': 2,\n",
       " 'batch_size': 1,\n",
       " 'epochs': 30,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.001,\n",
       " 'log_every': 5,\n",
       " 'min_freq': 1,\n",
       " 'ctx_max_len': 64,\n",
       " 'cand_max_len': 64,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8c6ec88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 DONE | pairwise dev_loss=0.6689 pairwise dev_acc=0.5000\n",
      "Epoch 2 DONE | pairwise dev_loss=0.6018 pairwise dev_acc=0.7000\n",
      "Epoch 3 DONE | pairwise dev_loss=0.5489 pairwise dev_acc=0.7000\n",
      "Epoch 4 DONE | pairwise dev_loss=0.5205 pairwise dev_acc=0.7000\n",
      "Epoch 5 DONE | pairwise dev_loss=0.4947 pairwise dev_acc=0.7000\n",
      "Epoch 6 DONE | pairwise dev_loss=0.5160 pairwise dev_acc=0.7000\n",
      "Epoch 7 DONE | pairwise dev_loss=0.5653 pairwise dev_acc=0.7000\n",
      "Epoch 8 DONE | pairwise dev_loss=0.4404 pairwise dev_acc=0.7000\n",
      "Epoch 9 DONE | pairwise dev_loss=0.4889 pairwise dev_acc=0.7000\n",
      "Epoch 10 DONE | pairwise dev_loss=0.4386 pairwise dev_acc=0.7000\n",
      "Epoch 11 DONE | pairwise dev_loss=0.3989 pairwise dev_acc=0.8000\n",
      "Epoch 12 DONE | pairwise dev_loss=0.3496 pairwise dev_acc=0.7000\n",
      "Epoch 13 DONE | pairwise dev_loss=0.3770 pairwise dev_acc=0.8000\n",
      "Epoch 14 DONE | pairwise dev_loss=0.2440 pairwise dev_acc=0.8000\n",
      "Epoch 15 DONE | pairwise dev_loss=0.1104 pairwise dev_acc=1.0000\n",
      "Epoch 16 DONE | pairwise dev_loss=0.0341 pairwise dev_acc=1.0000\n",
      "Epoch 17 DONE | pairwise dev_loss=0.0184 pairwise dev_acc=1.0000\n",
      "Epoch 18 DONE | pairwise dev_loss=0.0097 pairwise dev_acc=1.0000\n",
      "Epoch 19 DONE | pairwise dev_loss=0.0040 pairwise dev_acc=1.0000\n",
      "Epoch 20 DONE | pairwise dev_loss=0.0187 pairwise dev_acc=1.0000\n",
      "Epoch 21 DONE | pairwise dev_loss=0.0282 pairwise dev_acc=1.0000\n",
      "Epoch 22 DONE | pairwise dev_loss=0.0015 pairwise dev_acc=1.0000\n",
      "Epoch 23 DONE | pairwise dev_loss=0.0011 pairwise dev_acc=1.0000\n",
      "Epoch 24 DONE | pairwise dev_loss=0.0010 pairwise dev_acc=1.0000\n",
      "Epoch 25 DONE | pairwise dev_loss=0.0009 pairwise dev_acc=1.0000\n",
      "Epoch 26 DONE | pairwise dev_loss=0.0012 pairwise dev_acc=1.0000\n",
      "Epoch 27 DONE | pairwise dev_loss=0.0009 pairwise dev_acc=1.0000\n",
      "Epoch 28 DONE | pairwise dev_loss=0.0006 pairwise dev_acc=1.0000\n",
      "Epoch 29 DONE | pairwise dev_loss=0.0005 pairwise dev_acc=1.0000\n",
      "Epoch 30 DONE | pairwise dev_loss=0.0003 pairwise dev_acc=1.0000\n",
      "Training complete. Best dev acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "model_pw_attn, vocab_pw_attn = train_pairwise(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b86daa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(candidate | context) = 0.9996460676193237 logit: 7.9459357261657715\n"
     ]
    }
   ],
   "source": [
    "prob, logit = score_pair_in_memory(\n",
    "    model_pw_attn,\n",
    "    vocab_pw_attn,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    candidate=\"Apple is a fruit often red or green\",\n",
    ")\n",
    "print(\"P(candidate | context) =\", prob, \"logit:\", logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9848112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(candidate | context) = 0.9996460676193237 logit: 7.9459357261657715\n"
     ]
    }
   ],
   "source": [
    "print(\"P(candidate | context) =\", prob, \"logit:\", logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4da048b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax probs: [0.9999996423721313, 3.729472553004598e-07]\n",
      "chosen: fruit\n"
     ]
    }
   ],
   "source": [
    "best_idx, probs, logits = choose_among_candidates(\n",
    "    model_pw_attn,\n",
    "    vocab_pw_attn,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    candidates=[\n",
    "        \"Apple is a fruit often red or green\",  # correct\n",
    "        \"Apple Inc. is a technology company\",  # incorrect\n",
    "    ],\n",
    ")\n",
    "print(\"softmax probs:\", probs)\n",
    "print(\"chosen:\", [\"fruit\", \"company\"][best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fde90a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS.update({\"mode\": \"lw\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a5f46c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: listwise loss=0.6821\n",
      "Epoch 2: listwise loss=0.5622\n",
      "Epoch 3: listwise loss=0.5009\n",
      "Epoch 4: listwise loss=0.4575\n",
      "Epoch 5: listwise loss=0.3950\n",
      "Epoch 6: listwise loss=0.3535\n",
      "Epoch 7: listwise loss=0.3000\n",
      "Epoch 8: listwise loss=0.3067\n",
      "Epoch 9: listwise loss=0.2919\n",
      "Epoch 10: listwise loss=0.2532\n",
      "Epoch 11: listwise loss=0.2052\n",
      "Epoch 12: listwise loss=0.1877\n",
      "Epoch 13: listwise loss=0.1598\n",
      "Epoch 14: listwise loss=0.1291\n",
      "Epoch 15: listwise loss=0.0954\n",
      "Epoch 16: listwise loss=0.0628\n",
      "Epoch 17: listwise loss=0.0366\n",
      "Epoch 18: listwise loss=0.0160\n",
      "Epoch 19: listwise loss=0.0056\n",
      "Epoch 20: listwise loss=0.0029\n",
      "Epoch 21: listwise loss=0.0023\n",
      "Epoch 22: listwise loss=0.0020\n",
      "Epoch 23: listwise loss=0.0011\n",
      "Epoch 24: listwise loss=0.0009\n",
      "Epoch 25: listwise loss=0.0012\n",
      "Epoch 26: listwise loss=0.0006\n",
      "Epoch 27: listwise loss=0.0009\n",
      "Epoch 28: listwise loss=0.0006\n",
      "Epoch 29: listwise loss=0.0006\n",
      "Epoch 30: listwise loss=0.0004\n"
     ]
    }
   ],
   "source": [
    "model_lw_attn, vocab_lw_attn = train_listwise(ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed5ea16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': 'correct',\n",
       " 'softmax_probs': {'correct': 0.9998024106025696,\n",
       "  'incorrect': 0.00019758193229790777},\n",
       " 'logits': {'correct': 2.694718837738037, 'incorrect': -5.834440231323242}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_two(\n",
    "    model_lw_attn,\n",
    "    vocab_lw_attn,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    correct_candidate=\"Apple is a fruit often red or green\",\n",
    "    incorrect_candidate=\"Apple Inc. is a technology company\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3bca0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax probs: [0.9950313568115234, 0.0049686492420732975]\n",
      "chosen idx: 0\n"
     ]
    }
   ],
   "source": [
    "best_idx, probs, logits = choose_among_candidates(\n",
    "    model_lw_attn,\n",
    "    vocab_lw_attn,\n",
    "    context=\"Amazon released new cloud features\",\n",
    "    candidates=[\n",
    "        \"Amazon.com is an e-commerce and cloud company\",  # correct\n",
    "        \"Amazon rainforest is in South America\",  # incorrect\\\n",
    "    ],\n",
    ")\n",
    "print(\"softmax probs:\", probs)\n",
    "print(\"chosen idx:\", best_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef7bfe",
   "metadata": {},
   "source": [
    "Link: https://github.com/cyanic-selkie/aida-conll-yago-wikidata/tree/main/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ed1fa",
   "metadata": {},
   "source": [
    "Ahora utilicemos el Dataset AIDA-YAGO. Las dos celdas siguientes pueden correrlas para generar el dataset para poder ser aprovechado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35f4551",
   "metadata": {},
   "source": [
    "DESCOMENTAR LAS DOS CELDAS SUBSIGUIENTES PARA GENERAR EL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "427cf76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, re, csv, random, urllib.parse\n",
    "# from collections import defaultdict\n",
    "\n",
    "\n",
    "# def _title_from_url(url: str):\n",
    "#     if not url:\n",
    "#         return None\n",
    "#     try:\n",
    "#         t = url.split(\"/wiki/\", 1)[1]\n",
    "#     except Exception:\n",
    "#         return None\n",
    "#     return urllib.parse.unquote(t.replace(\" \", \"_\"))\n",
    "\n",
    "\n",
    "# def load_means(means_tsv, cap=100):\n",
    "#     alias = defaultdict(list)\n",
    "#     with open(means_tsv, encoding=\"utf-8\") as f:\n",
    "#         rdr = csv.reader(f, delimiter=\"\\t\")\n",
    "#         for row in rdr:\n",
    "#             if not row:\n",
    "#                 continue\n",
    "#             m = row[0].strip().strip('\"').lower()\n",
    "#             e = row[1].strip()\n",
    "#             if e and e != \"--NME--\" and len(alias[m]) < cap:\n",
    "#                 alias[m].append(e)\n",
    "#     return alias\n",
    "\n",
    "\n",
    "# def parse_aida_token_link_tsv(aida_tok_tsv_path, window=50):\n",
    "#     \"\"\"\n",
    "#     Reads the AIDA token+link TSV (your FIRST file) and returns:\n",
    "#       splits = {'train': [...], 'testa': [...], 'testb': [...]}\n",
    "#     Each item: {'mention': str, 'context': str, 'gold': str}\n",
    "#     \"\"\"\n",
    "#     docs = []\n",
    "#     cur = {\"doc\": None, \"split\": \"train\", \"tokens\": [], \"rows\": []}\n",
    "\n",
    "#     def flush():\n",
    "#         if cur[\"doc\"] is not None:\n",
    "#             docs.append(\n",
    "#                 {\n",
    "#                     \"doc\": cur[\"doc\"],\n",
    "#                     \"split\": cur[\"split\"],\n",
    "#                     \"tokens\": cur[\"tokens\"][:],\n",
    "#                     \"rows\": cur[\"rows\"][:],\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     with open(aida_tok_tsv_path, encoding=\"utf-8\") as f:\n",
    "#         for raw in f:\n",
    "#             line = raw.rstrip(\"\\n\")\n",
    "#             if not line:\n",
    "#                 continue\n",
    "#             if line.startswith(\"-DOCSTART-\"):\n",
    "#                 flush()\n",
    "#                 # doc name inside parentheses\n",
    "#                 m = re.search(r\"-DOCSTART-\\s*\\(([^)]+)\\)\", line)\n",
    "#                 docname = m.group(1) if m else line\n",
    "#                 split = (\n",
    "#                     \"testa\"\n",
    "#                     if \"testa\" in docname.lower()\n",
    "#                     else (\"testb\" if \"testb\" in docname.lower() else \"train\")\n",
    "#                 )\n",
    "#                 cur = {\"doc\": docname, \"split\": split, \"tokens\": [], \"rows\": []}\n",
    "#                 continue\n",
    "\n",
    "#             parts = line.split(\"\\t\")\n",
    "#             token = parts[0] if parts else \"\"\n",
    "#             cur[\"tokens\"].append(token)\n",
    "\n",
    "#             # Heuristic parse: many entity rows look like:\n",
    "#             # TOKEN  B|I  MENTION_SURFACE  WIKI_TITLE  WIKI_URL  WIKI_ID  MID\n",
    "#             bio = parts[1] if len(parts) >= 2 and parts[1] in {\"B\", \"I\", \"O\"} else None\n",
    "#             wiki_url = (\n",
    "#                 parts[4]\n",
    "#                 if len(parts) >= 5 and parts[4].startswith(\"http\")\n",
    "#                 else (\n",
    "#                     parts[3] if len(parts) >= 4 and parts[3].startswith(\"http\") else \"\"\n",
    "#                 )\n",
    "#             )\n",
    "#             wiki_title = (\n",
    "#                 parts[3] if len(parts) >= 4 and not parts[3].startswith(\"http\") else \"\"\n",
    "#             )\n",
    "#             gold = _title_from_url(wiki_url) or (\n",
    "#                 wiki_title if wiki_title and wiki_title != \"--NME--\" else None\n",
    "#             )\n",
    "\n",
    "#             mention_surface = parts[2] if len(parts) >= 3 else \"\"\n",
    "#             cur[\"rows\"].append(\n",
    "#                 {\n",
    "#                     \"token\": token,\n",
    "#                     \"bio\": bio,  # may be None for non-entity rows\n",
    "#                     \"surf\": mention_surface,\n",
    "#                     \"gold\": gold,  # None for --NME-- or non-entity\n",
    "#                 }\n",
    "#             )\n",
    "\n",
    "#     flush()\n",
    "\n",
    "#     # Build contiguous gold spans using BIO (+ same gold) and make context windows\n",
    "#     splits = {\"train\": [], \"testa\": [], \"testb\": []}\n",
    "#     for d in docs:\n",
    "#         toks = d[\"tokens\"]\n",
    "#         rows = d[\"rows\"]\n",
    "#         i, N = 0, len(rows)\n",
    "#         while i < N:\n",
    "#             r = rows[i]\n",
    "#             if r[\"bio\"] != \"B\" or not r[\"gold\"]:\n",
    "#                 i += 1\n",
    "#                 continue\n",
    "#             gold = r[\"gold\"]\n",
    "#             j = i + 1\n",
    "#             while j < N and rows[j][\"bio\"] == \"I\" and rows[j][\"gold\"] == gold:\n",
    "#                 j += 1\n",
    "#             # mention text: use tokens from i..j\n",
    "#             mention = \" \".join(toks[i:j])\n",
    "#             # context window\n",
    "#             L = max(0, i - window)\n",
    "#             R = min(N, j + window)\n",
    "#             context = \" \".join(toks[L:R]) or mention\n",
    "#             splits[d[\"split\"]].append(\n",
    "#                 {\"mention\": mention, \"context\": context, \"gold\": gold}\n",
    "#             )\n",
    "#             i = j\n",
    "#     return splits\n",
    "\n",
    "\n",
    "# def build_candidates(mention_text, gold, alias, fallback_golds, K_neg=5):\n",
    "#     # candidates: [gold] + K_neg negatives from alias table (by surface), fallback to other golds\n",
    "#     pool = [e for e in alias.get(mention_text.lower(), []) if e != gold]\n",
    "#     random.shuffle(pool)\n",
    "#     negs = pool[:K_neg]\n",
    "#     if len(negs) < K_neg:\n",
    "#         extra = [g for g in fallback_golds if g != gold]\n",
    "#         random.shuffle(extra)\n",
    "#         for e in extra:\n",
    "#             if len(negs) >= K_neg:\n",
    "#                 break\n",
    "#             if e not in negs:\n",
    "#                 negs.append(e)\n",
    "#     if not negs:  # ultra edge-case\n",
    "#         negs = [gold + \"_NEG\"]\n",
    "#     return [gold] + negs\n",
    "\n",
    "\n",
    "# def write_pairwise_and_listwise_from_token_link_tsv(\n",
    "#     aida_tok_tsv_path,\n",
    "#     means_tsv_path,\n",
    "#     outdir=\"data/aida_csv\",\n",
    "#     K_neg=5,\n",
    "#     seed=42,\n",
    "#     window=50,\n",
    "# ):\n",
    "#     random.seed(seed)\n",
    "#     splits = parse_aida_token_link_tsv(aida_tok_tsv_path, window=window)\n",
    "#     alias = load_means(means_tsv_path)\n",
    "#     os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "#     paths = {}\n",
    "#     for split, rows in splits.items():\n",
    "#         pw_path = os.path.join(outdir, f\"{split}_pairs.csv\")\n",
    "#         lw_path = os.path.join(outdir, f\"{split}_listwise.csv\")\n",
    "#         with open(pw_path, \"w\", newline=\"\", encoding=\"utf-8\") as f1, open(\n",
    "#             lw_path, \"w\", newline=\"\", encoding=\"utf-8\"\n",
    "#         ) as f2:\n",
    "#             cols = [\"id\", \"mention\", \"context\", \"candidate\", \"label\"]\n",
    "#             pw = csv.DictWriter(f1, fieldnames=cols)\n",
    "#             pw.writeheader()\n",
    "#             lw = csv.DictWriter(f2, fieldnames=cols)\n",
    "#             lw.writeheader()\n",
    "#             gid = rid = 0\n",
    "#             all_golds = [r[\"gold\"] for r in rows]\n",
    "#             for ex in rows:\n",
    "#                 m, ctx, gold = ex[\"mention\"], ex[\"context\"], ex[\"gold\"]\n",
    "#                 cands = build_candidates(m, gold, alias, all_golds, K_neg=K_neg)\n",
    "#                 # listwise group\n",
    "#                 for c in cands:\n",
    "#                     lw.writerow(\n",
    "#                         {\n",
    "#                             \"id\": gid,\n",
    "#                             \"mention\": m,\n",
    "#                             \"context\": ctx,\n",
    "#                             \"candidate\": c,\n",
    "#                             \"label\": int(c == gold),\n",
    "#                         }\n",
    "#                     )\n",
    "#                 gid += 1\n",
    "#                 # pairwise rows\n",
    "#                 for c in cands:\n",
    "#                     pw.writerow(\n",
    "#                         {\n",
    "#                             \"id\": rid,\n",
    "#                             \"mention\": m,\n",
    "#                             \"context\": ctx,\n",
    "#                             \"candidate\": c,\n",
    "#                             \"label\": int(c == gold),\n",
    "#                         }\n",
    "#                     )\n",
    "#                     rid += 1\n",
    "#         paths[split] = {\"pairwise\": pw_path, \"listwise\": lw_path}\n",
    "#         print(\"Wrote\", pw_path, \"and\", lw_path)\n",
    "#     return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c8f4677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = write_pairwise_and_listwise_from_token_link_tsv(\n",
    "#     aida_tok_tsv_path=\"/media/pdconte/hdd/Pablo/Personal/Colegio_Bourbaki/Natural_Language_Processsing/Semana4/aida_yago2/aida-yago2-dataset/AIDA-YAGO2-dataset.tsv\",  # ← your FIRST file\n",
    "#     means_tsv_path=\"/media/pdconte/hdd/Pablo/Personal/Colegio_Bourbaki/Natural_Language_Processsing/Semana4/aida_means.tsv\",  # uncompressed from aida_means.tsv.bz2\n",
    "#     outdir=\"data/aida_csv\",\n",
    "#     K_neg=5,\n",
    "#     seed=42,\n",
    "#     window=50,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a6945f",
   "metadata": {},
   "source": [
    "Vamos a entrenar los modelos con la nueva data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1bd10725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'pw',\n",
       " 'train': 'data/aida_csv/train_pairs.csv',\n",
       " 'dev': 'data/aida_csv/testa_pairs.csv',\n",
       " 'save_dir': 'models/',\n",
       " 'encoder': 'lstm',\n",
       " 'emb_dim': 256,\n",
       " 'hidden_dim': 512,\n",
       " 'dropout': 0.2,\n",
       " 'lstm_layers': 1,\n",
       " 'attn_heads': 4,\n",
       " 'attn_layers': 2,\n",
       " 'batch_size': 256,\n",
       " 'epochs': 10,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.001,\n",
       " 'log_every': 5,\n",
       " 'min_freq': 1,\n",
       " 'ctx_max_len': 64,\n",
       " 'cand_max_len': 64,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AARGS = dict(ARGS)\n",
    "AARGS.update(\n",
    "    {\n",
    "        \"mode\": \"pw\",\n",
    "        \"train\": \"data/aida_csv/train_pairs.csv\",\n",
    "        \"dev\": \"data/aida_csv/testa_pairs.csv\",\n",
    "        \"encoder\": \"lstm\",\n",
    "        \"batch_size\": 256,\n",
    "        'epochs': 10,\n",
    "    }\n",
    ")\n",
    "AARGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf804980",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Vamos a entrenar pocas etapas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b0481cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 DONE | pairwise dev_loss=0.4073 pairwise dev_acc=0.8387\n",
      "Epoch 2 DONE | pairwise dev_loss=0.3521 pairwise dev_acc=0.8699\n",
      "Epoch 3 DONE | pairwise dev_loss=0.3069 pairwise dev_acc=0.8876\n",
      "Epoch 4 DONE | pairwise dev_loss=0.2908 pairwise dev_acc=0.8742\n",
      "Epoch 5 DONE | pairwise dev_loss=0.3235 pairwise dev_acc=0.8568\n",
      "Epoch 6 DONE | pairwise dev_loss=0.6087 pairwise dev_acc=0.7828\n",
      "Epoch 7 DONE | pairwise dev_loss=0.4946 pairwise dev_acc=0.8162\n",
      "Epoch 8 DONE | pairwise dev_loss=0.5463 pairwise dev_acc=0.8228\n",
      "Epoch 9 DONE | pairwise dev_loss=0.6777 pairwise dev_acc=0.8101\n",
      "Epoch 10 DONE | pairwise dev_loss=0.8773 pairwise dev_acc=0.7975\n",
      "Training complete. Best dev acc: 0.8876017532874139\n"
     ]
    }
   ],
   "source": [
    "model_pw_lstm, vocab_pw_lstm = train_pairwise(AARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3355ef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, vocab, args):\n",
    "    \"\"\"Saves the trained model and vocabulary to disk.\"\"\"\n",
    "    os.makedirs(args.get(\"save_dir\", \"models\"), exist_ok=True)\n",
    "    last_path = os.path.join(\n",
    "        args[\"save_dir\"], f\"ned_{args['mode']}_{args['encoder']}_latest.pt\"\n",
    "    )\n",
    "\n",
    "    ckpt1 = {\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"vocab_itos\": getattr(vocab, \"itos\", None),  # your Vocab stores tokens here\n",
    "        \"args\": dict(args),\n",
    "    }\n",
    "    torch.save(ckpt1, last_path)\n",
    "    print(\"Saved\", last_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66f4f185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models/ned_pw_lstm_latest.pt\n"
     ]
    }
   ],
   "source": [
    "save_model(model_pw_lstm, vocab_pw_lstm, AARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24667396",
   "metadata": {},
   "source": [
    "Podemos usar esta rutina para cargar modelo y vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ce3e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load back (example) ---\n",
    "last_path = os.path.join(\n",
    "    AARGS[\"save_dir\"], f\"ned_{AARGS['mode']}_{AARGS['encoder']}_latest.pt\"\n",
    ")\n",
    "ckpt1 = torch.load(last_path, map_location=DEVICE)\n",
    "saved_args = ckpt1[\"args\"]\n",
    "\n",
    "# Rebuild vocab from itos\n",
    "vocab2 = Vocab()\n",
    "vocab2.itos = deepcopy(ckpt1[\"vocab_itos\"])\n",
    "vocab2.stoi = {t: i for i, t in enumerate(vocab2.itos)}\n",
    "\n",
    "# Rebuild encoder & model from saved args\n",
    "if saved_args[\"encoder\"] == \"lstm\":\n",
    "    enc2 = LSTMEncoder(\n",
    "        len(vocab2),\n",
    "        saved_args[\"emb_dim\"],\n",
    "        saved_args[\"hidden_dim\"],\n",
    "        saved_args[\"lstm_layers\"],\n",
    "        saved_args[\"dropout\"],\n",
    "        pad_idx=vocab2.pad_idx,\n",
    "    )\n",
    "else:\n",
    "    enc2 = SelfAttentionEncoder(\n",
    "        len(vocab2),\n",
    "        saved_args[\"emb_dim\"],\n",
    "        saved_args[\"hidden_dim\"],\n",
    "        saved_args[\"attn_heads\"],\n",
    "        saved_args[\"attn_layers\"],\n",
    "        saved_args[\"dropout\"],\n",
    "        pad_idx=vocab2.pad_idx,\n",
    "        max_len=max(saved_args[\"ctx_max_len\"], saved_args[\"cand_max_len\"]),\n",
    "    )\n",
    "\n",
    "model2 = PairwiseScorer(\n",
    "    enc2, hidden_dim=saved_args[\"hidden_dim\"], dropout=saved_args[\"dropout\"]\n",
    ").to(DEVICE)\n",
    "model2.load_state_dict(ckpt1[\"model_state\"])\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a32a8826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(candidate | context) = 0.32778704166412354 logit: -0.7182109951972961\n"
     ]
    }
   ],
   "source": [
    "prob, logit = score_pair_in_memory(\n",
    "    model_pw_lstm,\n",
    "    vocab_pw_lstm,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    candidate=\"Apple is a fruit often red or green\",\n",
    ")\n",
    "print(\"P(candidate | context) =\", prob, \"logit:\", logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e236b397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax probs: [0.9376494884490967, 0.06235046312212944]\n",
      "chosen: fruit\n"
     ]
    }
   ],
   "source": [
    "best_idx, probs, logits = choose_among_candidates(\n",
    "    model_pw_lstm,\n",
    "    vocab_pw_lstm,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    candidates=[\n",
    "        \"Apple is a fruit often red or green\",  # correct\n",
    "        \"Apple Inc. is a technology company\",  # incorrect\n",
    "    ],\n",
    ")\n",
    "print(\"softmax probs:\", probs)\n",
    "print(\"chosen:\", [\"fruit\", \"company\"][best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8269b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'lw',\n",
       " 'train': 'data/aida_csv/train_listwise.csv',\n",
       " 'dev': 'data/aida_csv/testa_listwise.csv',\n",
       " 'save_dir': 'models/',\n",
       " 'encoder': 'lstm',\n",
       " 'emb_dim': 256,\n",
       " 'hidden_dim': 512,\n",
       " 'dropout': 0.2,\n",
       " 'lstm_layers': 1,\n",
       " 'attn_heads': 4,\n",
       " 'attn_layers': 2,\n",
       " 'batch_size': 256,\n",
       " 'epochs': 10,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.001,\n",
       " 'log_every': 5,\n",
       " 'min_freq': 1,\n",
       " 'ctx_max_len': 64,\n",
       " 'cand_max_len': 64,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AARGS.update(\n",
    "    {\n",
    "        'mode':'lw',\n",
    "        \"train\": \"data/aida_csv/train_listwise.csv\",\n",
    "        \"dev\": \"data/aida_csv/testa_listwise.csv\",\n",
    "    }\n",
    ")\n",
    "AARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "084a23e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: listwise loss=1.7207\n",
      "Epoch 2: listwise loss=1.4952\n",
      "Epoch 3: listwise loss=1.2094\n",
      "Epoch 4: listwise loss=0.9450\n",
      "Epoch 5: listwise loss=0.7301\n",
      "Epoch 6: listwise loss=0.5623\n",
      "Epoch 7: listwise loss=0.4387\n",
      "Epoch 8: listwise loss=0.3422\n",
      "Epoch 9: listwise loss=0.2679\n",
      "Epoch 10: listwise loss=0.2039\n"
     ]
    }
   ],
   "source": [
    "model_lw_lstm, vocab_lw_lstm = train_listwise(AARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08c17c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models/ned_lw_lstm_latest.pt\n"
     ]
    }
   ],
   "source": [
    "save_model(model_lw_lstm, vocab_lw_lstm, AARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0db05f4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': 'correct',\n",
       " 'softmax_probs': {'correct': 0.9026119112968445,\n",
       "  'incorrect': 0.09738808870315552},\n",
       " 'logits': {'correct': -4.809988021850586, 'incorrect': -7.036576747894287}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_two(\n",
    "    model_lw_lstm,\n",
    "    vocab_lw_lstm,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    correct_candidate=\"Apple is a fruit often red or green\",\n",
    "    incorrect_candidate=\"Apple Inc. is a technology company\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "737d531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax probs: [0.26784127950668335, 0.7321587800979614]\n",
      "chosen idx: 1\n"
     ]
    }
   ],
   "source": [
    "best_idx, probs, logits = choose_among_candidates(\n",
    "    model_lw_lstm,\n",
    "    vocab_lw_lstm,\n",
    "    context=\"Amazon released new cloud features\",\n",
    "    candidates=[\n",
    "        \"Amazon.com is an e-commerce and cloud company\",  # correct\n",
    "        \"Amazon rainforest is in South America\",  # incorrect\n",
    "    ],\n",
    ")\n",
    "print(\"softmax probs:\", probs)\n",
    "print(\"chosen idx:\", best_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c20742d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'pw',\n",
       " 'train': 'data/aida_csv/train_pairs.csv',\n",
       " 'dev': 'data/aida_csv/testa_pairs.csv',\n",
       " 'save_dir': 'models/',\n",
       " 'encoder': 'attn',\n",
       " 'emb_dim': 256,\n",
       " 'hidden_dim': 512,\n",
       " 'dropout': 0.2,\n",
       " 'lstm_layers': 1,\n",
       " 'attn_heads': 4,\n",
       " 'attn_layers': 2,\n",
       " 'batch_size': 256,\n",
       " 'epochs': 10,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.001,\n",
       " 'log_every': 5,\n",
       " 'min_freq': 1,\n",
       " 'ctx_max_len': 64,\n",
       " 'cand_max_len': 64,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AARGS.update(\n",
    "    {\n",
    "        'mode':'pw',\n",
    "        \"train\": \"data/aida_csv/train_pairs.csv\",\n",
    "        \"dev\": \"data/aida_csv/testa_pairs.csv\",\n",
    "        \"encoder\": \"attn\",\n",
    "    }\n",
    ")\n",
    "AARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62708832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 DONE | pairwise dev_loss=0.3735 pairwise dev_acc=0.8342\n",
      "Epoch 2 DONE | pairwise dev_loss=0.4763 pairwise dev_acc=0.8415\n",
      "Epoch 3 DONE | pairwise dev_loss=0.3677 pairwise dev_acc=0.8597\n",
      "Epoch 4 DONE | pairwise dev_loss=0.4256 pairwise dev_acc=0.8614\n",
      "Epoch 5 DONE | pairwise dev_loss=0.4423 pairwise dev_acc=0.8650\n",
      "Epoch 6 DONE | pairwise dev_loss=0.5396 pairwise dev_acc=0.8533\n",
      "Epoch 7 DONE | pairwise dev_loss=0.6196 pairwise dev_acc=0.8489\n",
      "Epoch 8 DONE | pairwise dev_loss=0.6101 pairwise dev_acc=0.8601\n",
      "Epoch 9 DONE | pairwise dev_loss=0.7078 pairwise dev_acc=0.8472\n",
      "Epoch 10 DONE | pairwise dev_loss=0.8683 pairwise dev_acc=0.8422\n",
      "Training complete. Best dev acc: 0.8649899116398804\n"
     ]
    }
   ],
   "source": [
    "model_pw_attn, vocab_pw_attn = train_pairwise(AARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e8fe542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models/ned_pw_attn_latest.pt\n"
     ]
    }
   ],
   "source": [
    "save_model(model_pw_attn, vocab_pw_attn, AARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "499720fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(candidate | context) = 0.9879438877105713 logit: 4.406055450439453\n"
     ]
    }
   ],
   "source": [
    "prob, logit = score_pair_in_memory(\n",
    "    model_pw_attn,\n",
    "    vocab_pw_attn,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    candidate=\"Apple is a fruit often red or green\",\n",
    ")\n",
    "print(\"P(candidate | context) =\", prob, \"logit:\", logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b731539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax probs: [0.038246218115091324, 0.961753785610199]\n",
      "chosen idx: 1\n"
     ]
    }
   ],
   "source": [
    "best_idx, probs, logits = choose_among_candidates(\n",
    "    model_pw_attn,\n",
    "    vocab_pw_attn,\n",
    "    context=\"Amazon released new cloud features\",\n",
    "    candidates=[\n",
    "        \"Amazon.com is an e-commerce and cloud company\",  # correct\n",
    "        \"Amazon rainforest is in South America\",  # incorrect\n",
    "    ],\n",
    ")\n",
    "print(\"softmax probs:\", probs)\n",
    "print(\"chosen idx:\", best_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dee584cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mode': 'lw',\n",
       " 'train': 'data/aida_csv/train_listwise.csv',\n",
       " 'dev': 'data/aida_csv/testa_listwise.csv',\n",
       " 'save_dir': 'models/',\n",
       " 'encoder': 'attn',\n",
       " 'emb_dim': 256,\n",
       " 'hidden_dim': 512,\n",
       " 'dropout': 0.2,\n",
       " 'lstm_layers': 1,\n",
       " 'attn_heads': 4,\n",
       " 'attn_layers': 2,\n",
       " 'batch_size': 256,\n",
       " 'epochs': 10,\n",
       " 'lr': 0.0001,\n",
       " 'weight_decay': 0.001,\n",
       " 'log_every': 5,\n",
       " 'min_freq': 1,\n",
       " 'ctx_max_len': 64,\n",
       " 'cand_max_len': 64,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AARGS.update(\n",
    "    {\n",
    "        'mode': 'lw',\n",
    "        \"train\": \"data/aida_csv/train_listwise.csv\",\n",
    "        \"dev\": \"data/aida_csv/testa_listwise.csv\",\n",
    "    }\n",
    ")\n",
    "AARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69e92648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: listwise loss=1.7203\n",
      "Epoch 2: listwise loss=1.5338\n",
      "Epoch 3: listwise loss=1.1776\n",
      "Epoch 4: listwise loss=0.9476\n",
      "Epoch 5: listwise loss=0.7918\n",
      "Epoch 6: listwise loss=0.6660\n",
      "Epoch 7: listwise loss=0.5714\n",
      "Epoch 8: listwise loss=0.4860\n",
      "Epoch 9: listwise loss=0.4123\n",
      "Epoch 10: listwise loss=0.3608\n"
     ]
    }
   ],
   "source": [
    "model_lw_attn, vocab_lw_attn = train_listwise(AARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cfd03f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved models/ned_lw_attn_latest.pt\n"
     ]
    }
   ],
   "source": [
    "save_model(model_lw_attn, vocab_lw_attn, AARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d75f2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chosen': 'incorrect',\n",
       " 'softmax_probs': {'correct': 0.21120314300060272,\n",
       "  'incorrect': 0.7887968420982361},\n",
       " 'logits': {'correct': -1.2091745138168335, 'incorrect': 0.10851380228996277}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_two(\n",
    "    model_lw_attn,\n",
    "    vocab_lw_attn,\n",
    "    context=\"I ate a ripe apple today\",\n",
    "    correct_candidate=\"Apple is a fruit often red or green\",\n",
    "    incorrect_candidate=\"Apple Inc. is a technology company\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ae6090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax probs: [0.40954166650772095, 0.590458333492279]\n",
      "chosen idx: 1\n"
     ]
    }
   ],
   "source": [
    "best_idx, probs, logits = choose_among_candidates(\n",
    "    model_lw_attn,\n",
    "    vocab_lw_attn,\n",
    "    context=\"Amazon released new cloud features\",\n",
    "    candidates=[\n",
    "        \"Amazon.com is an e-commerce and cloud company\",  # correct\n",
    "        \"Amazon rainforest is in South America\",  # incorrect\n",
    "    ],\n",
    ")\n",
    "print(\"softmax probs:\", probs)\n",
    "print(\"chosen idx:\", best_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd58ef",
   "metadata": {},
   "source": [
    "### EJERCICIO:\n",
    "* Entrenar con más epocas para mejorar la salida de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18390784",
   "metadata": {},
   "source": [
    "![Lenguaje Matemático](./Images/Matematicas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13c8a4d",
   "metadata": {},
   "source": [
    "![Contacto](./Images/Contacto.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
